{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-04T21:28:00.954709Z",
     "iopub.status.busy": "2021-04-04T21:28:00.953899Z",
     "iopub.status.idle": "2021-04-04T21:28:04.166828Z",
     "shell.execute_reply": "2021-04-04T21:28:04.165153Z"
    },
    "papermill": {
     "duration": 3.257231,
     "end_time": "2021-04-04T21:28:04.167013",
     "exception": false,
     "start_time": "2021-04-04T21:28:00.909782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from bitarray import bitarray\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.options.mode.chained_assignment = None # discard the SettingWithCopyWarning\n",
    "np.seterr(divide='ignore', invalid='ignore'); # discard /0 warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-04-04T21:28:04.274883Z",
     "iopub.status.busy": "2021-04-04T21:28:04.236807Z",
     "iopub.status.idle": "2021-04-04T21:28:04.329081Z",
     "shell.execute_reply": "2021-04-04T21:28:04.328260Z"
    },
    "papermill": {
     "duration": 0.131446,
     "end_time": "2021-04-04T21:28:04.329386",
     "exception": false,
     "start_time": "2021-04-04T21:28:04.197940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# PATHS\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "# Datasets\n",
    "\n",
    "question_file = '../input/riiid-test-answer-prediction/questions.csv'\n",
    "lecture_file = '../input/riiid-test-answer-prediction/lectures.csv'\n",
    "example_test_file = '../input/riiid-test-answer-prediction/example_test.csv'\n",
    "\n",
    "\n",
    "# Offline preprocessed features\n",
    "\n",
    "\n",
    "offline_features_train_paths = { 'answered_correctly_avg_u': '../input/riiid-avg-score-u-corrected/avg_score_u_train_arr.npy',\n",
    "                                 'answered_correctly_sum_u': '../input/riiid-avg-score-u-corrected/sum_score_u_train_arr.npy',\n",
    "                                 'q_count_u': '../input/riiid-avg-score-u-corrected/q_count_u_train_arr.npy',\n",
    "                                 'is_first_attempt': '../input/riiid-is-first-atpt-rows-q/is_first_attempt_rows_train_q.npy',\n",
    "                                 'answered_correctly_avg_c': '../input/riiid-feats-rows-arr-01/avg_score_c_rows_train_questions.npy',\n",
    "                                 'answered_correctly_std_c': '../input/riiid-feats-rows-arr-01/std_score_c_rows_train_questions.npy',\n",
    "                                 'answered_correctly_median_c': '../input/riiid-feats-rows-arr-01/median_score_c_rows_train_questions.npy',\n",
    "                                 'part': '../input/riiid-feats-rows-arr-01/part_rows_train_questions.npy',\n",
    "                                 'prior_question_had_explanation': '../input/riiid-feats-rows-arr-01/prior_explanation_rows_train_questions.npy',\n",
    "                                 'prior_question_elapsed_time': '../input/riiid-feats-rows-arr-01/prior_elapsed_time_rows_train_questions.npy',\n",
    "                                 'l_count_u': '../input/riiid-count-u-lec-rows-q/count_u_lec_rows_train_q.npy',\n",
    "                                 'l_part_count_u': '../input/riiid-lec-part-u-rows-q/cnt_lec_part_u_rows_train_q.npy',\n",
    "                                 'session_num': '../input/riiid-session-basics-fe/session_num_train_arr.npy' ,\n",
    "                                 'same_container_as_last': '../input/riiid-same-cont-as-last-arr/same_container_as_last_train_arr.npy',\n",
    "                                 'last_container_sum_answ': '../input/riiid-last-cont-sum-answ-arr/last_container_sum_answ_train_arr.npy',\n",
    "                                 'last_cont_q_count': '../input/riiid-last-cont-q-count-arr/last_container_q_count_train_arr.npy' ,\n",
    "                                 'last_session_break_time': '../input/riiid-last-sess-break-time-pkl-arrs/last_session_break_time_train_arr.npy',\n",
    "                                 'avg_break_time': '../input/riiid-avg-break-time-pkl-arr/avg_break_time_train_arr.npy',\n",
    "                                 'avg_session_time': '../input/riiid-session-avg-time/session_avg_time_train_arr.npy',\n",
    "                                 'avg_session_q_count': '../input/riiid-avg-session-q-count-arr/avg_session_q_count_train_arr.npy',\n",
    "                                 'current_session_avg_score': '../input/riiid-curr-sess-avg-score-pkl-arr/current_session_avg_score_train_arr.npy',\n",
    "                                 'same_session_as_last': '../input/riiid-session-basics-fe/same_session_as_last_train_arr.npy',\n",
    "                                 'current_session_time': '../input/riiid-curr-session-time-arr-pkl/current_session_time_train_arr.npy',\n",
    "                                 'current_session_q_count': '../input/riiid-curr-sess-q-count-arr-pkl/current_session_q_count_train_arr.npy',\n",
    "                                 'avg_score_u_hist_100_75': '../input/riiid-hist-100-score-slope-clean/avg_score_u_hist_100_75_train_arr.npy',\n",
    "                                 'avg_score_u_hist_75_50': '../input/riiid-hist-100-score-slope-clean/avg_score_u_hist_75_50_train_arr.npy',\n",
    "                                 'avg_score_u_hist_50_25': '../input/riiid-hist-100-score-slope-clean/avg_score_u_hist_50_25_train_arr.npy',\n",
    "                                 'avg_score_u_hist_25_0': '../input/riiid-hist-100-score-slope-clean/avg_score_u_hist_25_0_train_arr.npy',\n",
    "                                 'hist_score_diff_u_100_50': '../input/riiid-hist-100-score-slope-clean/hist_score_diff_u_100_50_train_arr.npy',\n",
    "                                 'hist_score_diff_u_75_25': '../input/riiid-hist-100-score-slope-clean/hist_score_diff_u_75_25_train_arr.npy',\n",
    "                                 'hist_score_diff_u_50_0': '../input/riiid-hist-100-score-slope-clean/hist_score_diff_u_50_0_train_arr.npy',\n",
    "                                 'hist_100_score_slope_u': '../input/riiid-hist-100-score-slope-clean/hist_100_score_slope_u_train_arr.npy',\n",
    "                                 'current_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/current_right_answ_streak_train_arr.npy',\n",
    "                                 'max_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/max_right_answ_streak_train_arr.npy',\n",
    "                                 'hist_1_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/hist_1_right_answ_streak_train_arr.npy',\n",
    "                                 'hist_2_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/hist_2_right_answ_streak_train_arr.npy',\n",
    "                                 'hist_3_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/hist_3_right_answ_streak_train_arr.npy',\n",
    "                                 'avg_right_answ_streak_hist_3': '../input/riiid-answ-streak-u-pkl-arr/avg_right_answ_streak_hist_3_train_arr.npy',\n",
    "                                 'first_try_success_count_u': '../input/riiid-first-try-success-count-u-arr-pkl/first_try_success_count_u_train_arr.npy',\n",
    "                                 'unique_count_attempted_q_u': '../input/riiid-cnt-attempted-q-arr-pkl/count_attempted_q_u_train_arr.npy',\n",
    "                                 'avg_score_t': '../input/riiid-fe-tags-score-stats/avg_score_t_train_arr.npy',\n",
    "                                 'max_avg_score_t': '../input/riiid-fe-tags-score-stats/max_avg_score_t_train_arr.npy',\n",
    "                                 'min_avg_score_t': '../input/riiid-fe-tags-score-stats/min_avg_score_t_train_arr.npy',\n",
    "                                 'std_avg_score_t': '../input/riiid-fe-tags-score-stats/std_avg_score_t_train_arr.npy',\n",
    "                                 'avg_tag_freq': '../input/riiid-fe-tags-freq-stats/tag_freq_feats_arrs/avg_tag_frequency_train_arr.npy',\n",
    "                                 'max_tag_freq': '../input/riiid-fe-tags-freq-stats/tag_freq_feats_arrs/max_tag_frequency_train_arr.npy',\n",
    "                                 'min_tag_freq': '../input/riiid-fe-tags-freq-stats/tag_freq_feats_arrs/min_tag_frequency_train_arr.npy',\n",
    "                                 'std_tag_freq': '../input/riiid-fe-tags-freq-stats/tag_freq_feats_arrs/std_tag_frequency_train_arr.npy',\n",
    "                                 'tag_count_q': '../input/riiid-q-tag-count-dict/tag_count_q_train_arr.npy',\n",
    "                                 'avg_question_elapsed_time_c': '../input/riiid-question-time-stats/avg_question_elapsed_time_c_train_arr.npy',\n",
    "                                 'std_question_elapsed_time_c': '../input/riiid-question-time-stats/std_question_elapsed_time_c_train_arr.npy',\n",
    "                                 'avg_time_since_last_c': '../input/riiid-question-time-stats/avg_time_since_last_c_train_arr.npy',\n",
    "                                 'std_time_since_last_c': '../input/riiid-question-time-stats/std_time_since_last_c_train_arr.npy',\n",
    "                                 'diff_avg_question_elapsed_time_c': '../input/riiid-question-time-stats/diff_avg_question_elapsed_time_c_train_arr.npy',\n",
    "                                 'diff_avg_time_since_last_c': '../input/riiid-question-time-stats/diff_avg_time_since_last_c_train_arr.npy',\n",
    "                                 'ratio_diff_avg_question_elapsed_time_c': '../input/riiid-question-time-stats/ratio_diff_avg_question_elapsed_time_c_train_arr.npy',\n",
    "                                 'ratio_diff_avg_time_since_last_c': '../input/riiid-question-time-stats/ratio_diff_avg_time_since_last_c_train_arr.npy',\n",
    "                                 'q_explanation_avg_u': '../input/riiid-q-explanation-sum-avg/q_explanation_avg_u_train_arr.npy',\n",
    "                                 'q_explanation_sum_u': '../input/riiid-q-explanation-sum-avg/q_explanation_sum_u_train_arr.npy',\n",
    "                                 'avg_part_score_c': '../input/riiid-part-score-stats-avg-std/avg_part_score_c_train_arr.npy',\n",
    "                                 'std_part_score_c': '../input/riiid-part-score-stats-avg-std/std_part_score_c_train_arr.npy',\n",
    "                                 'time_since_last_sum_u': '../input/riiid-time-since-last-sum-avg/time_since_last_sum_u_train_arr.npy',\n",
    "                                 'time_since_last_avg_u': '../input/riiid-time-since-last-sum-avg/time_since_last_avg_u_train_arr.npy',\n",
    "                                 'curr_cont_score_avg_u': '../input/riiid-curr-cont-stats-arr-pkl/curr_cont_score_avg_u_train_arr.npy',\n",
    "                                 'curr_cont_score_sum_u': '../input/riiid-curr-cont-stats-arr-pkl/curr_cont_score_sum_u_train_arr.npy',\n",
    "                                 'curr_cont_tackled_q_count_u': '../input/riiid-curr-cont-stats-arr-pkl/curr_cont_tackled_q_count_u_train_arr.npy',\n",
    "                                 'avg_question_elapsed_time_c_right': '../input/riiid-q-elps-time-score-avg-arrs/avg_question_elapsed_time_c_right_train_arr.npy',\n",
    "                                 'avg_question_elapsed_time_c_wrong': '../input/riiid-q-elps-time-score-avg-arrs/avg_question_elapsed_time_c_wrong_train_arr.npy',\n",
    "                                 'avg_time_since_last_c_right': '../input/riiid-q-time-lag-score-avg-arrs/avg_time_since_last_c_right_train_arr.npy',\n",
    "                                 'avg_time_since_last_c_wrong': '../input/riiid-q-time-lag-score-avg-arrs/avg_time_since_last_c_wrong_train_arr.npy',\n",
    "                                 'diff_avg_question_elapsed_time_c_right': '../input/riiid-q-elps-time-score-diff-arrs/diff_avg_question_elapsed_time_c_right_train_arr.npy',\n",
    "                                 'diff_avg_question_elapsed_time_c_wrong': '../input/riiid-q-elps-time-score-diff-arrs/diff_avg_question_elapsed_time_c_wrong_train_arr.npy',\n",
    "                                 'diff_avg_time_since_last_c_right': '../input/riiid-q-time-lag-score-diff-arrs/diff_avg_time_since_last_c_right_train_arr.npy',\n",
    "                                 'diff_avg_time_since_last_c_wrong': '../input/riiid-q-time-lag-score-diff-arrs/diff_avg_time_since_last_c_wrong_train_arr.npy',\n",
    "                                 'ratio_avg_time_since_last_c_right': '../input/riiid-q-elps-time-right-ratios-arrs/ratio_avg_time_since_last_c_right_train_arr.npy',\n",
    "                                 'ratio_avg_time_since_last_c_wrong': '../input/riiid-q-elps-time-wrong-ratios-arrs/ratio_avg_time_since_last_c_wrong_train_arr.npy',\n",
    "                                 'ratio_diff_avg_question_elapsed_time_c_right': '../input/riiid-q-time-lag-right-ratios-arrs/ratio_diff_avg_question_elapsed_time_c_right_train_arr.npy',\n",
    "                                 'ratio_diff_avg_question_elapsed_time_c_wrong': '../input/riiid-q-time-lag-wrong-ratios-arrs/ratio_diff_avg_question_elapsed_time_c_wrong_train_arr.npy',\n",
    "                                 'difficulty_level': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_level_train_arr.npy',\n",
    "                                 'difficulty_level_avg_score_c': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_level_c_avg_train_arr.npy',\n",
    "                                 'difficulty_level_std_score_c': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_level_c_std_train_arr.npy',\n",
    "                                 'sum_score_q_level': '../input/riiid-u-score-difficulty-lvl-corr/sum_score_q_level_train_arr.npy',\n",
    "                                 'q_count_level_u': '../input/riiid-u-score-difficulty-lvl-corr/q_count_level_train_arr.npy',\n",
    "                                 'avg_score_q_level_u': '../input/riiid-u-score-difficulty-lvl-corr/avg_score_q_level_train_arr.npy',\n",
    "                                 'avg_q_elapsed_time_lvl': '../input/riiid-q-elpsd-time-dffclty-lvl-arr-dicts/avg_q_elapsed_time_level_train_arr.npy',\n",
    "                                 'diff_avg_q_elapsed_time_lvl': '../input/riiid-q-elpsd-time-dffclty-lvl-arr-dicts/diff_avg_q_elapsed_time_per_lvl_train_arr.npy',\n",
    "                                 'ratio_diff_avg_q_elapsed_time_lvl': '../input/riiid-q-elpsd-time-dffclty-lvl-arr-dicts/ratio_avg_q_elapsed_time_per_lvl_train_arr.npy',\n",
    "                                 'diff_avg_score_q_lvl': '../input/riiid-u-score-difficulty-lvl/diff_avg_score_q_lvl_train_arr.npy',\n",
    "                                 'ratio_diff_avg_score_q_lvl': '../input/riiid-u-score-difficulty-lvl/ratio_diff_avg_score_q_lvl_train_arr.npy',\n",
    "                                 'ratio_diff_avg_score_u_c': '../input/riiid-ratio-diff-score-u-c/ratio_diff_avg_score_u_c_train_arr.npy',\n",
    "                                 'num_answers_q': '../input/riiid-question-meta-stats/num_answers_q_train_arr.npy',\n",
    "                                 'q_count_trainset_c': '../input/riiid-question-meta-stats/q_count_trainset_c_train_arr.npy',\n",
    "                                 'tag_cluster': '../input/riiid-tags-cluster-dict-arrs/q_tags_clusters_train_arr.npy',\n",
    "                                 'avg_score_u_cluster_start': '../input/riiid-user-start-cluster/avg_score_u_cluster_start_train_arr.npy',\n",
    "                                 'first_answ': '../input/riiid-user-start-cluster/first_answ_train_arr.npy',\n",
    "                                 'u_cluster_start': '../input/riiid-user-start-cluster/u_cluster_start_train_arr.npy',\n",
    "                                 'avg_score_u_curr_day': '../input/riiid-fe-days-weeks-months/avg_score_u_curr_day_train_arr.npy',\n",
    "                                 'avg_score_u_curr_week': '../input/riiid-fe-days-weeks-months/avg_score_u_curr_week_train_arr.npy',\n",
    "                                 'avg_score_u_curr_month': '../input/riiid-fe-days-weeks-months/avg_score_u_curr_month_train_arr.npy',\n",
    "                                 'avg_score_u_last_day': '../input/riiid-fe-days-weeks-months/avg_score_u_last_day_train_arr.npy',\n",
    "                                 'avg_score_u_last_week': '../input/riiid-fe-days-weeks-months/avg_score_u_last_week_train_arr.npy',\n",
    "                                 'avg_score_u_last_month': '../input/riiid-fe-days-weeks-months/avg_score_u_last_month_train_arr.npy',\n",
    "                                 'curr_day_count_u': '../input/riiid-fe-days-weeks-months/curr_day_count_u_train_arr.npy',\n",
    "                                 'curr_week_count_u': '../input/riiid-fe-days-weeks-months/curr_week_count_u_train_arr.npy',\n",
    "                                 'curr_month_count_u': '../input/riiid-fe-days-weeks-months/curr_month_count_u_train_arr.npy',\n",
    "                                 'q_count_u_curr_day': '../input/riiid-fe-days-weeks-months/q_count_u_curr_day_train_arr.npy',\n",
    "                                 'q_count_u_curr_week': '../input/riiid-fe-days-weeks-months/q_count_u_curr_week_train_arr.npy',\n",
    "                                 'q_count_u_curr_month': '../input/riiid-fe-days-weeks-months/q_count_u_curr_month_train_arr.npy',\n",
    "                                 'q_count_u_last_day': '../input/riiid-fe-days-weeks-months/q_count_u_last_day_train_arr.npy',\n",
    "                                 'q_count_u_last_week': '../input/riiid-fe-days-weeks-months/q_count_u_last_week_train_arr.npy',\n",
    "                                 'q_count_u_last_month': '../input/riiid-fe-days-weeks-months/q_count_u_last_month_train_arr.npy',\n",
    "                                 'sum_score_u_curr_day': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_day_train_arr.npy',\n",
    "                                 'sum_score_u_curr_week': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_week_train_arr.npy',\n",
    "                                 'sum_score_u_curr_month': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_month_train_arr.npy',\n",
    "                                 'time_since_last': '../input/riiid-time-since-last-hist-3/time_since_last_train_arr.npy',\n",
    "                                 'time_since_last_2': '../input/riiid-time-since-last-hist-3/time_since_last_2_train_arr.npy',\n",
    "                                 'time_since_last_3': '../input/riiid-time-since-last-hist-3/time_since_last_3_train_arr.npy',\n",
    "                                 'time_since_last_right': '../input/riiid-time-since-last-right-wrong-corrected/time_since_last_right_train_arr.npy',\n",
    "                                 'time_since_last_wrong': '../input/riiid-time-since-last-right-wrong-corrected/time_since_last_wrong_train_arr.npy',\n",
    "                                 'curr_avg_q_elapsed_time': '../input/riiid-avg-q-elapsed-time-c/curr_avg_q_elapsed_time_train_arr.npy',\n",
    "                                 'curr_avg_q_is_explained': '../input/riiid-curr-avg-q-is-explained/curr_avg_q_is_explained_train_arr.npy',\n",
    "                                 'avg_score_u_part': '../input/riiid-u-score-part/avg_score_u_part_train_arr.npy',\n",
    "                                 'sum_score_u_part': '../input/riiid-u-score-part/sum_score_u_part_train_arr.npy',\n",
    "                                 'q_count_u_part': '../input/riiid-u-score-part/q_count_u_part_train_arr.npy',\n",
    "                                 'avg_prior_cont_time': '../input/riiid-prior-q-cont-elapsed-time/avg_prior_cont_time_train_arr.npy',\n",
    "                                 'avg_prior_q_elapsed_time': '../input/riiid-prior-q-cont-elapsed-time/avg_prior_q_elapsed_time_train_arr.npy',\n",
    "                                 'avg_prior_cont_time_part': '../input/riiid-prior-q-cont-elasped-time-part/avg_prior_cont_time_part_train_arr.npy',\n",
    "                                 'avg_prior_q_elapsed_time_part': '../input/riiid-prior-q-cont-elasped-time-part/avg_prior_q_elapsed_time_part_train_arr.npy',\n",
    "                                 'ratio_curr_by_avg_session_time': '../input/riiid-ratio-curr-by-avg-sess-time/ratio_curr_by_avg_session_time_train_arr.npy',\n",
    "                                 'avg_u_time_since_last': '../input/riiid-t-diff-u-avg-and-ratios/avg_u_time_since_last_train_arr.npy',\n",
    "                                 'ratio_tdiff_by_avg_tdiff_u': '../input/riiid-t-diff-u-avg-and-ratios/ratio_tdiff_by_avg_tdiff_u_train_arr.npy',\n",
    "                                 'answered_correctly': '../input/riiid-target-question-rows-arr/target_train_question_rows_arr.npy'\n",
    "                               }\n",
    "\n",
    "\n",
    "\n",
    "offline_features_valid_paths = { 'answered_correctly_avg_u': '../input/riiid-avg-score-u-corrected/avg_score_u_valid_arr.npy',\n",
    "                                 'answered_correctly_sum_u': '../input/riiid-avg-score-u-corrected/sum_score_u_valid_arr.npy',\n",
    "                                 'q_count_u': '../input/riiid-avg-score-u-corrected/q_count_u_valid_arr.npy',\n",
    "                                 'is_first_attempt': '../input/riiid-is-first-atpt-rows-q/is_first_attempt_rows_valid_q_full.npy',\n",
    "                                 'answered_correctly_avg_c': '../input/riiid-feats-rows-arr-01/avg_score_c_rows_valid_questions.npy',\n",
    "                                 'answered_correctly_std_c': '../input/riiid-feats-rows-arr-01/std_score_c_rows_valid_questions.npy',\n",
    "                                 'answered_correctly_median_c': '../input/riiid-feats-rows-arr-01/median_score_c_rows_valid_questions.npy',\n",
    "                                 'part': '../input/riiid-feats-rows-arr-01/part_rows_valid_questions.npy',\n",
    "                                 'prior_question_had_explanation': '../input/riiid-feats-rows-arr-01/prior_explanation_rows_valid_questions.npy',\n",
    "                                 'prior_question_elapsed_time': '../input/riiid-feats-rows-arr-01/prior_elapsed_time_rows_valid_questions.npy',\n",
    "                                 'l_count_u': '../input/riiid-count-u-lec-rows-q/count_u_lec_rows_valid_q_full.npy',\n",
    "                                 'l_part_count_u': '../input/riiid-lec-part-u-rows-q/cnt_lec_part_u_rows_valid_q_full.npy',\n",
    "                                 'session_num': '../input/riiid-session-basics-fe/session_num_valid_arr.npy' ,\n",
    "                                 'same_container_as_last': '../input/riiid-same-cont-as-last-arr/same_container_as_last_valid_arr.npy',\n",
    "                                 'last_container_sum_answ': '../input/riiid-last-cont-sum-answ-arr/last_container_sum_answ_valid_arr.npy',\n",
    "                                 'last_cont_q_count': '../input/riiid-last-cont-q-count-arr/last_container_q_count_valid_arr.npy' ,\n",
    "                                 'last_session_break_time': '../input/riiid-last-sess-break-time-pkl-arrs/last_session_break_time_valid_arr.npy',\n",
    "                                 'avg_break_time': '../input/riiid-avg-break-time-pkl-arr/avg_break_time_valid_arr.npy',\n",
    "                                 'avg_session_time': '../input/riiid-session-avg-time/session_avg_time_valid_arr.npy',\n",
    "                                 'avg_session_q_count': '../input/riiid-avg-session-q-count-arr/avg_session_q_count_valid_arr.npy',\n",
    "                                 'current_session_avg_score': '../input/riiid-curr-sess-avg-score-pkl-arr/current_session_avg_score_valid_arr.npy',\n",
    "                                 'same_session_as_last': '../input/riiid-session-basics-fe/same_session_as_last_valid_arr.npy',\n",
    "                                 'current_session_time': '../input/riiid-curr-session-time-arr-pkl/current_session_time_valid_arr.npy',\n",
    "                                 'current_session_q_count': '../input/riiid-curr-sess-q-count-arr-pkl/current_session_q_count_valid_arr.npy',\n",
    "                                 'avg_score_u_hist_100_75': '../input/riiid-hist-100-score-slope-clean/avg_score_u_hist_100_75_valid_arr.npy',\n",
    "                                 'avg_score_u_hist_75_50': '../input/riiid-hist-100-score-slope-clean/avg_score_u_hist_75_50_valid_arr.npy',\n",
    "                                 'avg_score_u_hist_50_25': '../input/riiid-hist-100-score-slope-clean/avg_score_u_hist_50_25_valid_arr.npy',\n",
    "                                 'avg_score_u_hist_25_0': '../input/riiid-hist-100-score-slope-clean/avg_score_u_hist_25_0_valid_arr.npy',\n",
    "                                 'hist_score_diff_u_100_50': '../input/riiid-hist-100-score-slope-clean/hist_score_diff_u_100_50_valid_arr.npy',\n",
    "                                 'hist_score_diff_u_75_25': '../input/riiid-hist-100-score-slope-clean/hist_score_diff_u_75_25_valid_arr.npy',\n",
    "                                 'hist_score_diff_u_50_0': '../input/riiid-hist-100-score-slope-clean/hist_score_diff_u_50_0_valid_arr.npy',\n",
    "                                 'hist_100_score_slope_u': '../input/riiid-hist-100-score-slope-clean/hist_100_score_slope_u_valid_arr.npy',\n",
    "                                 'current_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/current_right_answ_streak_valid_arr.npy',\n",
    "                                 'max_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/max_right_answ_streak_valid_arr.npy',\n",
    "                                 'hist_1_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/hist_1_right_answ_streak_valid_arr.npy',\n",
    "                                 'hist_2_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/hist_2_right_answ_streak_valid_arr.npy',\n",
    "                                 'hist_3_right_answ_streak': '../input/riiid-answ-streak-u-pkl-arr/hist_3_right_answ_streak_valid_arr.npy',\n",
    "                                 'avg_right_answ_streak_hist_3': '../input/riiid-answ-streak-u-pkl-arr/avg_right_answ_streak_hist_3_valid_arr.npy',\n",
    "                                 'first_try_success_count_u': '../input/riiid-first-try-success-count-u-arr-pkl/first_try_success_count_u_valid_arr.npy',\n",
    "                                 'unique_count_attempted_q_u': '../input/riiid-cnt-attempted-q-arr-pkl/count_attempted_q_u_val_arr.npy',\n",
    "                                 'avg_score_t': '../input/riiid-fe-tags-score-stats/avg_score_t_valid_arr.npy',\n",
    "                                 'max_avg_score_t': '../input/riiid-fe-tags-score-stats/max_avg_score_t_valid_arr.npy',\n",
    "                                 'min_avg_score_t': '../input/riiid-fe-tags-score-stats/min_avg_score_t_valid_arr.npy',\n",
    "                                 'std_avg_score_t': '../input/riiid-fe-tags-score-stats/std_avg_score_t_valid_arr.npy',\n",
    "                                 'avg_tag_freq': '../input/riiid-fe-tags-freq-stats/tag_freq_feats_arrs/avg_tag_frequency_valid_arr.npy',\n",
    "                                 'max_tag_freq': '../input/riiid-fe-tags-freq-stats/tag_freq_feats_arrs/max_tag_frequency_valid_arr.npy',\n",
    "                                 'min_tag_freq': '../input/riiid-fe-tags-freq-stats/tag_freq_feats_arrs/min_tag_frequency_valid_arr.npy',\n",
    "                                 'std_tag_freq': '../input/riiid-fe-tags-freq-stats/tag_freq_feats_arrs/std_tag_frequency_valid_arr.npy',\n",
    "                                 'tag_count_q': '../input/riiid-q-tag-count-dict/tag_count_q_valid_arr.npy',\n",
    "                                 'avg_question_elapsed_time_c': '../input/riiid-question-time-stats/avg_question_elapsed_time_c_valid_arr.npy',\n",
    "                                 'std_question_elapsed_time_c': '../input/riiid-question-time-stats/std_question_elapsed_time_c_valid_arr.npy',\n",
    "                                 'avg_time_since_last_c': '../input/riiid-question-time-stats/avg_time_since_last_c_valid_arr.npy',\n",
    "                                 'std_time_since_last_c': '../input/riiid-question-time-stats/std_time_since_last_c_valid_arr.npy',\n",
    "                                 'diff_avg_question_elapsed_time_c': '../input/riiid-question-time-stats/diff_avg_question_elapsed_time_c_valid_arr.npy',\n",
    "                                 'diff_avg_time_since_last_c': '../input/riiid-question-time-stats/diff_avg_time_since_last_c_valid_arr.npy',\n",
    "                                 'ratio_diff_avg_question_elapsed_time_c': '../input/riiid-question-time-stats/ratio_diff_avg_question_elapsed_time_c_valid_arr.npy',\n",
    "                                 'ratio_diff_avg_time_since_last_c': '../input/riiid-question-time-stats/ratio_diff_avg_time_since_last_c_valid_arr.npy',\n",
    "                                 'q_explanation_avg_u': '../input/riiid-q-explanation-sum-avg/q_explanation_avg_u_valid_arr.npy',\n",
    "                                 'q_explanation_sum_u': '../input/riiid-q-explanation-sum-avg/q_explanation_sum_u_valid_arr.npy',\n",
    "                                 'avg_part_score_c': '../input/riiid-part-score-stats-avg-std/avg_part_score_c_valid_arr.npy',\n",
    "                                 'std_part_score_c': '../input/riiid-part-score-stats-avg-std/std_part_score_c_valid_arr.npy',\n",
    "                                 'time_since_last_sum_u': '../input/riiid-time-since-last-sum-avg/time_since_last_sum_u_valid_arr.npy',\n",
    "                                 'time_since_last_avg_u': '../input/riiid-time-since-last-sum-avg/time_since_last_avg_u_valid_arr.npy',\n",
    "                                 'curr_cont_score_avg_u': '../input/riiid-curr-cont-stats-arr-pkl/curr_cont_score_avg_u_valid_arr.npy',\n",
    "                                 'curr_cont_score_sum_u': '../input/riiid-curr-cont-stats-arr-pkl/curr_cont_score_sum_u_valid_arr.npy',\n",
    "                                 'curr_cont_tackled_q_count_u': '../input/riiid-curr-cont-stats-arr-pkl/curr_cont_tackled_q_count_u_valid_arr.npy',\n",
    "                                 'avg_question_elapsed_time_c_right': '../input/riiid-q-elps-time-score-avg-arrs/avg_question_elapsed_time_c_right_valid_arr.npy',\n",
    "                                 'avg_question_elapsed_time_c_wrong': '../input/riiid-q-elps-time-score-avg-arrs/avg_question_elapsed_time_c_wrong_valid_arr.npy',\n",
    "                                 'avg_time_since_last_c_right': '../input/riiid-q-time-lag-score-avg-arrs/avg_time_since_last_c_right_valid_arr.npy',\n",
    "                                 'avg_time_since_last_c_wrong': '../input/riiid-q-time-lag-score-avg-arrs/avg_time_since_last_c_wrong_valid_arr.npy',\n",
    "                                 'diff_avg_question_elapsed_time_c_right': '../input/riiid-q-elps-time-score-diff-arrs/diff_avg_question_elapsed_time_c_right_valid_arr.npy',\n",
    "                                 'diff_avg_question_elapsed_time_c_wrong': '../input/riiid-q-elps-time-score-diff-arrs/diff_avg_question_elapsed_time_c_wrong_valid_arr.npy',\n",
    "                                 'diff_avg_time_since_last_c_right': '../input/riiid-q-time-lag-score-diff-arrs/diff_avg_time_since_last_c_right_valid_arr.npy',\n",
    "                                 'diff_avg_time_since_last_c_wrong': '../input/riiid-q-time-lag-score-diff-arrs/diff_avg_time_since_last_c_wrong_valid_arr.npy',\n",
    "                                 'ratio_avg_time_since_last_c_right': '../input/riiid-q-elps-time-right-ratios-arrs/ratio_avg_time_since_last_c_right_valid_arr.npy',\n",
    "                                 'ratio_avg_time_since_last_c_wrong': '../input/riiid-q-elps-time-wrong-ratios-arrs/ratio_avg_time_since_last_c_wrong_valid_arr.npy',\n",
    "                                 'ratio_diff_avg_question_elapsed_time_c_right': '../input/riiid-q-time-lag-right-ratios-arrs/ratio_diff_avg_question_elapsed_time_c_right_valid_arr.npy',\n",
    "                                 'ratio_diff_avg_question_elapsed_time_c_wrong': '../input/riiid-q-time-lag-wrong-ratios-arrs/ratio_diff_avg_question_elapsed_time_c_wrong_valid_arr.npy',\n",
    "                                 'difficulty_level': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_level_valid_arr.npy',\n",
    "                                 'difficulty_level_avg_score_c': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_level_c_avg_valid_arr.npy',\n",
    "                                 'difficulty_level_std_score_c': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_level_c_std_valid_arr.npy',\n",
    "                                 'sum_score_q_level': '../input/riiid-u-score-difficulty-lvl-corr/sum_score_q_level_valid_arr.npy',\n",
    "                                 'q_count_level_u': '../input/riiid-u-score-difficulty-lvl-corr/q_count_level_valid_arr.npy',\n",
    "                                 'avg_score_q_level_u': '../input/riiid-u-score-difficulty-lvl-corr/avg_score_q_level_valid_arr.npy',\n",
    "                                 'avg_q_elapsed_time_lvl': '../input/riiid-q-elpsd-time-dffclty-lvl-arr-dicts/avg_q_elapsed_time_level_valid_arr.npy',\n",
    "                                 'diff_avg_q_elapsed_time_lvl': '../input/riiid-q-elpsd-time-dffclty-lvl-arr-dicts/diff_avg_q_elapsed_time_per_lvl_valid_arr.npy',\n",
    "                                 'ratio_diff_avg_q_elapsed_time_lvl': '../input/riiid-q-elpsd-time-dffclty-lvl-arr-dicts/ratio_avg_q_elapsed_time_per_lvl_valid_arr.npy',\n",
    "                                 'diff_avg_score_q_lvl': '../input/riiid-u-score-difficulty-lvl/diff_avg_score_q_lvl_valid_arr.npy',\n",
    "                                 'ratio_diff_avg_score_q_lvl': '../input/riiid-u-score-difficulty-lvl/ratio_diff_avg_score_q_lvl_valid_arr.npy',\n",
    "                                 'ratio_diff_avg_score_u_c': '../input/riiid-ratio-diff-score-u-c/ratio_diff_avg_score_u_c_valid_arr.npy', \n",
    "                                 'num_answers_q': '../input/riiid-question-meta-stats/num_answers_q_valid_arr.npy',\n",
    "                                 'q_count_trainset_c': '../input/riiid-question-meta-stats/q_count_trainset_c_valid_arr.npy',\n",
    "                                 'tag_cluster': '../input/riiid-tags-cluster-dict-arrs/q_tags_clusters_valid_arr.npy',\n",
    "                                 'avg_score_u_cluster_start': '../input/riiid-user-start-cluster/avg_score_u_cluster_start_valid_arr.npy',\n",
    "                                 'first_answ': '../input/riiid-user-start-cluster/first_answ_valid_arr.npy',\n",
    "                                 'u_cluster_start': '../input/riiid-user-start-cluster/u_cluster_start_valid_arr.npy',\n",
    "                                 'avg_score_u_curr_day': '../input/riiid-fe-days-weeks-months/avg_score_u_curr_day_valid_arr.npy',\n",
    "                                 'avg_score_u_curr_week': '../input/riiid-fe-days-weeks-months/avg_score_u_curr_week_valid_arr.npy',\n",
    "                                 'avg_score_u_curr_month': '../input/riiid-fe-days-weeks-months/avg_score_u_curr_month_valid_arr.npy',\n",
    "                                 'avg_score_u_last_day': '../input/riiid-fe-days-weeks-months/avg_score_u_last_day_valid_arr.npy',\n",
    "                                 'avg_score_u_last_week': '../input/riiid-fe-days-weeks-months/avg_score_u_last_week_valid_arr.npy',\n",
    "                                 'avg_score_u_last_month': '../input/riiid-fe-days-weeks-months/avg_score_u_last_month_valid_arr.npy',\n",
    "                                 'curr_day_count_u': '../input/riiid-fe-days-weeks-months/curr_day_count_u_valid_arr.npy',\n",
    "                                 'curr_week_count_u': '../input/riiid-fe-days-weeks-months/curr_week_count_u_valid_arr.npy',\n",
    "                                 'curr_month_count_u': '../input/riiid-fe-days-weeks-months/curr_month_count_u_valid_arr.npy',\n",
    "                                 'q_count_u_curr_day': '../input/riiid-fe-days-weeks-months/q_count_u_curr_day_valid_arr.npy',\n",
    "                                 'q_count_u_curr_week': '../input/riiid-fe-days-weeks-months/q_count_u_curr_week_valid_arr.npy',\n",
    "                                 'q_count_u_curr_month': '../input/riiid-fe-days-weeks-months/q_count_u_curr_month_valid_arr.npy',\n",
    "                                 'q_count_u_last_day': '../input/riiid-fe-days-weeks-months/q_count_u_last_day_valid_arr.npy',\n",
    "                                 'q_count_u_last_week': '../input/riiid-fe-days-weeks-months/q_count_u_last_week_valid_arr.npy',\n",
    "                                 'q_count_u_last_month': '../input/riiid-fe-days-weeks-months/q_count_u_last_month_valid_arr.npy',\n",
    "                                 'sum_score_u_curr_day': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_day_valid_arr.npy',\n",
    "                                 'sum_score_u_curr_week': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_week_valid_arr.npy',\n",
    "                                 'sum_score_u_curr_month': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_month_valid_arr.npy',\n",
    "                                 'time_since_last': '../input/riiid-time-since-last-hist-3/time_since_last_valid_arr.npy',\n",
    "                                 'time_since_last_2': '../input/riiid-time-since-last-hist-3/time_since_last_2_valid_arr.npy',\n",
    "                                 'time_since_last_3': '../input/riiid-time-since-last-hist-3/time_since_last_3_valid_arr.npy',\n",
    "                                 'time_since_last_right': '../input/riiid-time-since-last-right-wrong-corrected/time_since_last_right_valid_arr.npy',\n",
    "                                 'time_since_last_wrong': '../input/riiid-time-since-last-right-wrong-corrected/time_since_last_wrong_valid_arr.npy',\n",
    "                                 'curr_avg_q_elapsed_time': '../input/riiid-avg-q-elapsed-time-c/curr_avg_q_elapsed_time_valid_arr.npy',\n",
    "                                 'curr_avg_q_is_explained': '../input/riiid-curr-avg-q-is-explained/curr_avg_q_is_explained_valid_arr.npy',\n",
    "                                 'avg_score_u_part': '../input/riiid-u-score-part/avg_score_u_part_valid_arr.npy',\n",
    "                                 'sum_score_u_part': '../input/riiid-u-score-part/sum_score_u_part_valid_arr.npy',\n",
    "                                 'q_count_u_part': '../input/riiid-u-score-part/q_count_u_part_valid_arr.npy',\n",
    "                                 'avg_prior_cont_time': '../input/riiid-prior-q-cont-elapsed-time/avg_prior_cont_time_valid_arr.npy',\n",
    "                                 'avg_prior_q_elapsed_time': '../input/riiid-prior-q-cont-elapsed-time/avg_prior_q_elapsed_time_valid_arr.npy',\n",
    "                                 'avg_prior_cont_time_part': '../input/riiid-prior-q-cont-elasped-time-part/avg_prior_cont_time_part_valid_arr.npy',\n",
    "                                 'avg_prior_q_elapsed_time_part': '../input/riiid-prior-q-cont-elasped-time-part/avg_prior_q_elapsed_time_part_valid_arr.npy',\n",
    "                                 'ratio_curr_by_avg_session_time': '../input/riiid-ratio-curr-by-avg-sess-time/ratio_curr_by_avg_session_time_valid_arr.npy',\n",
    "                                 'avg_u_time_since_last': '../input/riiid-t-diff-u-avg-and-ratios/avg_u_time_since_last_valid_arr.npy',\n",
    "                                 'ratio_tdiff_by_avg_tdiff_u': '../input/riiid-t-diff-u-avg-and-ratios/ratio_tdiff_by_avg_tdiff_u_valid_arr.npy',\n",
    "                                 'answered_correctly': '../input/riiid-target-question-rows-arr/target_valid_question_rows_arr.npy'\n",
    "                               }\n",
    "\n",
    "\n",
    "# Raw df (Q or Q+L) paths :\n",
    "#---------------------------\n",
    "# -> Q : online features (in data_preprocessing)\n",
    "# -> Q+L : for local-CV (target in iter_test API)\n",
    "\n",
    "MODE_10M_ROWS = True\n",
    "\n",
    "\n",
    "if MODE_10M_ROWS :\n",
    "    # 10M\n",
    "    df_questions_kfold_paths = { 'fullset': {'train': '../input/riiid-df-q-kfold/raw_df_kfolds_10m/train_cv0_q_df_10M',\n",
    "                                             'valid': '../input/riiid-df-q-kfold/raw_df_kfolds/valid_cv0_q_df'},\n",
    "                                 'cv0': {'train': '../input/riiid-df-q-kfold/raw_df_kfolds_10m/train_cv0_q_df_10M',\n",
    "                                         'valid': '../input/riiid-df-q-kfold/raw_df_kfolds/valid_cv0_q_df'},\n",
    "                               }\n",
    "else:\n",
    "    # 5M\n",
    "    df_questions_kfold_paths = { 'fullset': {'train': '../input/riiid-df-q-kfold/raw_df_kfolds/train_cv0_q_df',\n",
    "                                             'valid': '../input/riiid-df-q-kfold/raw_df_kfolds/valid_cv0_q_df'},\n",
    "                                 'cv0': {'train': '../input/riiid-df-q-kfold/raw_df_kfolds/train_cv0_q_df',\n",
    "                                         'valid': '../input/riiid-df-q-kfold/raw_df_kfolds/valid_cv0_q_df'},\n",
    "                               }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "df_questions_lectures_kfold_paths = { 'fullset': {'train': '../input/riiid-df-q-kfold/raw_df_kfolds/train_cv0_ql_df',\n",
    "                                                  'valid': '../input/riiid-df-q-kfold/raw_df_kfolds/valid_cv0_ql_df'},\n",
    "                                      'cv0': {'train': '../input/riiid-df-q-kfold/raw_df_kfolds/train_cv0_ql_df',\n",
    "                                              'valid': '../input/riiid-df-q-kfold/raw_df_kfolds/valid_cv0_ql_df'},\n",
    "                                    }\n",
    "\n",
    "\n",
    "\n",
    "# Offline preprocessed state dicts\n",
    "\n",
    "\n",
    "# User states : K-fold dicts\n",
    "#-------------------------------\n",
    "\n",
    "\n",
    "users_feature_states_paths = {\n",
    "                        'fullset': {\n",
    "                              'answered_correctly_sum': '../input/riiid-avg-score-u-corrected/sum_score_u_fullset_dict.pkl',\n",
    "                              'q_count': '../input/riiid-avg-score-u-corrected/q_count_u_fullset_dict.pkl',\n",
    "                              'l_count': '../input/riiid-l-count-u-kfolds/l_count_u_dict_full.pkl',\n",
    "                              'q_attempted': '../input/riiid-attempted-q-u-kfolds/attempted_q_u_dict_full.pkl',\n",
    "                              'curr_cont_id': '../input/riiid-time-sin-last-cont-kfold/curr_cont_u_dict_full.pkl',\n",
    "                              'curr_timestamp': '../input/riiid-time-sin-last-cont-kfold/curr_time_u_dict_full.pkl',\n",
    "                              'last_timestamp': '../input/riiid-time-sin-last-cont-kfold/last_time_u_dict_full.pkl',\n",
    "                              'last_t_diff': '../input/riiid-time-related-kfolds/last_t_diff_per_u_dict_full.pkl',\n",
    "                              'session_num': '../input/riiid-session-related-kfolds/session_num_u_dict_full.pkl',\n",
    "                              'last_cont_sum_answ': '../input/riiid-container-related-kfolds/last_cont_sum_answ_u_dict_full.pkl',\n",
    "                              'curr_cont_sum_answ': '../input/riiid-container-related-kfolds/curr_cont_sum_answ_u_dict_full.pkl',\n",
    "                              'curr_cont_q_count': '../input/riiid-container-related-kfolds/curr_cont_q_count_u_dict_full.pkl',\n",
    "                              'last_cont_q_count': '../input/riiid-container-related-kfolds/last_cont_q_count_u_dict_full.pkl',\n",
    "                              'last_session_break_time': '../input/riiid-session-related-kfolds/last_session_break_time_u_dict_full.pkl',\n",
    "                              'sum_session_break_time': '../input/riiid-session-related-kfolds/session_break_time_sum_u_dict_full.pkl',\n",
    "                              'curr_session_sum_score': '../input/riiid-session-related-kfolds/curr_session_sum_scores_u_dict_full.pkl',\n",
    "                              'first_cont_session': '../input/riiid-session-related-kfolds/first_cont_session_u_dict_full.pkl',\n",
    "                              'time_start_session': '../input/riiid-session-related-kfolds/time_start_session_u_dict_full.pkl',\n",
    "                              'q_count_start_session': '../input/riiid-session-related-kfolds/q_count_start_session_u_dict_full.pkl',\n",
    "                              'hist_100_answ': '../input/riiid-hist-100-to-10-score-deque/hist_100_bin_u_fullset_dict.pkl',\n",
    "                              'curr_answ_streak': '../input/riiid-answ-streak-kfolds/curr_answ_streak_u_dict_full.pkl',\n",
    "                              'max_answ_streak': '../input/riiid-answ-streak-kfolds/max_answ_streak_u_dict_full.pkl',\n",
    "                              'hist_3_answ_streak': '../input/riiid-answ-streak-kfolds/hist_3_answ_streak_u_dict_full.pkl',\n",
    "                              'first_try_success_count': '../input/riiid-first-try-success-count-u-arr-pkl/first_try_success_count_u_fullset_dict.pkl',\n",
    "                              'unique_count_attempted_q': '../input/riiid-cnt-attempted-q-arr-pkl/count_attempted_q_u_fullset_dict.pkl',\n",
    "                              'q_explanation_sum': '../input/riiid-q-explanation-sum-avg/q_explanation_sum_u_fullset_dict.pkl',\n",
    "                              'time_since_last_sum_u': '../input/riiid-time-since-last-sum-avg/time_since_last_sum_u_fullset_dict.pkl',\n",
    "                              'difficulty_lvl_count_u': '../input/riiid-u-score-difficulty-lvl-corr/cv_difficulty_lvl_count_u_fullset_dict.pkl',\n",
    "                              'difficulty_lvl_sum_u': '../input/riiid-u-score-difficulty-lvl-corr/cv_difficulty_lvl_sum_u_fullset_dict.pkl',\n",
    "                              'u_first_answ': '../input/riiid-user-start-cluster/u_first_answ_fullset_dict.pkl',\n",
    "                              'u_first_q_cluster': '../input/riiid-user-start-cluster/u_first_q_cluster_fullset_dict.pkl',\n",
    "                              'avg_score_u_last_day': '../input/riiid-fe-days-weeks-months/avg_score_u_last_day_fullset_dict.pkl',\n",
    "                              'avg_score_u_last_week': '../input/riiid-fe-days-weeks-months/avg_score_u_last_week_fullset_dict.pkl',\n",
    "                              'avg_score_u_last_month': '../input/riiid-fe-days-weeks-months/avg_score_u_last_month_fullset_dict.pkl',\n",
    "                              'curr_day_count_u': '../input/riiid-fe-days-weeks-months/curr_day_count_u_fullset_dict.pkl',\n",
    "                              'curr_week_count_u': '../input/riiid-fe-days-weeks-months/curr_week_count_u_fullset_dict.pkl',\n",
    "                              'curr_month_count_u': '../input/riiid-fe-days-weeks-months/curr_month_count_u_fullset_dict.pkl',\n",
    "                              'q_count_u_curr_day': '../input/riiid-fe-days-weeks-months/q_count_u_curr_day_fullset_dict.pkl',\n",
    "                              'q_count_u_curr_week': '../input/riiid-fe-days-weeks-months/q_count_u_curr_week_fullset_dict.pkl',\n",
    "                              'q_count_u_curr_month': '../input/riiid-fe-days-weeks-months/q_count_u_curr_month_fullset_dict.pkl',\n",
    "                              'q_count_u_last_day': '../input/riiid-fe-days-weeks-months/q_count_u_last_day_fullset_dict.pkl',\n",
    "                              'q_count_u_last_week': '../input/riiid-fe-days-weeks-months/q_count_u_last_week_fullset_dict.pkl',\n",
    "                              'q_count_u_last_month': '../input/riiid-fe-days-weeks-months/q_count_u_last_month_fullset_dict.pkl',\n",
    "                              'sum_score_u_curr_day': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_day_fullset_dict.pkl',\n",
    "                              'sum_score_u_curr_week': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_week_fullset_dict.pkl',\n",
    "                              'sum_score_u_curr_month': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_month_fullset_dict.pkl',\n",
    "                              'time_since_last_hist_3': '../input/riiid-time-since-last-hist-3/time_since_last_hist_3_fullset_dict.pkl',\n",
    "                              'last_time_right': '../input/riiid-time-since-last-right-wrong-corrected/time_last_right_fullset_dict.pkl',\n",
    "                              'last_time_wrong': '../input/riiid-time-since-last-right-wrong-corrected/time_last_wrong_fullset_dict.pkl',\n",
    "                              'q_count_u_part': '../input/riiid-u-score-part/q_count_u_part_fullset_dict.pkl',\n",
    "                              'sum_score_u_part': '../input/riiid-u-score-part/sum_score_u_part_fullset_dict.pkl',\n",
    "                              'sum_prior_q_elapsed_time': '../input/riiid-prior-q-cont-elapsed-time/sum_prior_q_elapsed_time_fullset_dict.pkl',\n",
    "                              'cont_count_part': '../input/riiid-prior-q-cont-elasped-time-part/cont_count_part_fullset_dict.pkl',\n",
    "                              'last_cont_part': '../input/riiid-prior-q-cont-elasped-time-part/last_cont_part_fullset_dict.pkl', \n",
    "                              'sum_prior_q_elapsed_time_part': '../input/riiid-prior-q-cont-elasped-time-part/sum_prior_q_elapsed_time_part_fullset_dict.pkl',\n",
    "                              'sum_time_since_last_u': '../input/riiid-t-diff-u-avg-and-ratios/sum_time_since_last_u_fullset_dict.pkl',\n",
    "                             },\n",
    "                        'cv0': {\n",
    "                              'answered_correctly_sum': '../input/riiid-avg-score-u-corrected/sum_score_u_cv0_dict.pkl',\n",
    "                              'q_count': '../input/riiid-avg-score-u-corrected/q_count_u_cv0_dict.pkl',\n",
    "                              'l_count': '../input/riiid-l-count-u-kfolds/l_count_u_dict_cv0.pkl',\n",
    "                              'q_attempted': '../input/riiid-attempted-q-u-kfolds/attempted_q_u_dict_cv0.pkl',\n",
    "                              'curr_cont_id': '../input/riiid-time-sin-last-cont-kfold/curr_cont_u_dict_cv0.pkl',\n",
    "                              'curr_timestamp': '../input/riiid-time-sin-last-cont-kfold/curr_time_u_dict_cv0.pkl',\n",
    "                              'last_timestamp': '../input/riiid-time-sin-last-cont-kfold/last_time_u_dict_cv0.pkl',\n",
    "                              'last_t_diff': '../input/riiid-time-related-kfolds/last_t_diff_per_u_dict_cv0.pkl',\n",
    "                              'session_num': '../input/riiid-session-related-kfolds/session_num_u_dict_cv0.pkl',\n",
    "                              'last_cont_sum_answ': '../input/riiid-container-related-kfolds/last_cont_sum_answ_u_dict_cv0.pkl',\n",
    "                              'curr_cont_sum_answ': '../input/riiid-container-related-kfolds/curr_cont_sum_answ_u_dict_cv0.pkl',\n",
    "                              'curr_cont_q_count': '../input/riiid-container-related-kfolds/curr_cont_q_count_u_dict_cv0.pkl',\n",
    "                              'last_cont_q_count': '../input/riiid-container-related-kfolds/last_cont_q_count_u_dict_cv0.pkl',\n",
    "                              'last_session_break_time': '../input/riiid-session-related-kfolds/last_session_break_time_u_dict_cv0.pkl',\n",
    "                              'sum_session_break_time': '../input/riiid-session-related-kfolds/session_break_time_sum_u_dict_cv0.pkl',\n",
    "                              'curr_session_sum_score': '../input/riiid-session-related-kfolds/curr_session_sum_scores_u_dict_cv0.pkl',\n",
    "                              'first_cont_session': '../input/riiid-session-related-kfolds/first_cont_session_u_dict_cv0.pkl',\n",
    "                              'time_start_session': '../input/riiid-session-related-kfolds/time_start_session_u_dict_cv0.pkl',\n",
    "                              'q_count_start_session': '../input/riiid-session-related-kfolds/q_count_start_session_u_dict_cv0.pkl',\n",
    "                              'hist_100_answ': '../input/riiid-hist-100-to-10-score-deque/hist_100_bin_u_cv0_dict.pkl',\n",
    "                              'curr_answ_streak': '../input/riiid-answ-streak-kfolds/curr_answ_streak_u_dict_cv0.pkl',\n",
    "                              'max_answ_streak': '../input/riiid-answ-streak-kfolds/max_answ_streak_u_dict_cv0.pkl',\n",
    "                              'hist_3_answ_streak': '../input/riiid-answ-streak-kfolds/hist_3_answ_streak_u_dict_cv0.pkl',\n",
    "                              'first_try_success_count': '../input/riiid-first-try-success-count-u-arr-pkl/first_try_success_count_u_cv0_dict.pkl',\n",
    "                              'unique_count_attempted_q': '../input/riiid-cnt-attempted-q-arr-pkl/count_attempted_q_u_cv0_dict.pkl',\n",
    "                              'q_explanation_sum': '../input/riiid-q-explanation-sum-avg/q_explanation_sum_u_cv0_dict.pkl',\n",
    "                              'time_since_last_sum_u': '../input/riiid-time-since-last-sum-avg/time_since_last_sum_u_cv0_dict.pkl',\n",
    "                              'difficulty_lvl_count_u': '../input/riiid-u-score-difficulty-lvl-corr/cv_difficulty_lvl_count_u_cv0_dict.pkl',\n",
    "                              'difficulty_lvl_sum_u': '../input/riiid-u-score-difficulty-lvl-corr/cv_difficulty_lvl_sum_u_cv0_dict.pkl',\n",
    "                              'u_first_answ': '../input/riiid-user-start-cluster/u_first_answ_cv0_dict.pkl',\n",
    "                              'u_first_q_cluster': '../input/riiid-user-start-cluster/u_first_q_cluster_cv0_dict.pkl',\n",
    "                              'avg_score_u_last_day': '../input/riiid-fe-days-weeks-months/avg_score_u_last_day_cv0_dict.pkl',\n",
    "                              'avg_score_u_last_week': '../input/riiid-fe-days-weeks-months/avg_score_u_last_week_cv0_dict.pkl',\n",
    "                              'avg_score_u_last_month': '../input/riiid-fe-days-weeks-months/avg_score_u_last_month_cv0_dict.pkl',\n",
    "                              'curr_day_count_u': '../input/riiid-fe-days-weeks-months/curr_day_count_u_cv0_dict.pkl',\n",
    "                              'curr_week_count_u': '../input/riiid-fe-days-weeks-months/curr_week_count_u_cv0_dict.pkl',\n",
    "                              'curr_month_count_u': '../input/riiid-fe-days-weeks-months/curr_month_count_u_cv0_dict.pkl',\n",
    "                              'q_count_u_curr_day': '../input/riiid-fe-days-weeks-months/q_count_u_curr_day_cv0_dict.pkl',\n",
    "                              'q_count_u_curr_week': '../input/riiid-fe-days-weeks-months/q_count_u_curr_week_cv0_dict.pkl',\n",
    "                              'q_count_u_curr_month': '../input/riiid-fe-days-weeks-months/q_count_u_curr_month_cv0_dict.pkl',\n",
    "                              'q_count_u_last_day': '../input/riiid-fe-days-weeks-months/q_count_u_last_day_cv0_dict.pkl',\n",
    "                              'q_count_u_last_week': '../input/riiid-fe-days-weeks-months/q_count_u_last_week_cv0_dict.pkl',\n",
    "                              'q_count_u_last_month': '../input/riiid-fe-days-weeks-months/q_count_u_last_month_cv0_dict.pkl',\n",
    "                              'sum_score_u_curr_day': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_day_cv0_dict.pkl',\n",
    "                              'sum_score_u_curr_week': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_week_cv0_dict.pkl',\n",
    "                              'sum_score_u_curr_month': '../input/riiid-fe-days-weeks-months/sum_score_u_curr_month_cv0_dict.pkl',\n",
    "                              'time_since_last_hist_3': '../input/riiid-time-since-last-hist-3/time_since_last_hist_3_cv0_dict.pkl',\n",
    "                              'last_time_right': '../input/riiid-time-since-last-right-wrong-corrected/time_last_right_cv0_dict.pkl',\n",
    "                              'last_time_wrong': '../input/riiid-time-since-last-right-wrong-corrected/time_last_wrong_cv0_dict.pkl',\n",
    "                              'q_count_u_part': '../input/riiid-u-score-part/q_count_u_part_cv0_dict.pkl',\n",
    "                              'sum_score_u_part': '../input/riiid-u-score-part/sum_score_u_part_cv0_dict.pkl',\n",
    "                              'sum_prior_q_elapsed_time': '../input/riiid-prior-q-cont-elapsed-time/sum_prior_q_elapsed_time_cv0_dict.pkl',\n",
    "                              'cont_count_part': '../input/riiid-prior-q-cont-elasped-time-part/cont_count_part_cv0_dict.pkl',\n",
    "                              'last_cont_part': '../input/riiid-prior-q-cont-elasped-time-part/last_cont_part_cv0_dict.pkl', \n",
    "                              'sum_prior_q_elapsed_time_part': '../input/riiid-prior-q-cont-elasped-time-part/sum_prior_q_elapsed_time_part_cv0_dict.pkl',\n",
    "                              'sum_time_since_last_u': '../input/riiid-t-diff-u-avg-and-ratios/sum_time_since_last_u_cv0_dict.pkl',\n",
    "                            },\n",
    "                        }\n",
    "\n",
    "\n",
    "question_feature_stats_paths = {'answered_correctly_avg': '../input/riiid-q-stats-dicts-pkl/answered_correctly_avg_c_dict.pkl',\n",
    "                                'answered_correctly_std': '../input/riiid-q-stats-dicts-pkl/answered_correctly_std_c_dict.pkl',\n",
    "                                'answered_correctly_median': '../input/riiid-q-stats-dicts-pkl/answered_correctly_median_c_dict.pkl',\n",
    "                                'part': '../input/riiid-q-stats-dicts-pkl/questions_parts_dict.pkl',\n",
    "                                'tags': '../input/riiid-q-stats-dicts-pkl/tags_per_questions_no_nan.pkl.pkl',\n",
    "                                'tag_count': '../input/riiid-q-tag-count-dict/dict_q_tag_count.pkl', # buy time with RAM\n",
    "                                'avg_question_elapsed_time': '../input/riiid-question-time-stats/avg_question_elapsed_time_c_dict.pkl',\n",
    "                                'std_question_elapsed_time': '../input/riiid-question-time-stats/std_question_elapsed_time_c_dict.pkl',\n",
    "                                'avg_time_since_last': '../input/riiid-question-time-stats/avg_time_since_last_c_dict.pkl',\n",
    "                                'std_time_since_last': '../input/riiid-question-time-stats/std_time_since_last_c_dict.pkl',\n",
    "                                'avg_part_score': '../input/riiid-part-score-stats-avg-std/avg_part_score_c_dict.pkl',\n",
    "                                'std_part_score': '../input/riiid-part-score-stats-avg-std/std_part_score_c_dict.pkl',\n",
    "                                'avg_question_elapsed_time_right': '../input/riiid-question-time-score-stats-dicts/avg_question_elapsed_time_c_right_dict.pkl',\n",
    "                                'avg_question_elapsed_time_wrong': '../input/riiid-question-time-score-stats-dicts/avg_question_elapsed_time_c_wrong_dict.pkl',\n",
    "                                'avg_time_since_last_c_right': '../input/riiid-question-time-score-stats-dicts/avg_time_since_last_c_right_dict.pkl',\n",
    "                                'avg_time_since_last_c_wrong': '../input/riiid-question-time-score-stats-dicts/avg_time_since_last_c_wrong_dict.pkl',\n",
    "                                'difficulty_level': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_lvl_c_dict.pkl', # c_id -> level\n",
    "                                'num_answers_q': '../input/riiid-question-meta-stats/num_answers_q_dict.pkl',\n",
    "                                'q_count_trainset_c': '../input/riiid-question-meta-stats/q_count_trainset_c_dict.pkl',\n",
    "                                'tag_cluster': '../input/riiid-tags-cluster-dict-arrs/q_tags_clusters_dict.pkl',\n",
    "                                'curr_avg_q_elapsed_time_c': '../input/riiid-avg-q-elapsed-time-c/curr_avg_q_elapsed_time_c_dict.pkl',\n",
    "                                'curr_avg_q_is_explained_c': '../input/riiid-curr-avg-q-is-explained/curr_avg_is_explained_c_dict.pkl',\n",
    "                               }\n",
    "\n",
    "\n",
    "\n",
    "user_cluster_avg_score_start_200_path = \"../input/riiid-user-start-cluster/u_cluster_avg_start_200_dict.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "question_level_feature_stats_paths = {'difficulty_level_avg_score_c': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_lvl_c_avg_score_dict.pkl',\n",
    "                                      'difficulty_level_std_score_c': '../input/riiid-difficulty-lvl-q-arr-dicts/difficulty_q_lvl_c_std_score_dict.pkl',\n",
    "                                      'avg_q_elapsed_time_level' : '../input/riiid-q-elpsd-time-dffclty-lvl-arr-dicts/avg_q_elapsed_time_per_lvl_dict.pkl',\n",
    "                                      }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tags_feature_stats_paths = {'answered_correctly_sum': '../input/riiid-t-pckls-of-dicts/answered_correctly_sum_t_dict.pkl',\n",
    "                            'q_count': '../input/riiid-t-pckls-of-dicts/q_count_t_dict.pkl', # key t : kow much q in the train set with t\n",
    "                            'ncount': '../input/riiid-fe-tags-freq-stats/ncount_tags_dict.pkl', # key t : kow much unique q with t\n",
    "                            }\n",
    "\n",
    "\n",
    "lecture_feature_stats_paths = {'part': '../input/riiid-part-per-lecture-pkl/part_per_lecture_dict.pkl'\n",
    "                               }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SAKT hyperparameters\n",
    "\n",
    "\n",
    "SAKT_GROUP_PICKLE_FULLSET_PATH = '../input/riiid-sakt-02/group.pkl'\n",
    "SAKT_MODEL_STATE_DICT_FULLSET_PATH = '../input/riiid-sakt-enc1/sakt_enc1.pth'\n",
    "\n",
    "SAKT_SKILL_NP_PATH = '../input/riiid-sakt-train-metadata-1/sakt_skills.npy'\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# GLOBAL VARIABLES\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "# CV config\n",
    "\n",
    "INDEX_CV_SLICE = 0\n",
    "\n",
    "# Constants\n",
    "\n",
    "TIMESTAMP_MAX_TRAINSET = 82106074887\n",
    "\n",
    "LOG_TIME_RATIO_MEDIAN = -3.304619\n",
    "PRIOR_QUESTION_ELAPSED_TIME_TRAIN_MEAN = 25439.41\n",
    "\n",
    "TIME_SINCE_LAST_MEDIAN = 67233\n",
    "LIMIT_TIME_SESSION = TIME_SINCE_LAST_MEDIAN*10\n",
    "\n",
    "CLUSTERING_U_START_Q_IDS = [7900, 128, 5692]\n",
    "\n",
    "\n",
    "\n",
    "NUM_TRAIN_ROWS = 10000000 if MODE_10M_ROWS else 5000000 \n",
    "NUM_VALID_ROWS = 2500000\n",
    "\n",
    "\n",
    "MAX_ROWS_TRAIN = 96817414\n",
    "MAX_ROWS_VALID = 2453886\n",
    "\n",
    "NUM_UNIQUE_QUESTIONS = 13523\n",
    "\n",
    "\n",
    "EARLY_STOPPING_ROUNDS_LGBM = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Running args \n",
    "\n",
    "debug = False\n",
    "\n",
    "training_flg = True # do training / load pretrained weights ?\n",
    "validaten_flg = False # apply CV / submit ?\n",
    "unit_test_inference_flg = False\n",
    "\n",
    "smaller_local_cv_validset = False\n",
    "\n",
    "from_train_flg = False if debug else True\n",
    "\n",
    "feature_importance_flg = True\n",
    "\n",
    "verbose = True\n",
    "\n",
    "\n",
    "# Utils\n",
    "\n",
    "def log(msg, end = '\\n'):\n",
    "    if verbose :\n",
    "        print(msg, end=end)\n",
    "    # add txt export\n",
    "    \n",
    "# Seeds        \n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-04T21:28:04.396066Z",
     "iopub.status.busy": "2021-04-04T21:28:04.395276Z",
     "iopub.status.idle": "2021-04-04T21:28:04.407525Z",
     "shell.execute_reply": "2021-04-04T21:28:04.408175Z"
    },
    "papermill": {
     "duration": 0.047797,
     "end_time": "2021-04-04T21:28:04.408368",
     "exception": false,
     "start_time": "2021-04-04T21:28:04.360571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Train/Valid slices config) ind slice : 0 | train slice : [86817414, 96817414] | valid slice : [0, 2453886]\n"
     ]
    }
   ],
   "source": [
    "# Train / Valid CV split :\n",
    "\n",
    "i_cv_slice = INDEX_CV_SLICE\n",
    "\n",
    "def get_train_valid_slices(idx_cv_slice):\n",
    "    train_slice_r = [MAX_ROWS_TRAIN - (idx_cv_slice+1)*NUM_TRAIN_ROWS, MAX_ROWS_TRAIN - idx_cv_slice*NUM_TRAIN_ROWS]\n",
    "\n",
    "    if idx_cv_slice == 0:\n",
    "        # Former train/valid split : end of the trainset / full valid set\n",
    "        valid_slice_r = [0, MAX_ROWS_VALID]\n",
    "    else:\n",
    "        # Split from train set : slice from the trainset/ the following subset as valid\n",
    "        valid_slice_r = [train_slice_r[1], train_slice_r[1]+MAX_ROWS_VALID]\n",
    "    \n",
    "    return train_slice_r, valid_slice_r\n",
    "    \n",
    "\n",
    "train_slice_rows, valid_slice_rows = get_train_valid_slices(i_cv_slice)\n",
    "\n",
    "print('(Train/Valid slices config) ind slice :', i_cv_slice, '| train slice :', train_slice_rows, '| valid slice :', valid_slice_rows)\n",
    "\n",
    "\n",
    "def get_kfold_name(i_cv_slice, valid_flag):\n",
    "    'Get the name of the fold corresponding to i_cv_slice & the valid / submission config.'\n",
    "    assert i_cv_slice <= 2 and i_cv_slice >= 0\n",
    "    if not valid_flag:\n",
    "        return 'fullset'\n",
    "    else:\n",
    "        return 'cv'+str(i_cv_slice)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030507,
     "end_time": "2021-04-04T21:28:04.470140",
     "exception": false,
     "start_time": "2021-04-04T21:28:04.439633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-04T21:28:04.548139Z",
     "iopub.status.busy": "2021-04-04T21:28:04.542721Z",
     "iopub.status.idle": "2021-04-04T21:28:04.598372Z",
     "shell.execute_reply": "2021-04-04T21:28:04.597706Z"
    },
    "papermill": {
     "duration": 0.097296,
     "end_time": "2021-04-04T21:28:04.598508",
     "exception": false,
     "start_time": "2021-04-04T21:28:04.501212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor(object):\n",
    "    \"\"\" Object prepare data for training. \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, offline_feats_train_paths, offline_feats_valid_paths, \\\n",
    "                     train_raw_q_path, valid_raw_q_path, X_feats, y_feats, train_slice_rows_i, valid_slice_rows_i, valid_flag=False): \n",
    "\n",
    "        self.feats_user_dicts = {}\n",
    "\n",
    "        log('Offline features loading ...')\n",
    "        self.train = self.init_df_offline(offline_feats_train_paths, slice_rows=train_slice_rows_i)\n",
    "        if not valid_flag:\n",
    "            if valid_slice_rows_i[0] == 0 : # beginning of the valid set\n",
    "                self.valid = self.init_df_offline(offline_feats_valid_paths, slice_rows=valid_slice_rows_i)\n",
    "            else: # slice from the train set\n",
    "                self.valid = self.init_df_offline(offline_feats_train_paths, slice_rows=valid_slice_rows_i)\n",
    "                \n",
    "        \n",
    "        log('Online features loading ...')\n",
    "        self.train = self.get_feats_online(self.train, train_raw_q_path)\n",
    "        if not valid_flag:\n",
    "            self.valid = self.get_feats_online(self.valid, valid_raw_q_path)\n",
    "        \n",
    "        log('Features selection ...')\n",
    "        dro_cols = list(set(self.train.columns) - set(X_feats))\n",
    "        self.y_tr = self.train[y_feats]\n",
    "        self.train.drop(dro_cols, axis=1, inplace=True)\n",
    "        if not valid_flag:\n",
    "            self.y_va = self.valid[y_feats]\n",
    "            self.valid.drop(dro_cols, axis=1, inplace=True)\n",
    "        _=gc.collect()\n",
    "    \n",
    "        log('Features type casting ...')\n",
    "        self.train = self.init_types_df(self.train)\n",
    "        if not valid_flag:\n",
    "            self.valid = self.init_types_df(self.valid)\n",
    "    \n",
    "        log('Data preprocessor init complete.')\n",
    "\n",
    "        \n",
    "    ########################################################################################\n",
    "    # PREPROCESSING MAIN FUNCTIONS\n",
    "    ########################################################################################\n",
    "\n",
    "        \n",
    "    def init_df_offline(self, dict_features_paths, slice_rows = None):\n",
    "        \"\"\" Initialize a dataframe from a dict of path to each features np array. \n",
    "        Get the last 5% of raws if smaller_df is True. \"\"\"\n",
    "\n",
    "        start_row, end_row = slice_rows\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for feat in tqdm(dict_features_paths, desc= 'Loading offline features'):\n",
    "            df[feat] = np.load(dict_features_paths[feat])[start_row:end_row]\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def get_feats_online(self, df, raw_df_q_feather_path):\n",
    "        \"\"\"Extract from raw_df the needed features, and add them to df. Return the updated df.\n",
    "        Get the last 5% of raws if smaller_df is True.\"\"\"\n",
    "        \n",
    "        \n",
    "        field_needed = ['row_id','user_id', 'timestamp','content_type_id', 'answered_correctly', 'task_container_id', 'content_id', 'prior_question_had_explanation']\n",
    "        \n",
    "        raw_df = pd.read_feather(raw_df_q_feather_path)[field_needed]\n",
    "        assert(len(df) == len(raw_df))\n",
    "\n",
    "        # Raw feats\n",
    "        log('...raw feats', ' ')\n",
    "        df['timestamp'] = raw_df['timestamp'].values\n",
    "        df['task_container_id'] = raw_df['task_container_id'].values\n",
    "        df['content_id'] = raw_df['content_id'].values\n",
    "        df['prior_question_had_explanation'] = raw_df['prior_question_had_explanation'].fillna(False).astype('int8').values\n",
    "\n",
    "        # Straightforward computations\n",
    "        log('...straightforward computations', ' ')\n",
    "        df['time_btw_cont_mean'] = self.get_time_btw_containers_mean(df)\n",
    "        df['time_per_action_mean'] = self.get_time_per_action_mean(df)\n",
    "        df['is_first_question'] = self.get_is_first_question(df)\n",
    "        df['last_cont_score_mean'] = self.get_last_cont_score_mean(df)\n",
    "\n",
    "        df['cont_q_count'] = self.get_cont_q_count_preproc(raw_df)\n",
    "        df['question_elapsed_time'] = self.get_question_elapsed_time(df)\n",
    "        df['time_ratio'] = self.get_time_ratio(df)\n",
    "        df['log_time_ratio'] = self.get_log_time_ratio(df)\n",
    "        \n",
    "        df['ratio_score_u_time_ratio'] = self.get_ratio_score_u_time_ratio(df)\n",
    "        df['ratio_q_count_u_time_ratio'] = self.get_ratio_q_count_u_time_ratio(df)\n",
    "        \n",
    "        del raw_df\n",
    "        gc.collect()\n",
    "\n",
    "        log('...done')\n",
    "        return df\n",
    "\n",
    " \n",
    "    def init_types_df(self, df):\n",
    "        conversion_table = {\"avg_score_t\": np.float16,\n",
    "                          \"answered_correctly_avg_u\": np.float16,\n",
    "                          \"answered_correctly_avg_c\": np.float16,\n",
    "                          \"answered_correctly_std_c\": np.float16,\n",
    "                          \"prior_question_elapsed_time\": np.float16,\n",
    "                          \"avg_session_q_count\": np.float16,\n",
    "                          \"avg_score_u_hist_100_75\": np.float16,\n",
    "                          \"avg_score_u_hist_75_50\": np.float16,\n",
    "                          \"avg_score_u_hist_50_25\": np.float16,\n",
    "                          \"avg_score_u_hist_25_0\": np.float16,\n",
    "                          \"hist_score_diff_u_100_50\": np.float16,\n",
    "                          \"hist_score_diff_u_75_25\": np.float16,\n",
    "                          \"hist_score_diff_u_50_0\": np.float16,\n",
    "                          \"hist_100_score_slope_u\": np.float16,\n",
    "                          \"max_avg_score_t\": np.float16,\n",
    "                          \"min_avg_score_t\": np.float16,\n",
    "                          \"std_avg_score_t\": np.float16,\n",
    "                          \"avg_tag_freq\": np.float16,\n",
    "                          \"max_tag_freq\": np.float16,\n",
    "                          \"min_tag_freq\": np.float16,\n",
    "                          \"std_tag_freq\" : np.float16, \n",
    "                          \"ratio_diff_avg_question_elapsed_time_c\": np.float16,\n",
    "                          \"ratio_diff_avg_time_since_last_c\": np.float16,\n",
    "                          \"avg_q_elapsed_time_lvl\": np.float16,\n",
    "                          \"diff_avg_q_elapsed_time_lvl\": np.float16,\n",
    "                          \"last_cont_score_mean\": np.float16,\n",
    "                          \"question_elapsed_time\": np.float16,\n",
    "                          \"ratio_score_u_time_ratio\": np.float16,\n",
    "                          \"q_count_u_curr_day\": np.int16,\n",
    "                          \"q_count_u_curr_week\": np.int16,\n",
    "                          \"q_count_u_last_day\": np.int16,\n",
    "                          \"q_count_u_last_week\": np.int16,}\n",
    "        \n",
    "        \n",
    "        conversion_table_feats = set(conversion_table)\n",
    "        train_feats = set(df.columns)\n",
    "        \n",
    "        required_conversion = train_feats.intersection(conversion_table_feats)\n",
    "        \n",
    "        conversion_output_dict={}\n",
    "        for feat in required_conversion:\n",
    "            conversion_output_dict[feat] = conversion_table[feat]\n",
    "            \n",
    "        return df.astype(conversion_output_dict)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ########################################################################################\n",
    "    # ONLINE FEATURES COMPUTATION FUNCTIONS\n",
    "    ########################################################################################\n",
    "    \n",
    "    \n",
    "    def get_time_btw_containers_mean(self, df):\n",
    "        '''Mean time between containers. Return the array of features to store as \"time_btw_cont_mean\".'''\n",
    "        return (df.timestamp/df.task_container_id).replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.int64) \n",
    "\n",
    "    def get_time_per_action_mean(self, df):\n",
    "        return (df.timestamp/(df.q_count_u + df.l_count_u)).fillna(0).astype(np.float32)\n",
    "\n",
    "    def get_is_first_question(self, df):\n",
    "        '''Boolean (stored in int8) feature which describe whether a row corresponds to the first question of the user\n",
    "        or not. count_u must have been already initialize. Should be the case if df is \"train\" or \"valid\", or even\n",
    "        \"test_df\" (in inference loop after having added user feats).'''\n",
    "        return (df.q_count_u==0).values.astype(np.int8) \n",
    "\n",
    "\n",
    "    def get_last_cont_score_mean(self, df):\n",
    "        '''Mean score (answered_correctly) obtained in the last container. \n",
    "        Return the array of features to store as \"last_cont_score_mean\"\n",
    "        'last_container_sum_answ' and 'last_cont_num_q' must have already been initialized (should be\n",
    "        the case for \"train\" and \"valid\", even \"test_df\" (end of the inference loop).'''\n",
    "        return (df.last_container_sum_answ/df.last_cont_q_count).fillna(0).astype(np.float32)\n",
    "\n",
    "\n",
    "    def get_cont_q_count_preproc(self, df):\n",
    "        '''Get the number of question per container for df.\n",
    "        Requires user_id',task_container_id,row_id.\n",
    "        Return an ndarray contianint the values of \"num_q_cont\". (values : from 1 to 6). '''\n",
    "\n",
    "        g = df[['user_id','task_container_id','row_id']].groupby(['user_id','task_container_id']).count()\n",
    "        g.reset_index(inplace=True)\n",
    "        g.rename(columns={'row_id':'num_q_cont'},inplace=True)\n",
    "        g['num_q_cont'] = g['num_q_cont'].astype(np.int8)\n",
    "\n",
    "        df = df.merge(g, on=['user_id','task_container_id'])\n",
    "        df.loc[df[df.num_q_cont >= 6].index,'num_q_cont'] = 6\n",
    "\n",
    "        del g\n",
    "        gc.collect()\n",
    "\n",
    "        return df.num_q_cont.values\n",
    "\n",
    "\n",
    "    def get_question_elapsed_time(self, df):\n",
    "        '''Get the mean time the user spent on each question of the current container.\n",
    "        Requires : time_since_last, num_q_cont.'''\n",
    "        return (df.time_since_last/df.cont_q_count).astype(np.float32).fillna(0)\n",
    "\n",
    "    \n",
    "    def get_time_ratio(self, df):\n",
    "        '''Get the current time ratio, w.r.t. the max timestamp of the given training set. Use train set max\n",
    "        to be more robust to very large timestamp of the testset.'''\n",
    "        return (df.timestamp/TIMESTAMP_MAX_TRAINSET).astype(np.float32).fillna(0)\n",
    "    \n",
    "    \n",
    "    def get_log_time_ratio(self, df):\n",
    "        '''Get the log of the current time ratio, to emphasize differences btw small ratios, which are more \n",
    "        common than large one.'''\n",
    "        return np.log(df.time_ratio).fillna(LOG_TIME_RATIO_MEDIAN)\n",
    "\n",
    "\n",
    "    def get_ratio_score_u_time_ratio(self, df):\n",
    "        '''Get the ratio between user score and its time ratio. Type : float32'''\n",
    "        return (df.answered_correctly_avg_u/df.time_ratio).fillna(0)\n",
    "        \n",
    "    def get_ratio_q_count_u_time_ratio(self, df):\n",
    "        '''Get \"ratio_q_count_time_ratio\" : q_count_u/time_ratio_u. Type : int32 '''\n",
    "        ratio_q_count_u_time_ratio_arr = (df.q_count_u.to_numpy()/df.time_ratio.to_numpy()).astype(np.int32)\n",
    "        ratio_q_count_u_time_ratio_arr = np.nan_to_num(ratio_q_count_u_time_ratio_arr, nan=0, posinf=2e7, neginf=0)\n",
    "        np.clip(ratio_q_count_u_time_ratio_arr, 0, 2e7, out=ratio_q_count_u_time_ratio_arr)\n",
    "        return ratio_q_count_u_time_ratio_arr\n",
    "        \n",
    "    \n",
    "    \n",
    " \n",
    "    ########################################################################################\n",
    "    # UTILS\n",
    "    ########################################################################################\n",
    "        \n",
    "\n",
    "    def train_info(self):\n",
    "        self.train.info(memory_usage='deep')\n",
    "        \n",
    "    def get_train(self, FEATS):\n",
    "        return self.train[FEATS], self.y_tr\n",
    "    \n",
    "    def get_valid(self, FEATS):\n",
    "        return self.valid[FEATS], self.y_va\n",
    "    \n",
    "    \n",
    "    def free_train(self):\n",
    "        del self.train, self.y_tr # raw_data are free. Keep valid for ROC CV\n",
    "        _=gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-04T21:28:04.675670Z",
     "iopub.status.busy": "2021-04-04T21:28:04.674451Z",
     "iopub.status.idle": "2021-04-04T21:37:08.678434Z",
     "shell.execute_reply": "2021-04-04T21:37:08.679131Z"
    },
    "papermill": {
     "duration": 544.049437,
     "end_time": "2021-04-04T21:37:08.679384",
     "exception": false,
     "start_time": "2021-04-04T21:28:04.629947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline features loading ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1e1905983343638fac95a7a76f784e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading offline features', max=132.0, style=ProgressStyle"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1126a537c6345379fc5ae0eeafb7bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading offline features', max=132.0, style=ProgressStyle"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Online features loading ...\n",
      "...raw feats ...straightforward computations ...done\n",
      "...raw feats ...straightforward computations ...done\n",
      "Features selection ...\n",
      "Features type casting ...\n",
      "Data preprocessor init complete.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 49 columns):\n",
      " #   Column                                  Dtype  \n",
      "---  ------                                  -----  \n",
      " 0   answered_correctly_avg_u                float16\n",
      " 1   answered_correctly_sum_u                int32  \n",
      " 2   is_first_attempt                        int8   \n",
      " 3   answered_correctly_avg_c                float16\n",
      " 4   answered_correctly_std_c                float16\n",
      " 5   answered_correctly_median_c             int8   \n",
      " 6   part                                    int8   \n",
      " 7   prior_question_elapsed_time             float16\n",
      " 8   l_count_u                               int32  \n",
      " 9   last_session_break_time                 int64  \n",
      " 10  avg_session_q_count                     float16\n",
      " 11  current_session_avg_score               float16\n",
      " 12  current_session_time                    int64  \n",
      " 13  current_session_q_count                 int32  \n",
      " 14  avg_score_u_hist_25_0                   float16\n",
      " 15  current_right_answ_streak               int16  \n",
      " 16  hist_1_right_answ_streak                int16  \n",
      " 17  avg_right_answ_streak_hist_3            float32\n",
      " 18  first_try_success_count_u               int16  \n",
      " 19  unique_count_attempted_q_u              int16  \n",
      " 20  avg_tag_freq                            float16\n",
      " 21  min_tag_freq                            float16\n",
      " 22  avg_time_since_last_c                   int64  \n",
      " 23  std_time_since_last_c                   int64  \n",
      " 24  ratio_diff_avg_question_elapsed_time_c  float16\n",
      " 25  ratio_diff_avg_time_since_last_c        float16\n",
      " 26  q_explanation_avg_u                     float16\n",
      " 27  q_explanation_sum_u                     int16  \n",
      " 28  difficulty_level                        int8   \n",
      " 29  sum_score_q_level                       int32  \n",
      " 30  q_count_level_u                         int32  \n",
      " 31  avg_score_q_level_u                     float16\n",
      " 32  ratio_diff_avg_q_elapsed_time_lvl       float16\n",
      " 33  q_count_trainset_c                      int16  \n",
      " 34  avg_score_u_cluster_start               float16\n",
      " 35  time_since_last                         int64  \n",
      " 36  time_since_last_2                       int64  \n",
      " 37  time_since_last_3                       int64  \n",
      " 38  time_since_last_right                   int64  \n",
      " 39  time_since_last_wrong                   int64  \n",
      " 40  curr_avg_q_elapsed_time                 float32\n",
      " 41  curr_avg_q_is_explained                 float16\n",
      " 42  avg_score_u_part                        float16\n",
      " 43  sum_score_u_part                        int32  \n",
      " 44  avg_prior_q_elapsed_time_part           float32\n",
      " 45  task_container_id                       int16  \n",
      " 46  last_cont_score_mean                    float16\n",
      " 47  question_elapsed_time                   float16\n",
      " 48  ratio_score_u_time_ratio                float16\n",
      "dtypes: float16(20), float32(3), int16(7), int32(6), int64(9), int8(4)\n",
      "memory usage: 1.5 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FEATS = ['answered_correctly_avg_c', 'avg_score_q_level_u',\n",
    "       'avg_score_u_part', 'is_first_attempt', 'time_since_last',\n",
    "       'time_since_last_2', 'answered_correctly_std_c',\n",
    "       'current_session_q_count', 'current_session_time',\n",
    "       'question_elapsed_time', 'ratio_diff_avg_question_elapsed_time_c',\n",
    "       'current_right_answ_streak', 'ratio_diff_avg_time_since_last_c',\n",
    "       'curr_avg_q_is_explained', 'avg_score_u_hist_25_0',\n",
    "       'time_since_last_right', 'first_try_success_count_u',\n",
    "       'answered_correctly_avg_u', 'avg_prior_q_elapsed_time_part',\n",
    "       'difficulty_level', 'curr_avg_q_elapsed_time',\n",
    "       'unique_count_attempted_q_u', 'answered_correctly_median_c',\n",
    "       'q_count_level_u', 'last_cont_score_mean', 'sum_score_q_level',\n",
    "       'task_container_id',\n",
    "       'prior_question_elapsed_time', 'avg_score_u_cluster_start',\n",
    "       'min_tag_freq',\n",
    "       'ratio_diff_avg_q_elapsed_time_lvl', 'sum_score_u_part',\n",
    "       'time_since_last_wrong', 'avg_session_q_count',\n",
    "       'answered_correctly_sum_u', 'time_since_last_3',\n",
    "       'std_time_since_last_c',\n",
    "       'current_session_avg_score', 'avg_tag_freq',\n",
    "       'hist_1_right_answ_streak', 'part', 'q_explanation_sum_u',\n",
    "       'ratio_score_u_time_ratio',\n",
    "       'last_session_break_time', 'q_count_trainset_c',\n",
    "       'avg_right_answ_streak_hist_3', 'q_explanation_avg_u',\n",
    "       'avg_time_since_last_c', 'l_count_u']\n",
    "\n",
    "\n",
    "TARGET = 'answered_correctly'\n",
    "\n",
    "\n",
    "kfold_name = get_kfold_name(i_cv_slice, validaten_flg)\n",
    "\n",
    "data_preproc_params = {'offline_feats_train_paths': offline_features_train_paths, \n",
    "                       'offline_feats_valid_paths': offline_features_valid_paths, \n",
    "                       'train_raw_q_path': df_questions_kfold_paths[kfold_name]['train'], \n",
    "                       'valid_raw_q_path': df_questions_kfold_paths[kfold_name]['valid'],\n",
    "                       'X_feats': FEATS,\n",
    "                       'y_feats': TARGET,\n",
    "                       'train_slice_rows_i': train_slice_rows,\n",
    "                       'valid_slice_rows_i': valid_slice_rows,\n",
    "                       'valid_flag': False} # always False for training\n",
    "                       \n",
    "data_preprocessor = DataPreprocessor(**data_preproc_params)\n",
    "\n",
    "\n",
    "if verbose :\n",
    "    data_preprocessor.train_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037709,
     "end_time": "2021-04-04T21:37:08.754362",
     "exception": false,
     "start_time": "2021-04-04T21:37:08.716653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-04T21:37:08.837805Z",
     "iopub.status.busy": "2021-04-04T21:37:08.836920Z",
     "iopub.status.idle": "2021-04-04T21:37:10.369445Z",
     "shell.execute_reply": "2021-04-04T21:37:10.368283Z"
    },
    "papermill": {
     "duration": 1.57816,
     "end_time": "2021-04-04T21:37:10.369587",
     "exception": false,
     "start_time": "2021-04-04T21:37:08.791427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train = data_preprocessor.get_train(FEATS)\n",
    "X_valid, y_valid = data_preprocessor.get_valid(FEATS)\n",
    "\n",
    "# LGB datasets\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_valid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "\n",
    "data_preprocessor.free_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-04T21:37:10.448440Z",
     "iopub.status.busy": "2021-04-04T21:37:10.447751Z",
     "iopub.status.idle": "2021-04-04T21:37:10.450603Z",
     "shell.execute_reply": "2021-04-04T21:37:10.451374Z"
    },
    "papermill": {
     "duration": 0.046087,
     "end_time": "2021-04-04T21:37:10.451587",
     "exception": false,
     "start_time": "2021-04-04T21:37:10.405500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ready for {'training' if training_flg else 'loading'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-04T21:37:10.535759Z",
     "iopub.status.busy": "2021-04-04T21:37:10.535024Z",
     "iopub.status.idle": "2021-04-04T22:19:30.794492Z",
     "shell.execute_reply": "2021-04-04T22:19:30.795345Z"
    },
    "papermill": {
     "duration": 2540.30662,
     "end_time": "2021-04-04T22:19:30.795833",
     "exception": false,
     "start_time": "2021-04-04T21:37:10.489213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's binary_logloss: 0.528484\tvalid_1's binary_logloss: 0.53143\n",
      "[200]\ttraining's binary_logloss: 0.525318\tvalid_1's binary_logloss: 0.528426\n",
      "[300]\ttraining's binary_logloss: 0.523797\tvalid_1's binary_logloss: 0.527125\n",
      "[400]\ttraining's binary_logloss: 0.522781\tvalid_1's binary_logloss: 0.526369\n",
      "[500]\ttraining's binary_logloss: 0.521962\tvalid_1's binary_logloss: 0.5258\n",
      "[600]\ttraining's binary_logloss: 0.521309\tvalid_1's binary_logloss: 0.525391\n",
      "[700]\ttraining's binary_logloss: 0.520703\tvalid_1's binary_logloss: 0.525044\n",
      "[800]\ttraining's binary_logloss: 0.52021\tvalid_1's binary_logloss: 0.524818\n",
      "[900]\ttraining's binary_logloss: 0.519709\tvalid_1's binary_logloss: 0.524569\n",
      "[1000]\ttraining's binary_logloss: 0.519206\tvalid_1's binary_logloss: 0.524301\n",
      "[1100]\ttraining's binary_logloss: 0.518781\tvalid_1's binary_logloss: 0.524126\n",
      "[1200]\ttraining's binary_logloss: 0.51834\tvalid_1's binary_logloss: 0.523927\n",
      "[1300]\ttraining's binary_logloss: 0.517913\tvalid_1's binary_logloss: 0.523772\n",
      "[1400]\ttraining's binary_logloss: 0.517525\tvalid_1's binary_logloss: 0.523637\n",
      "[1500]\ttraining's binary_logloss: 0.517138\tvalid_1's binary_logloss: 0.523521\n",
      "[1600]\ttraining's binary_logloss: 0.516791\tvalid_1's binary_logloss: 0.523441\n",
      "[1700]\ttraining's binary_logloss: 0.516456\tvalid_1's binary_logloss: 0.523356\n",
      "[1800]\ttraining's binary_logloss: 0.516111\tvalid_1's binary_logloss: 0.523283\n",
      "[1900]\ttraining's binary_logloss: 0.515759\tvalid_1's binary_logloss: 0.523182\n",
      "Early stopping, best iteration is:\n",
      "[1951]\ttraining's binary_logloss: 0.515588\tvalid_1's binary_logloss: 0.523138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "if training_flg:\n",
    "    model = lgb.train( \n",
    "                        {'objective': 'binary',\n",
    "                        'lambda_l1': 1.0,\n",
    "                        'lambda_l2': 1.0,\n",
    "                        'subsample': 0.5,\n",
    "                        'feature_fraction': 0.7,\n",
    "                        }, \n",
    "                        lgb_train,\n",
    "                        valid_sets=[lgb_train, lgb_valid],\n",
    "                        verbose_eval=100,\n",
    "                        num_boost_round=10000,\n",
    "                        early_stopping_rounds=EARLY_STOPPING_ROUNDS_LGBM\n",
    "                    )\n",
    "\n",
    "\n",
    "    # Feature importance\n",
    "    #_ = lgb.plot_importance(model) # Rough approach : rely on SHAP instead.\n",
    "\n",
    "\n",
    "    \n",
    "else:\n",
    "    PRETRAINED_MODEL_PATH = '../input/riiid-lgb-fi-no-overfeat-49f/lgb_FI_no_overfeat_49feats.txt'\n",
    "    model = lgb.Booster(model_file=PRETRAINED_MODEL_PATH)\n",
    "    \n",
    "    _ = lgb.plot_importance(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-04T22:19:30.921592Z",
     "iopub.status.busy": "2021-04-04T22:19:30.920490Z",
     "iopub.status.idle": "2021-04-04T22:19:31.096684Z",
     "shell.execute_reply": "2021-04-04T22:19:31.097453Z"
    },
    "papermill": {
     "duration": 0.244214,
     "end_time": "2021-04-04T22:19:31.097676",
     "exception": false,
     "start_time": "2021-04-04T22:19:30.853462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export\n",
    "if training_flg:\n",
    "    model.save_model('lgb_sakt_' + str(len(FEATS)) + 'feats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-04T22:19:31.224185Z",
     "iopub.status.busy": "2021-04-04T22:19:31.223281Z",
     "iopub.status.idle": "2021-04-05T04:48:17.564507Z",
     "shell.execute_reply": "2021-04-05T04:48:17.563803Z"
    },
    "papermill": {
     "duration": 23326.410007,
     "end_time": "2021-04-05T04:48:17.564700",
     "exception": false,
     "start_time": "2021-04-04T22:19:31.154693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>feature_importance_vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answered_correctly_avg_c</td>\n",
       "      <td>1.122389e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avg_score_q_level_u</td>\n",
       "      <td>1.061189e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avg_score_u_part</td>\n",
       "      <td>3.997866e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is_first_attempt</td>\n",
       "      <td>2.995849e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time_since_last</td>\n",
       "      <td>2.540033e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>time_since_last_2</td>\n",
       "      <td>2.538638e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>first_try_success_count_u</td>\n",
       "      <td>2.116330e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>current_session_q_count</td>\n",
       "      <td>1.746903e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>current_session_time</td>\n",
       "      <td>1.726405e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>task_container_id</td>\n",
       "      <td>1.717823e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>answered_correctly_std_c</td>\n",
       "      <td>1.670351e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>curr_avg_q_is_explained</td>\n",
       "      <td>1.651069e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>question_elapsed_time</td>\n",
       "      <td>1.643166e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>answered_correctly_avg_u</td>\n",
       "      <td>1.570913e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ratio_diff_avg_question_elapsed_time_c</td>\n",
       "      <td>1.489684e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>current_right_answ_streak</td>\n",
       "      <td>1.431744e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>avg_prior_q_elapsed_time_part</td>\n",
       "      <td>1.381902e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ratio_diff_avg_time_since_last_c</td>\n",
       "      <td>1.352928e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>avg_score_u_hist_25_0</td>\n",
       "      <td>1.301875e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>time_since_last_right</td>\n",
       "      <td>1.272831e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>unique_count_attempted_q_u</td>\n",
       "      <td>1.175117e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>curr_avg_q_elapsed_time</td>\n",
       "      <td>1.118096e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sum_score_q_level</td>\n",
       "      <td>9.066524e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>q_count_level_u</td>\n",
       "      <td>8.747611e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>last_cont_score_mean</td>\n",
       "      <td>8.612443e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>prior_question_elapsed_time</td>\n",
       "      <td>7.837377e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>avg_score_u_cluster_start</td>\n",
       "      <td>7.209830e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sum_score_u_part</td>\n",
       "      <td>6.973523e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>current_session_avg_score</td>\n",
       "      <td>6.924908e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>q_explanation_sum_u</td>\n",
       "      <td>6.912425e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>min_tag_freq</td>\n",
       "      <td>6.329487e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>part</td>\n",
       "      <td>6.303134e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>answered_correctly_sum_u</td>\n",
       "      <td>6.108554e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>time_since_last_wrong</td>\n",
       "      <td>5.407988e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>time_since_last_3</td>\n",
       "      <td>4.982862e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ratio_diff_avg_q_elapsed_time_lvl</td>\n",
       "      <td>4.864301e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>std_time_since_last_c</td>\n",
       "      <td>4.827129e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>avg_session_q_count</td>\n",
       "      <td>4.766151e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>answered_correctly_median_c</td>\n",
       "      <td>4.692161e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>avg_tag_freq</td>\n",
       "      <td>4.522663e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>difficulty_level</td>\n",
       "      <td>4.305097e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>q_count_trainset_c</td>\n",
       "      <td>4.058882e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>q_explanation_avg_u</td>\n",
       "      <td>3.953288e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>hist_1_right_answ_streak</td>\n",
       "      <td>3.834377e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>ratio_score_u_time_ratio</td>\n",
       "      <td>3.809908e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>last_session_break_time</td>\n",
       "      <td>3.654578e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>avg_time_since_last_c</td>\n",
       "      <td>3.513240e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>avg_right_answ_streak_hist_3</td>\n",
       "      <td>3.294884e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>l_count_u</td>\n",
       "      <td>3.235399e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  col_name  feature_importance_vals\n",
       "0                 answered_correctly_avg_c             1.122389e+06\n",
       "1                      avg_score_q_level_u             1.061189e+06\n",
       "2                         avg_score_u_part             3.997866e+05\n",
       "3                         is_first_attempt             2.995849e+05\n",
       "4                          time_since_last             2.540033e+05\n",
       "5                        time_since_last_2             2.538638e+05\n",
       "16               first_try_success_count_u             2.116330e+05\n",
       "7                  current_session_q_count             1.746903e+05\n",
       "8                     current_session_time             1.726405e+05\n",
       "26                       task_container_id             1.717823e+05\n",
       "6                 answered_correctly_std_c             1.670351e+05\n",
       "13                 curr_avg_q_is_explained             1.651069e+05\n",
       "9                    question_elapsed_time             1.643166e+05\n",
       "17                answered_correctly_avg_u             1.570913e+05\n",
       "10  ratio_diff_avg_question_elapsed_time_c             1.489684e+05\n",
       "11               current_right_answ_streak             1.431744e+05\n",
       "18           avg_prior_q_elapsed_time_part             1.381902e+05\n",
       "12        ratio_diff_avg_time_since_last_c             1.352928e+05\n",
       "14                   avg_score_u_hist_25_0             1.301875e+05\n",
       "15                   time_since_last_right             1.272831e+05\n",
       "21              unique_count_attempted_q_u             1.175117e+05\n",
       "20                 curr_avg_q_elapsed_time             1.118096e+05\n",
       "25                       sum_score_q_level             9.066524e+04\n",
       "23                         q_count_level_u             8.747611e+04\n",
       "24                    last_cont_score_mean             8.612443e+04\n",
       "27             prior_question_elapsed_time             7.837377e+04\n",
       "28               avg_score_u_cluster_start             7.209830e+04\n",
       "31                        sum_score_u_part             6.973523e+04\n",
       "37               current_session_avg_score             6.924908e+04\n",
       "41                     q_explanation_sum_u             6.912425e+04\n",
       "29                            min_tag_freq             6.329487e+04\n",
       "40                                    part             6.303134e+04\n",
       "34                answered_correctly_sum_u             6.108554e+04\n",
       "32                   time_since_last_wrong             5.407988e+04\n",
       "35                       time_since_last_3             4.982862e+04\n",
       "30       ratio_diff_avg_q_elapsed_time_lvl             4.864301e+04\n",
       "36                   std_time_since_last_c             4.827129e+04\n",
       "33                     avg_session_q_count             4.766151e+04\n",
       "22             answered_correctly_median_c             4.692161e+04\n",
       "38                            avg_tag_freq             4.522663e+04\n",
       "19                        difficulty_level             4.305097e+04\n",
       "44                      q_count_trainset_c             4.058882e+04\n",
       "46                     q_explanation_avg_u             3.953288e+04\n",
       "39                hist_1_right_answ_streak             3.834377e+04\n",
       "42                ratio_score_u_time_ratio             3.809908e+04\n",
       "43                 last_session_break_time             3.654578e+04\n",
       "47                   avg_time_since_last_c             3.513240e+04\n",
       "45            avg_right_answ_streak_hist_3             3.294884e+04\n",
       "48                               l_count_u             3.235399e+04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature impoprtance\n",
    "if feature_importance_flg:\n",
    "    \n",
    "    def get_feature_importance(shap_values, num_feat_max, X_valid):\n",
    "        vals = np.abs(shap_values).mean(0)\n",
    "        feature_importance = pd.DataFrame(list(zip(X_valid.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "        feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "        return feature_importance[:num_feat_max]\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_valid)\n",
    "    \n",
    "    top_features_df_1 = get_feature_importance(shap_values, len(FEATS), X_valid)\n",
    "    display(top_features_df_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047364,
     "end_time": "2021-04-05T04:48:17.658224",
     "exception": false,
     "start_time": "2021-04-05T04:48:17.610860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference\n",
    "\n",
    "Credit : The local CV API has been written by : https://www.kaggle.com/its7171/time-series-api-iter-test-emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047375,
     "end_time": "2021-04-05T04:48:17.752371",
     "exception": false,
     "start_time": "2021-04-05T04:48:17.704996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:48:17.870849Z",
     "iopub.status.busy": "2021-04-05T04:48:17.864676Z",
     "iopub.status.idle": "2021-04-05T04:48:17.885345Z",
     "shell.execute_reply": "2021-04-05T04:48:17.885979Z"
    },
    "papermill": {
     "duration": 0.087026,
     "end_time": "2021-04-05T04:48:17.886186",
     "exception": false,
     "start_time": "2021-04-05T04:48:17.799160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# LOCAL-CV API\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "class Iter_Valid(object):\n",
    "    def __init__(self, df, max_user=1000):\n",
    "        df = df.reset_index(drop=True)\n",
    "        self.df = df\n",
    "        self.user_answer = df['user_answer'].astype(str).values\n",
    "        self.answered_correctly = df['answered_correctly'].astype(str).values\n",
    "        df['prior_group_responses'] = \"[]\"\n",
    "        df['prior_group_answers_correct'] = \"[]\"\n",
    "        self.sample_df = df[df['content_type_id'] == 0][['row_id']]\n",
    "        self.sample_df['answered_correctly'] = 0\n",
    "        self.len = len(df)\n",
    "        self.user_id = df.user_id.values\n",
    "        self.task_container_id = df.task_container_id.values\n",
    "        self.content_type_id = df.content_type_id.values\n",
    "        self.max_user = max_user\n",
    "        self.current = 0\n",
    "        self.pre_user_answer_list = []\n",
    "        self.pre_answered_correctly_list = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fix_df(self, user_answer_list, answered_correctly_list, pre_start):\n",
    "        df= self.df[pre_start:self.current].copy()\n",
    "        sample_df = self.sample_df[pre_start:self.current].copy()\n",
    "        df.loc[pre_start,'prior_group_responses'] = '[' + \",\".join(self.pre_user_answer_list) + ']'\n",
    "        df.loc[pre_start,'prior_group_answers_correct'] = '[' + \",\".join(self.pre_answered_correctly_list) + ']'\n",
    "        self.pre_user_answer_list = user_answer_list\n",
    "        self.pre_answered_correctly_list = answered_correctly_list\n",
    "        return df, sample_df\n",
    "\n",
    "    def __next__(self):\n",
    "        added_user = set()\n",
    "        pre_start = self.current\n",
    "        pre_added_user = -1\n",
    "        pre_task_container_id = -1\n",
    "\n",
    "        user_answer_list = []\n",
    "        answered_correctly_list = []\n",
    "        while self.current < self.len:\n",
    "            crr_user_id = self.user_id[self.current]\n",
    "            crr_task_container_id = self.task_container_id[self.current]\n",
    "            crr_content_type_id = self.content_type_id[self.current]\n",
    "            if crr_content_type_id == 1:\n",
    "                # no more than one task_container_id of \"questions\" from any single user\n",
    "                # so we only care for content_type_id == 0 to break loop\n",
    "                user_answer_list.append(self.user_answer[self.current])\n",
    "                answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "                self.current += 1\n",
    "                continue\n",
    "            if crr_user_id in added_user and ((crr_user_id != pre_added_user) or (crr_task_container_id != pre_task_container_id)):\n",
    "                # known user(not prev user or differnt task container)\n",
    "                return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "            if len(added_user) == self.max_user:\n",
    "                if  crr_user_id == pre_added_user and crr_task_container_id == pre_task_container_id:\n",
    "                    user_answer_list.append(self.user_answer[self.current])\n",
    "                    answered_correctly_list.append(self.answered_correctly[selfa.current])\n",
    "                    self.current += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "            added_user.add(crr_user_id)\n",
    "            pre_added_user = crr_user_id\n",
    "            pre_task_container_id = crr_task_container_id\n",
    "            user_answer_list.append(self.user_answer[self.current])\n",
    "            answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "            self.current += 1\n",
    "        if pre_start < self.current:\n",
    "            return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "        else:\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:48:17.992353Z",
     "iopub.status.busy": "2021-04-05T04:48:17.991061Z",
     "iopub.status.idle": "2021-04-05T04:48:18.036940Z",
     "shell.execute_reply": "2021-04-05T04:48:18.035961Z"
    },
    "papermill": {
     "duration": 0.103555,
     "end_time": "2021-04-05T04:48:18.037142",
     "exception": false,
     "start_time": "2021-04-05T04:48:17.933587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Submission mode) Environmnent initialization : OK\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# TRIGGER LOCAL CV / SUBMISSION MODE\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "if validaten_flg:\n",
    "\n",
    "    target_feather_path = df_questions_lectures_kfold_paths[kfold_name]['valid']\n",
    "\n",
    "    if smaller_local_cv_validset:\n",
    "        target_df = pd.read_feather(target_feather_path)[:100]\n",
    "    else:\n",
    "        target_df = pd.read_feather(target_feather_path)\n",
    "\n",
    "    if debug:\n",
    "        target_df = target_df[:10000]\n",
    "\n",
    "    iter_test = Iter_Valid(target_df,max_user=1000)\n",
    "\n",
    "    predicted = []\n",
    "    def set_predict(df):\n",
    "        predicted.append(df)\n",
    "\n",
    "    log('(Local CV) Environmnent initialization : OK')\n",
    "\n",
    "else:\n",
    "    import riiideducation\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test() \n",
    "    set_predict = env.predict\n",
    "\n",
    "    log('(Submission mode) Environmnent initialization : OK')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.045838,
     "end_time": "2021-04-05T04:48:18.130291",
     "exception": false,
     "start_time": "2021-04-05T04:48:18.084453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### State variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:48:18.231146Z",
     "iopub.status.busy": "2021-04-05T04:48:18.230305Z",
     "iopub.status.idle": "2021-04-05T04:48:18.234690Z",
     "shell.execute_reply": "2021-04-05T04:48:18.233889Z"
    },
    "papermill": {
     "duration": 0.058097,
     "end_time": "2021-04-05T04:48:18.234856",
     "exception": false,
     "start_time": "2021-04-05T04:48:18.176759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:48:18.353946Z",
     "iopub.status.busy": "2021-04-05T04:48:18.342137Z",
     "iopub.status.idle": "2021-04-05T04:48:18.580935Z",
     "shell.execute_reply": "2021-04-05T04:48:18.580305Z"
    },
    "papermill": {
     "duration": 0.299065,
     "end_time": "2021-04-05T04:48:18.581089",
     "exception": false,
     "start_time": "2021-04-05T04:48:18.282024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class UsersState(object):\n",
    "    \"\"\" Object which keeps track of user's variables state during inference. \"\"\"\n",
    "    \n",
    "    def __init__(self, users_feature_states_paths, users_cluster_avg_score_start_paths):\n",
    "        self.data = {}\n",
    "        \n",
    "        log('Loading offline features dict to user state ...')\n",
    "        for feat in users_feature_states_paths:\n",
    "            self.add_feat_to_data(feat, users_feature_states_paths[feat])\n",
    "            \n",
    "        log('Loading user cluster avg start hist 200 dict.')\n",
    "        self.u_cluster_avg_start_200_dict = load_obj(users_cluster_avg_score_start_paths)\n",
    "        \n",
    "        log('User state init complete.')\n",
    "    \n",
    "    \n",
    "    def add_feat_to_data(self, feat_name, feat_path):\n",
    "        \"\"\" Load a feature state stored as a dict (pickle file) and set it to data for each user. \"\"\" \n",
    "        feat_dict = load_obj(feat_path)\n",
    "        \n",
    "        for u_id in feat_dict:\n",
    "            \n",
    "            if u_id not in self.data:\n",
    "                self.add_new_user(u_id)\n",
    "            \n",
    "            self.data[u_id][feat_name] = feat_dict[u_id]\n",
    "            \n",
    "            \n",
    "    def add_new_user(self, u_id):\n",
    "        \"\"\"Initialize data state dicts for a new user.\"\"\"\n",
    "        self.data[u_id] = {}\n",
    "        self.data[u_id]['answered_correctly_sum'] = 0\n",
    "        self.data[u_id]['q_count'] = 0\n",
    "        self.data[u_id]['l_count'] = 0\n",
    "        self.data[u_id]['q_attempted'] = bitarray(13550, endian='little')\n",
    "        self.data[u_id]['q_attempted'].setall(0)\n",
    "        self.data[u_id]['curr_cont_id'] = -1\n",
    "        self.data[u_id]['curr_timestamp'] = 0\n",
    "        self.data[u_id]['last_timestamp'] = 0\n",
    "        self.data[u_id]['last_t_diff'] = -1\n",
    "        self.data[u_id]['last_cont_sum_answ'] = 0\n",
    "        self.data[u_id]['curr_cont_sum_answ'] = 0\n",
    "        self.data[u_id]['curr_cont_q_count'] = 0\n",
    "        self.data[u_id]['last_cont_q_count'] = 0\n",
    "        self.data[u_id]['session_num'] = 0\n",
    "        self.data[u_id]['first_cont_session'] = 0\n",
    "        self.data[u_id]['time_start_session'] = 0\n",
    "        self.data[u_id]['q_count_start_session'] = 0\n",
    "        self.data[u_id]['last_session_break_time'] = 0\n",
    "        self.data[u_id]['sum_session_break_time'] = 0\n",
    "        self.data[u_id]['curr_session_sum_score'] = 0\n",
    "        self.data[u_id]['hist_100_answ'] = deque(maxlen=100)\n",
    "        self.data[u_id]['curr_answ_streak'] = 0\n",
    "        self.data[u_id]['max_answ_streak'] = 0\n",
    "        self.data[u_id]['hist_3_answ_streak'] = deque(maxlen=3)\n",
    "        self.data[u_id]['first_try_success_count'] = 0\n",
    "        self.data[u_id]['unique_count_attempted_q'] = 0\n",
    "        self.data[u_id]['q_explanation_sum'] = 0\n",
    "        self.data[u_id]['time_since_last_sum_u'] = 0\n",
    "        self.data[u_id]['difficulty_lvl_count_u'] = {}\n",
    "        self.data[u_id]['difficulty_lvl_count_u'].update({0: 0, 1: 0, 2: 0, 3: 0, 4: 0})\n",
    "        self.data[u_id]['difficulty_lvl_sum_u'] = {}\n",
    "        self.data[u_id]['difficulty_lvl_sum_u'].update({0: 0, 1: 0, 2: 0, 3: 0, 4: 0})\n",
    "        self.data[u_id]['u_first_answ'] = np.int8(-1)\n",
    "        self.data[u_id]['u_first_q_cluster'] = np.int16(0) # convention : 0 is undefined, -1 is 'others'\n",
    "        \n",
    "        self.data[u_id]['avg_score_u_last_day'] = 0\n",
    "        self.data[u_id]['avg_score_u_last_week'] = 0\n",
    "        self.data[u_id]['avg_score_u_last_month'] = 0\n",
    "        self.data[u_id]['curr_day_count_u'] = 0\n",
    "        self.data[u_id]['curr_week_count_u'] = 0\n",
    "        self.data[u_id]['curr_month_count_u'] = 0\n",
    "        self.data[u_id]['q_count_u_curr_day'] = 0\n",
    "        self.data[u_id]['q_count_u_curr_week'] = 0\n",
    "        self.data[u_id]['q_count_u_curr_month'] = 0\n",
    "        self.data[u_id]['q_count_u_last_day'] = 0\n",
    "        self.data[u_id]['q_count_u_last_week'] = 0\n",
    "        self.data[u_id]['q_count_u_last_month'] = 0\n",
    "        self.data[u_id]['sum_score_u_curr_day'] = 0\n",
    "        self.data[u_id]['sum_score_u_curr_week'] = 0\n",
    "        self.data[u_id]['sum_score_u_curr_month'] = 0\n",
    "        \n",
    "        self.data[u_id]['time_since_last_hist_3'] = deque(maxlen=3)\n",
    "        self.data[u_id]['last_time_right'] = 0\n",
    "        self.data[u_id]['last_time_wrong'] = 0\n",
    "        \n",
    "        self.data[u_id]['q_count_u_part'] = {}\n",
    "        self.data[u_id]['q_count_u_part'].update({1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0})\n",
    "        self.data[u_id]['sum_score_u_part'] = {}\n",
    "        self.data[u_id]['sum_score_u_part'].update({1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0})\n",
    "        \n",
    "        self.data[u_id]['sum_prior_q_elapsed_time'] = 0\n",
    "        \n",
    "        self.data[u_id]['sum_prior_q_elapsed_time_part'] = {}\n",
    "        self.data[u_id]['sum_prior_q_elapsed_time_part'].update({1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0})\n",
    "        self.data[u_id]['cont_count_part'] = {}\n",
    "        self.data[u_id]['cont_count_part'].update({1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0})\n",
    "        self.data[u_id]['last_cont_part'] = -1\n",
    "    \n",
    "        self.data[u_id]['sum_time_since_last_u'] = 0\n",
    "    \n",
    "    \n",
    "    def add_online_feats_to_data(self, feat_name, feat_dict):\n",
    "        \"\"\" Load a feature state stored as a dict (pickle file) and set it to data for each user. \"\"\" \n",
    "\n",
    "        for u_id in feat_dict:\n",
    "            \n",
    "            if u_id not in self.data:\n",
    "                self.add_new_user(u_id)\n",
    "            \n",
    "            self.data[u_id][feat_name] = feat_dict[u_id]\n",
    "        \n",
    "\n",
    "        \n",
    "    ########################################################################################\n",
    "    # DICTS UPDATE\n",
    "    ########################################################################################\n",
    "      \n",
    "\n",
    "    def update_dicts_from_prev_state(self, df_prev):\n",
    "        \"\"\" Update data using information contained by df for feats features. \"\"\"\n",
    "        \n",
    "        feats = ['user_id','answered_correctly','content_type_id', 'task_container_id', 'content_id', 'timestamp', 'prior_question_had_explanation', 'difficulty_level', 'part', 'prior_question_elapsed_time']\n",
    "        \n",
    "        for (u_id, answ_correctly, c_type_id, cont_id, c_id, u_time, pre_is_expl, lvl, part, prior_q_elpsd_t) in df_prev[feats].values:\n",
    "            \n",
    "            # Type casts\n",
    "            u_id = np.int32(u_id)\n",
    "            answ_correctly = np.int8(answ_correctly)\n",
    "            c_type_id = np.int8(c_type_id)\n",
    "            cont_id = np.int16(cont_id)\n",
    "            c_id = np.int16(c_id)\n",
    "            u_time = np.int64(u_time)\n",
    "            pre_is_expl = np.int8(pre_is_expl)\n",
    "            lvl = np.int8(lvl)\n",
    "            part = np.int8(part)\n",
    "            prior_q_elpsd_t = np.float32(prior_q_elpsd_t)\n",
    "            \n",
    "            \n",
    "            is_lecture = (c_type_id == 1)\n",
    "            if is_lecture:\n",
    "                self.data[u_id]['l_count'] += 1\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            if u_id not in self.data:\n",
    "                self.add_new_user(u_id)\n",
    "\n",
    "                \n",
    "            self.data[u_id]['answered_correctly_sum'] += np.int8(answ_correctly)\n",
    "            self.data[u_id]['q_count'] += 1\n",
    "            \n",
    "            if self.data[u_id]['q_attempted'][c_id] == 0:\n",
    "                self.data[u_id]['q_attempted'][c_id] = 1\n",
    "                self.data[u_id]['first_try_success_count'] += np.int8(answ_correctly)\n",
    "                self.data[u_id]['unique_count_attempted_q'] += 1\n",
    "\n",
    "\n",
    "            is_new_cont_first_row = cont_id != self.data[u_id]['curr_cont_id'] \n",
    "            if is_new_cont_first_row :\n",
    "                self.data[u_id]['last_timestamp'] = self.data[u_id]['curr_timestamp']\n",
    "                self.data[u_id]['last_cont_q_count'] = self.data[u_id]['curr_cont_q_count']\n",
    "                self.data[u_id]['last_cont_sum_answ'] = self.data[u_id]['curr_cont_sum_answ']\n",
    "                \n",
    "                self.data[u_id]['curr_cont_id'] = np.int16(cont_id)\n",
    "                \n",
    "                self.data[u_id]['curr_timestamp'] = np.int64(u_time)\n",
    "                self.data[u_id]['curr_cont_q_count'] = 0\n",
    "                self.data[u_id]['curr_cont_sum_answ'] = 0\n",
    "            \n",
    "            \n",
    "            self.data[u_id]['curr_cont_sum_answ'] += np.int8(answ_correctly)\n",
    "            self.data[u_id]['curr_cont_q_count'] += 1\n",
    "            \n",
    "            \n",
    "            t_diff = u_time - self.data[u_id]['last_timestamp']\n",
    "\n",
    "            \n",
    "            \n",
    "            len_time_since_hist = len(self.data[u_id]['time_since_last_hist_3'])\n",
    "\n",
    "            if t_diff > 0:\n",
    "                if len_time_since_hist > 0:\n",
    "                    if t_diff != self.data[u_id]['time_since_last_hist_3'][-1] :\n",
    "                        self.data[u_id]['time_since_last_hist_3'].append(t_diff)\n",
    "                else:\n",
    "                    self.data[u_id]['time_since_last_hist_3'].append(t_diff)\n",
    "            \n",
    "            \n",
    "            if answ_correctly :\n",
    "                self.data[u_id]['last_time_right'] = np.int64(u_time)\n",
    "            else :\n",
    "                self.data[u_id]['last_time_wrong'] = np.int64(u_time)\n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "            is_new_sess = (self.data[u_id]['last_t_diff'] != -1) & \\\n",
    "                              ((t_diff > 100*self.data[u_id]['last_t_diff']) or (t_diff > LIMIT_TIME_SESSION)) & \\\n",
    "                                  (t_diff != 0)\n",
    "            \n",
    "            \n",
    "            if is_new_sess: \n",
    "                self.data[u_id]['session_num'] += 1\n",
    "                self.data[u_id]['first_cont_session'] = np.int16(cont_id)\n",
    "                self.data[u_id]['time_start_session'] = np.int64(u_time)\n",
    "                self.data[u_id]['q_count_start_session'] = self.data[u_id]['q_count'] - 1 # discard current q (next sess)\n",
    "                self.data[u_id]['last_session_break_time'] = t_diff\n",
    "                self.data[u_id]['sum_session_break_time'] += t_diff\n",
    "                self.data[u_id]['curr_session_sum_score'] = 0\n",
    "                \n",
    "            if t_diff != 0:\n",
    "                self.data[u_id]['last_t_diff'] = t_diff\n",
    "                \n",
    "            self.data[u_id]['hist_100_answ'].append(np.int8(answ_correctly))\n",
    "            \n",
    "\n",
    "            if answ_correctly == 1 :\n",
    "                self.data[u_id]['curr_answ_streak'] += 1\n",
    "            else:\n",
    "                if self.data[u_id]['curr_answ_streak'] > self.data[u_id]['max_answ_streak']:\n",
    "                    self.data[u_id]['max_answ_streak'] = self.data[u_id]['curr_answ_streak']\n",
    "                \n",
    "                self.data[u_id]['hist_3_answ_streak'].append(self.data[u_id]['curr_answ_streak'])\n",
    "                self.data[u_id]['curr_answ_streak'] = 0            \n",
    "        \n",
    "        \n",
    "            self.data[u_id]['q_explanation_sum'] += np.int8(pre_is_expl)\n",
    "            \n",
    "            if is_new_cont_first_row:\n",
    "                self.data[u_id]['time_since_last_sum_u'] += t_diff\n",
    "                                                       \n",
    "        \n",
    "            self.data[u_id]['difficulty_lvl_sum_u'][lvl] += np.int8(answ_correctly)\n",
    "            self.data[u_id]['difficulty_lvl_count_u'][lvl] += 1\n",
    "            \n",
    "\n",
    "            \n",
    "            is_first_answ_undefined = self.data[u_id]['u_first_answ'] == -1\n",
    "            if is_first_answ_undefined :\n",
    "                self.data[u_id]['u_first_answ'] = np.int8(answ_correctly)\n",
    "            \n",
    "            is_first_q_cluster_undefined = self.data[u_id]['u_first_q_cluster'] == 0\n",
    "            if is_first_q_cluster_undefined :\n",
    "                self.data[u_id]['u_first_q_cluster'] = np.int16(c_id) if (c_id in CLUSTERING_U_START_Q_IDS) else np.int16(-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.data[u_id]['q_count_u_part'][part] += 1\n",
    "            self.data[u_id]['sum_score_u_part'][part] += np.int8(answ_correctly)\n",
    "            \n",
    "            if not np.isnan(prior_q_elpsd_t):\n",
    "                self.data[u_id]['sum_prior_q_elapsed_time'] += int(prior_q_elpsd_t)*self.data[u_id]['last_cont_q_count']\n",
    "            \n",
    "            if is_new_cont_first_row :\n",
    "                \n",
    "                is_last_cont_part_first_row = self.data[u_id]['last_cont_part'] == -1        \n",
    "                if not is_last_cont_part_first_row :\n",
    "                    part_last_cont = self.data[u_id]['last_cont_part']\n",
    "\n",
    "                    if not np.isnan(prior_q_elpsd_t):\n",
    "                        self.data[u_id]['sum_prior_q_elapsed_time_part'][part_last_cont] += int(prior_q_elpsd_t)*self.data[u_id]['last_cont_q_count']\n",
    "                        \n",
    "                self.data[u_id]['cont_count_part'][part] += 1\n",
    "                \n",
    "                \n",
    "            self.data[u_id]['last_cont_part'] = part\n",
    "            \n",
    "            if is_new_cont_first_row and (t_diff > 0):\n",
    "                self.data[u_id]['sum_time_since_last_u'] += t_diff \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    ########################################################################################\n",
    "    # FEATURES GETTERS\n",
    "    ########################################################################################\n",
    "            \n",
    "    \n",
    "    def get_q_feats_from_curr_state(self, df_curr):\n",
    "        \"\"\" Get question features data using current user's states for each row of df_curr. Return a dataframe with \n",
    "        the new columns concatenated to df_curr.\n",
    "        Warning : df_curr must only contains questions. \"\"\"\n",
    "        \n",
    "        answ_corr_sum_arr = np.zeros(len(df_curr), dtype=np.int32)\n",
    "        q_count_arr = np.zeros(len(df_curr), dtype=np.int32)\n",
    "        is_first_attempt_arr = np.zeros(len(df_curr),dtype=np.int8)\n",
    "        \n",
    "        sess_num_arr = np.zeros(len(df_curr), dtype=np.int16)\n",
    "        same_container_as_last_arr = np.zeros(len(df_curr),dtype=np.int8)\n",
    "        last_cont_sum_answ_u_arr = np.zeros(len(df_curr),dtype=np.int8)\n",
    "        last_cont_q_count_u_arr = np.zeros(len(df_curr),dtype=np.int8)\n",
    "        same_sess_as_last_u_arr = np.full(len(df_curr), 1, dtype=np.int8)\n",
    "        current_session_time_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        current_session_q_count_arr = np.zeros(len(df_curr), dtype=np.int32)\n",
    "        last_session_break_time_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        avg_break_time_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        session_avg_time_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        avg_session_q_count_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        current_session_avg_score_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "        \n",
    "        avg_score_u_hist_25_0_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        avg_score_u_hist_50_25_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        avg_score_u_hist_75_50_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        avg_score_u_hist_100_75_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        hist_score_diff_u_50_0_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        hist_score_diff_u_75_25_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        hist_score_diff_u_100_50_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        hist_100_score_slope_u_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        \n",
    "        current_right_answ_streak_arr = np.zeros(len(df_curr), dtype=np.int16)\n",
    "        max_right_answ_streak_arr = np.zeros(len(df_curr), dtype=np.int16)\n",
    "        hist_1_right_answ_streak_arr = np.zeros(len(df_curr), dtype=np.int16)\n",
    "        hist_2_right_answ_streak_arr = np.zeros(len(df_curr), dtype=np.int16)\n",
    "        hist_3_right_answ_streak_arr = np.zeros(len(df_curr), dtype=np.int16)\n",
    "        avg_right_answ_streak_hist_3_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        \n",
    "        first_try_success_count_u_arr = np.zeros(len(df_curr),dtype=np.int16)\n",
    "        unique_count_attempted_q_u_arr = np.zeros(len(df_curr),dtype=np.int16)\n",
    "        \n",
    "        q_explanation_sum_u_arr = np.zeros(len(df_curr),dtype=np.int16)\n",
    "        q_explanation_avg_u_arr = np.zeros(len(df_curr),dtype=np.float16)\n",
    "        \n",
    "        time_since_last_sum_u_arr = np.zeros(len(df_curr),dtype=np.int64)\n",
    "        time_since_last_avg_u_arr = np.zeros(len(df_curr),dtype=np.int32)\n",
    "        \n",
    "        curr_cont_score_sum_u_arr = np.zeros(len(df_curr),dtype=np.int8)\n",
    "        curr_cont_tackled_q_count_u_arr = np.zeros(len(df_curr),dtype=np.int8)\n",
    "        curr_cont_score_avg_u_arr = np.zeros(len(df_curr),dtype=np.float16)\n",
    "        \n",
    "        sum_score_q_level_arr = np.zeros(len(df_curr), dtype=np.int32)\n",
    "        q_count_level_arr = np.zeros(len(df_curr), dtype=np.int32)\n",
    "        avg_score_q_level_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "        \n",
    "        first_answ_arr = np.zeros(len(df_curr), dtype=np.int8)\n",
    "        u_cluster_start_arr = np.zeros(len(df_curr), dtype=np.int16)\n",
    "        avg_score_u_cluster_start_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "        \n",
    "        time_since_last_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        time_since_last_2_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        time_since_last_3_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        \n",
    "        time_since_last_right_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        time_since_last_wrong_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        \n",
    "        sum_score_u_part_arr = np.zeros(len(df_curr), dtype=np.int32)\n",
    "        q_count_u_part_arr = np.zeros(len(df_curr), dtype=np.int32)\n",
    "        avg_score_u_part_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "        \n",
    "        avg_prior_cont_time_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        avg_prior_q_elapsed_time_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "    \n",
    "        avg_prior_cont_time_part_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        avg_prior_q_elapsed_time_part_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        \n",
    "        ratio_curr_by_avg_session_time_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "    \n",
    "        avg_u_time_since_last_arr = np.zeros(len(df_curr), dtype=np.int64)\n",
    "        ratio_tdiff_by_avg_tdiff_u_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        df_curr_feats = df_curr[['user_id','content_id','task_container_id','timestamp', 'prior_question_had_explanation', 'difficulty_level', 'part', 'prior_question_elapsed_time']].values\n",
    "    \n",
    "        for i_row, (u_id, c_id, cont_id, u_time, pre_is_expl, lvl, part, prior_q_elpsd_t) in enumerate(df_curr_feats):\n",
    "            \n",
    "            # Type casts\n",
    "            u_id = np.int32(u_id)\n",
    "            c_id = np.int16(c_id)\n",
    "            cont_id = np.int16(cont_id)\n",
    "            u_time = np.int64(u_time)\n",
    "            pre_is_expl = np.int8(pre_is_expl)\n",
    "            lvl = np.int8(lvl)\n",
    "            part = np.int8(part)\n",
    "            prior_q_elpsd_t = np.float32(prior_q_elpsd_t)\n",
    "            \n",
    "            \n",
    "            if u_id not in self.data:\n",
    "                self.add_new_user(u_id)\n",
    "                    \n",
    "                    \n",
    "            answ_corr_sum_arr[i_row] = self.data[u_id]['answered_correctly_sum']\n",
    "            q_count_arr[i_row] = self.data[u_id]['q_count']\n",
    "            \n",
    "            if self.data[u_id]['q_attempted'][c_id] == 0:\n",
    "                is_first_attempt_arr[i_row] = 1\n",
    "            \n",
    "            is_new_container = cont_id != self.data[u_id]['curr_cont_id']\n",
    "            if is_new_container:\n",
    "                t_diff = u_time - self.data[u_id]['curr_timestamp']\n",
    "                last_cont_q_count_u_arr[i_row] = self.data[u_id]['curr_cont_q_count']\n",
    "                last_cont_sum_answ_u_arr[i_row] = self.data[u_id]['curr_cont_sum_answ']\n",
    "                \n",
    "                curr_cont_score_avg_u_arr[i_row] = 0\n",
    "                curr_cont_score_sum_u_arr[i_row] = 0\n",
    "                curr_cont_tackled_q_count_u_arr[i_row] = 0 \n",
    "                \n",
    "            else:                \n",
    "                t_diff = u_time - self.data[u_id]['last_timestamp']\n",
    "                last_cont_q_count_u_arr[i_row] = self.data[u_id]['last_cont_q_count']\n",
    "                last_cont_sum_answ_u_arr[i_row] = self.data[u_id]['last_cont_sum_answ']\n",
    "                same_container_as_last_arr[i_row] = 1\n",
    "                \n",
    "                curr_cont_score_avg_u_arr[i_row] = self.data[u_id]['curr_cont_sum_answ'] / self.data[u_id]['curr_cont_q_count'] if (self.data[u_id]['curr_cont_q_count'] != 0) else 0\n",
    "                curr_cont_score_sum_u_arr[i_row] = self.data[u_id]['curr_cont_sum_answ']\n",
    "                curr_cont_tackled_q_count_u_arr[i_row] = self.data[u_id]['curr_cont_q_count']\n",
    "                \n",
    "                \n",
    "            \n",
    "            len_time_since_hist = len(self.data[u_id]['time_since_last_hist_3'])\n",
    "        \n",
    "            if is_new_container:\n",
    "                time_since_last_arr[i_row] = t_diff # == u_time - curr_timestamp_dict[u_id]\n",
    "                time_since_last_2_arr[i_row] = self.data[u_id]['time_since_last_hist_3'][-1] if len_time_since_hist > 0 else 0\n",
    "                time_since_last_3_arr[i_row] = self.data[u_id]['time_since_last_hist_3'][-2] if len_time_since_hist > 1 else 0\n",
    "            else:                \n",
    "                time_since_last_arr[i_row] = self.data[u_id]['time_since_last_hist_3'][-1] if len_time_since_hist > 0 else 0\n",
    "                time_since_last_2_arr[i_row] = self.data[u_id]['time_since_last_hist_3'][-2] if len_time_since_hist > 1 else 0\n",
    "                time_since_last_3_arr[i_row] = self.data[u_id]['time_since_last_hist_3'][-3] if len_time_since_hist > 2 else 0\n",
    "\n",
    "                \n",
    "            time_since_last_right_arr[i_row] = u_time - self.data[u_id]['last_time_right']\n",
    "            time_since_last_wrong_arr[i_row] = u_time - self.data[u_id]['last_time_wrong']\n",
    "            \n",
    "    \n",
    "    \n",
    "            is_new_sess = (self.data[u_id]['last_t_diff'] != -1) & \\\n",
    "                              ((t_diff > 100*self.data[u_id]['last_t_diff']) or (t_diff > LIMIT_TIME_SESSION)) & \\\n",
    "                                  (t_diff != 0) # returns 1 until session_num is updated\n",
    "            \n",
    "\n",
    "            \n",
    "            sess_num = self.data[u_id]['session_num'] + int(is_new_sess)\n",
    "            sess_num_arr[i_row] = sess_num \n",
    "            \n",
    "            if is_new_sess or (cont_id == self.data[u_id]['first_cont_session']):\n",
    "                same_sess_as_last_u_arr[i_row] = 0\n",
    "            \n",
    "            current_session_time_arr[i_row] = 0 if is_new_sess else (u_time - self.data[u_id]['time_start_session'])\n",
    "            \n",
    "            curr_sess_q_count = 0 if is_new_sess else (self.data[u_id]['q_count'] - self.data[u_id]['q_count_start_session']) # noisy effect on q spread across batches\n",
    "            current_session_q_count_arr[i_row] = curr_sess_q_count\n",
    "            \n",
    "            if is_new_sess :\n",
    "                last_session_break_time_arr[i_row] = t_diff\n",
    "            else: \n",
    "                last_session_break_time_arr[i_row] = self.data[u_id]['last_session_break_time']\n",
    "            \n",
    "            \n",
    "            \n",
    "            curr_session_break_time = self.data[u_id]['sum_session_break_time']\n",
    "            if is_new_sess:\n",
    "                curr_session_break_time += t_diff\n",
    "            \n",
    "            \n",
    "            if sess_num != 0:\n",
    "                avg_break_time_arr[i_row] = curr_session_break_time / sess_num    # sess_num == number of breaks\n",
    "\n",
    "            session_avg_time_arr[i_row] = (u_time - curr_session_break_time) / (sess_num + 1) # sess_num == (number of run session + 1)\n",
    "            \n",
    "\n",
    "            avg_session_q_count_arr[i_row] = self.data[u_id]['q_count'] / (sess_num + 1)\n",
    "            \n",
    "\n",
    "            if curr_sess_q_count != 0 :\n",
    "                current_session_avg_score_arr[i_row] = self.data[u_id]['curr_session_sum_score'] / curr_sess_q_count\n",
    "                \n",
    "\n",
    "                \n",
    "            if len(self.data[u_id]['hist_100_answ']) != 0 :\n",
    "                hist_100 = np.array(self.data[u_id]['hist_100_answ'])\n",
    "                avg_score_u_hist_25_0_arr[i_row] = hist_100[-25:].mean() if (len(hist_100) > 0) else 0\n",
    "                avg_score_u_hist_50_25_arr[i_row] = hist_100[-50:-25].mean() if (len(hist_100) > 25) else 0\n",
    "                avg_score_u_hist_75_50_arr[i_row] = hist_100[-75:-50].mean() if (len(hist_100) > 50) else 0\n",
    "                avg_score_u_hist_100_75_arr[i_row] = hist_100[:-75].mean() if (len(hist_100) > 75) else 0\n",
    "                \n",
    "                hist_score_diff_u_50_0_arr[i_row] = avg_score_u_hist_25_0_arr[i_row] - avg_score_u_hist_50_25_arr[i_row]\n",
    "                hist_score_diff_u_75_25_arr[i_row] = avg_score_u_hist_50_25_arr[i_row] - avg_score_u_hist_75_50_arr[i_row]\n",
    "                hist_score_diff_u_100_50_arr[i_row] = avg_score_u_hist_75_50_arr[i_row] - avg_score_u_hist_100_75_arr[i_row]\n",
    "                \n",
    "                hist_100_score_slope_u_arr[i_row] = np.mean([hist_score_diff_u_50_0_arr[i_row], \\\n",
    "                                                              hist_score_diff_u_75_25_arr[i_row] , \\\n",
    "                                                               hist_score_diff_u_100_50_arr[i_row]])\n",
    "                \n",
    "                \n",
    "            current_right_answ_streak_arr[i_row] = self.data[u_id]['curr_answ_streak']\n",
    "            max_right_answ_streak_arr[i_row] = self.data[u_id]['max_answ_streak']\n",
    "            \n",
    "            if len(self.data[u_id]['hist_3_answ_streak']) > 0:\n",
    "                hist_1_right_answ_streak_arr[i_row] = self.data[u_id]['hist_3_answ_streak'][-1]\n",
    "            if len(self.data[u_id]['hist_3_answ_streak']) > 1:\n",
    "                hist_2_right_answ_streak_arr[i_row] = self.data[u_id]['hist_3_answ_streak'][-2]\n",
    "            if len(self.data[u_id]['hist_3_answ_streak']) > 2:\n",
    "                hist_3_right_answ_streak_arr[i_row] = self.data[u_id]['hist_3_answ_streak'][-3]\n",
    "\n",
    "            if len(self.data[u_id]['hist_3_answ_streak']) > 0:\n",
    "                avg_right_answ_streak_hist_3_arr[i_row] = sum(self.data[u_id]['hist_3_answ_streak'])/len(self.data[u_id]['hist_3_answ_streak'])\n",
    "\n",
    "                \n",
    "            first_try_success_count_u_arr[i_row] = self.data[u_id]['first_try_success_count']\n",
    "            unique_count_attempted_q_u_arr[i_row] = self.data[u_id]['unique_count_attempted_q']\n",
    "            \n",
    "            \n",
    "            q_explanation_sum_u_arr[i_row] = self.data[u_id]['q_explanation_sum']+pre_is_expl\n",
    "            q_explanation_avg_u_arr[i_row] = (self.data[u_id]['q_explanation_sum']+pre_is_expl)/self.data[u_id]['q_count'] if (self.data[u_id]['q_count'] != 0) else 0\n",
    "        \n",
    "            time_since_last_sum_u_arr[i_row] = self.data[u_id]['time_since_last_sum_u']\n",
    "            time_since_last_avg_u_arr[i_row] = self.data[u_id]['time_since_last_sum_u']/cont_id if (cont_id != 0) else 0\n",
    "                \n",
    "\n",
    "            sum_score_q_level_arr[i_row] = self.data[u_id]['difficulty_lvl_sum_u'][lvl]\n",
    "            q_count_level_arr[i_row] = self.data[u_id]['difficulty_lvl_count_u'][lvl]\n",
    "            avg_score_q_level_arr[i_row] = self.data[u_id]['difficulty_lvl_sum_u'][lvl] / self.data[u_id]['difficulty_lvl_count_u'][lvl] if (self.data[u_id]['difficulty_lvl_count_u'][lvl] != 0) else 0\n",
    "                           \n",
    "        \n",
    "        \n",
    "            first_answ = self.data[u_id]['u_first_answ']\n",
    "            first_answ_arr[i_row] = first_answ\n",
    "            \n",
    "            u_clstr_start = self.data[u_id]['u_first_q_cluster']            \n",
    "            u_cluster_start_arr[i_row] = u_clstr_start\n",
    "        \n",
    "        \n",
    "            is_clustr_start_defined = (u_clstr_start != 0) & (cont_id != 0)\n",
    "            if is_clustr_start_defined:\n",
    "                if cont_id < 200:\n",
    "                    avg_score_u_cluster_start_arr[i_row] = self.u_cluster_avg_start_200_dict[u_clstr_start][first_answ][cont_id]\n",
    "                else:\n",
    "                    avg_score_u_cluster_start_arr[i_row] = self.u_cluster_avg_start_200_dict[u_clstr_start][first_answ][199]\n",
    "            else:\n",
    "                avg_score_u_cluster_start_arr[i_row] = -1\n",
    "\n",
    "\n",
    "            \n",
    "            sum_score_u_part_arr[i_row] = self.data[u_id]['sum_score_u_part'][part]\n",
    "            q_count_u_part_arr[i_row] = self.data[u_id]['q_count_u_part'][part]\n",
    "            avg_score_u_part_arr[i_row] = self.data[u_id]['sum_score_u_part'][part] / self.data[u_id]['q_count_u_part'][part] if (self.data[u_id]['q_count_u_part'][part] != 0) else 0 \n",
    "\n",
    "            \n",
    "            if not np.isnan(prior_q_elpsd_t):\n",
    "                updated_sum_prior_q_elapsed_time = int(prior_q_elpsd_t)*last_cont_q_count_u_arr[i_row] + self.data[u_id]['sum_prior_q_elapsed_time']\n",
    "                avg_prior_cont_time_arr[i_row] = updated_sum_prior_q_elapsed_time/cont_id if (cont_id != 0) else 0\n",
    "                avg_prior_q_elapsed_time_arr[i_row] = updated_sum_prior_q_elapsed_time/self.data[u_id]['q_count'] if (self.data[u_id]['q_count'] != 0) else 0\n",
    "        \n",
    "            \n",
    "            is_same_part_as_last_cont = self.data[u_id]['last_cont_part'] == part\n",
    "            if is_new_container and is_same_part_as_last_cont and not np.isnan(prior_q_elpsd_t):\n",
    "                # Last cont is related to the same part => dicts are not updated with the current prior_question_elapsed_time\n",
    "                # value yet, but we can already use it to compute current features.\n",
    "                updated_sum_prior_q_elapsed_time_part = int(prior_q_elpsd_t)*last_cont_q_count_u_arr[i_row] + self.data[u_id]['sum_prior_q_elapsed_time_part'][part]\n",
    "\n",
    "                avg_prior_cont_time_part_arr[i_row] = updated_sum_prior_q_elapsed_time_part/self.data[u_id]['cont_count_part'][part] if (self.data[u_id]['cont_count_part'][part] != 0) else 0\n",
    "                avg_prior_q_elapsed_time_part_arr[i_row] = updated_sum_prior_q_elapsed_time_part/self.data[u_id]['q_count_u_part'][part] if (self.data[u_id]['q_count_u_part'][part] != 0) else 0\n",
    "\n",
    "            else:\n",
    "                # first row, or dicts related to this part have already been updated\n",
    "                avg_prior_cont_time_part_arr[i_row] = self.data[u_id]['sum_prior_q_elapsed_time_part'][part]/self.data[u_id]['cont_count_part'][part] if (self.data[u_id]['cont_count_part'][part] != 0) else 0\n",
    "                avg_prior_q_elapsed_time_part_arr[i_row] = self.data[u_id]['sum_prior_q_elapsed_time_part'][part]/self.data[u_id]['q_count_u_part'][part] if (self.data[u_id]['q_count_u_part'][part] != 0) else 0\n",
    "        \n",
    "        \n",
    "            ratio_curr_by_avg_session_time_arr[i_row] = current_session_time_arr[i_row] / session_avg_time_arr[i_row] if (session_avg_time_arr[i_row] != 0) else 0\n",
    "        \n",
    "            \n",
    "            if is_new_container:\n",
    "                # sum of tdiff dict not updated yet\n",
    "                sum_time_since_last_updated = self.data[u_id]['sum_time_since_last_u']  + t_diff\n",
    "                avg_u_time_since_last_arr[i_row] = sum_time_since_last_updated/cont_id if (cont_id != 0) else 0\n",
    "                ratio_tdiff_by_avg_tdiff_u_arr[i_row] = t_diff/avg_u_time_since_last_arr[i_row] if (avg_u_time_since_last_arr[i_row] != 0) else 0\n",
    "            else:\n",
    "                avg_u_time_since_last_arr[i_row] = self.data[u_id]['sum_time_since_last_u']/cont_id if (cont_id != 0) else 0\n",
    "                ratio_tdiff_by_avg_tdiff_u_arr[i_row] = t_diff/avg_u_time_since_last_arr[i_row] if (avg_u_time_since_last_arr[i_row] != 0) else 0\n",
    "        \n",
    "        \n",
    "        user_feats_df = pd.DataFrame({'answered_correctly_avg_u': answ_corr_sum_arr/q_count_arr, \\\n",
    "                                      'answered_correctly_sum_u': answ_corr_sum_arr, \\\n",
    "                                      'q_count_u': q_count_arr, \\\n",
    "                                      'is_first_attempt': is_first_attempt_arr, \\\n",
    "                                      'session_num': sess_num_arr, \\\n",
    "                                      'same_container_as_last': same_container_as_last_arr, \\\n",
    "                                      'last_container_sum_answ': last_cont_sum_answ_u_arr, \\\n",
    "                                      'last_cont_q_count': last_cont_q_count_u_arr,\n",
    "                                      'same_session_as_last': same_sess_as_last_u_arr,\n",
    "                                      'current_session_time': current_session_time_arr,\n",
    "                                      'current_session_q_count': current_session_q_count_arr,\n",
    "                                      'last_session_break_time': last_session_break_time_arr,\n",
    "                                      'avg_break_time': avg_break_time_arr,\n",
    "                                      'avg_session_time': session_avg_time_arr,\n",
    "                                      'avg_session_q_count': avg_session_q_count_arr,\n",
    "                                      'current_session_avg_score': current_session_avg_score_arr,\n",
    "                                      'current_right_answ_streak': current_right_answ_streak_arr,\n",
    "                                      'max_right_answ_streak': max_right_answ_streak_arr,\n",
    "                                      'hist_1_right_answ_streak': hist_1_right_answ_streak_arr,\n",
    "                                      'hist_2_right_answ_streak': hist_2_right_answ_streak_arr,\n",
    "                                      'hist_3_right_answ_streak': hist_3_right_answ_streak_arr,\n",
    "                                      'avg_right_answ_streak_hist_3': avg_right_answ_streak_hist_3_arr,\n",
    "                                      'first_try_success_count_u': first_try_success_count_u_arr,\n",
    "                                      'unique_count_attempted_q_u': unique_count_attempted_q_u_arr,\n",
    "                                      'avg_score_u_hist_100_75': avg_score_u_hist_100_75_arr,\n",
    "                                      'avg_score_u_hist_75_50': avg_score_u_hist_75_50_arr,\n",
    "                                      'avg_score_u_hist_50_25': avg_score_u_hist_50_25_arr,\n",
    "                                      'avg_score_u_hist_25_0': avg_score_u_hist_25_0_arr,\n",
    "                                      'hist_score_diff_u_100_50': hist_score_diff_u_100_50_arr,\n",
    "                                      'hist_score_diff_u_75_25': hist_score_diff_u_75_25_arr,\n",
    "                                      'hist_score_diff_u_50_0': hist_score_diff_u_50_0_arr,\n",
    "                                      'hist_100_score_slope_u': hist_100_score_slope_u_arr,\n",
    "                                      'q_explanation_avg_u': q_explanation_avg_u_arr,\n",
    "                                      'q_explanation_sum_u': q_explanation_sum_u_arr,\n",
    "                                      'time_since_last_sum_u': time_since_last_sum_u_arr,\n",
    "                                      'time_since_last_avg_u': time_since_last_avg_u_arr,\n",
    "                                      'curr_cont_score_avg_u': curr_cont_score_avg_u_arr,\n",
    "                                      'curr_cont_score_sum_u': curr_cont_score_sum_u_arr,\n",
    "                                      'curr_cont_tackled_q_count_u': curr_cont_tackled_q_count_u_arr,\n",
    "                                      'sum_score_q_level': sum_score_q_level_arr,\n",
    "                                      'q_count_level_u': q_count_level_arr,\n",
    "                                      'avg_score_q_level_u': avg_score_q_level_arr,\n",
    "                                      'first_answ': first_answ_arr,\n",
    "                                      'u_cluster_start': u_cluster_start_arr,\n",
    "                                      'avg_score_u_cluster_start': avg_score_u_cluster_start_arr,\n",
    "                                      'time_since_last': time_since_last_arr,\n",
    "                                      'time_since_last_2': time_since_last_2_arr,\n",
    "                                      'time_since_last_3': time_since_last_3_arr,\n",
    "                                      'time_since_last_right': time_since_last_right_arr,\n",
    "                                      'time_since_last_wrong': time_since_last_wrong_arr,\n",
    "                                      'avg_score_u_part': avg_score_u_part_arr, \n",
    "                                      'sum_score_u_part': sum_score_u_part_arr,\n",
    "                                      'q_count_u_part': q_count_u_part_arr,\n",
    "                                      'avg_prior_cont_time': avg_prior_cont_time_arr,\n",
    "                                      'avg_prior_q_elapsed_time': avg_prior_q_elapsed_time_arr,\n",
    "                                      'avg_prior_cont_time_part': avg_prior_cont_time_part_arr,\n",
    "                                      'avg_prior_q_elapsed_time_part': avg_prior_q_elapsed_time_part_arr,\n",
    "                                      'ratio_curr_by_avg_session_time': ratio_curr_by_avg_session_time_arr,\n",
    "                                      'avg_u_time_since_last': avg_u_time_since_last_arr,\n",
    "                                      'ratio_tdiff_by_avg_tdiff_u': ratio_tdiff_by_avg_tdiff_u_arr,\n",
    "                                       })\n",
    "\n",
    "\n",
    "        \n",
    "        user_feats_df['answered_correctly_avg_u'] = user_feats_df['answered_correctly_avg_u'].fillna(0)\n",
    "\n",
    "        return pd.concat([df_curr, user_feats_df], axis=1) \n",
    "    \n",
    "\n",
    "\n",
    "    def get_l_feats_df_from_curr_state(self, df_curr):\n",
    "        \"\"\" Get lecture features data using current user's states for each row of df_curr. \n",
    "        \n",
    "        Must be called on test_df \n",
    "            - before dropping the questions ;\n",
    "            - after calling get_lq_feats_from_curr_state() : \n",
    "             /!\\ 'part' must already be in df_curr for both lec & questions /!\\\n",
    "             \n",
    "        Returns a pd dataframe containing features related to lectures. This df should be concatenate to test_df \n",
    "        as soon as questions will be dropped in the inference loop. \"\"\"\n",
    "        \n",
    "        l_count_arr = np.zeros(len(df_curr[df_curr.content_type_id == 0]), dtype=np.int32)\n",
    "        i_row_arr = 0\n",
    "\n",
    "        for i_row, (u_id, c_type_id, part) in enumerate(df_curr[['user_id','content_type_id', 'part']].values):\n",
    "\n",
    "            if u_id not in self.data:\n",
    "                self.add_new_user(u_id)\n",
    "\n",
    "            if(c_type_id == 1): # lecture\n",
    "                self.data[u_id]['l_count'] += 1\n",
    "                \n",
    "            else: # question\n",
    "                l_count_arr[i_row_arr] = self.data[u_id]['l_count']\n",
    "                i_row_arr += 1\n",
    "\n",
    "        return pd.DataFrame({'l_count_u': l_count_arr})\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def get_lq_feats_from_curr_state(self, df_curr, lecture_stats, question_stats):\n",
    "        \"\"\" Get features which requires both questions & lectures from the current state. \n",
    "        Must be called on test_df before dropping the questions.\n",
    "        Return a dataframe with the new columns concatenated to df_curr. \"\"\"\n",
    "\n",
    "        part_arr = np.zeros(len(df_curr), dtype=np.int8)\n",
    "\n",
    "        for i_row, (u_id, c_id, c_type_id) in enumerate(df_curr[['user_id','content_id','content_type_id']].values):\n",
    "\n",
    "            if u_id not in self.data:\n",
    "                self.add_new_user(u_id)\n",
    "\n",
    "            if(c_type_id == 0): # question\n",
    "                part_arr[i_row] = question_stats.data[c_id]['part']\n",
    "            else: # lecture\n",
    "                part_arr[i_row] = lecture_stats.data[c_id]['part']\n",
    "\n",
    "        lq_feats_df = pd.DataFrame({'part': part_arr}) \n",
    "        \n",
    "        df_curr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return pd.concat([df_curr, lq_feats_df], axis=1) \n",
    "    \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:48:18.687298Z",
     "iopub.status.busy": "2021-04-05T04:48:18.685976Z",
     "iopub.status.idle": "2021-04-05T04:48:18.726717Z",
     "shell.execute_reply": "2021-04-05T04:48:18.725869Z"
    },
    "papermill": {
     "duration": 0.098008,
     "end_time": "2021-04-05T04:48:18.726862",
     "exception": false,
     "start_time": "2021-04-05T04:48:18.628854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class QuestionStats(object):\n",
    "    \"\"\" Object which contains stats related to questions for inference. \"\"\"\n",
    "    \n",
    "    def __init__(self, question_feat_stats_paths, question_level_feat_stats_paths):\n",
    "\n",
    "        # [c_id] = { ... }\n",
    "        self.data = {}\n",
    "        for feat in question_feat_stats_paths:\n",
    "            self.add_feat_to_data_path(feat, question_feat_stats_paths[feat])\n",
    "\n",
    "        # [level] = { ... }\n",
    "        self.data_level = {}\n",
    "        for feat in question_level_feat_stats_paths:\n",
    "            self.add_feat_to_data_level_path(feat, question_level_feat_stats_paths[feat])\n",
    "   \n",
    "\n",
    "    \n",
    "    def add_feat_to_data_path(self, feat_name, feat_path):\n",
    "        \"\"\" Load a feature state stored as a dict (pickle file) and set it to data for each question. \"\"\" \n",
    "        feat_dict = load_obj(feat_path)\n",
    "        \n",
    "        for q_id in feat_dict:\n",
    "            \n",
    "            if q_id not in self.data:\n",
    "                self.add_new_question(q_id)\n",
    "                \n",
    "            self.data[q_id][feat_name] = feat_dict[q_id]\n",
    "    \n",
    "    \n",
    "    def add_feat_to_data_level_path(self, feat_name, feat_path):\n",
    "        \"\"\" Load a feature state stored as a dict (pickle file) and set it to data level for each question. \"\"\" \n",
    "        feat_dict = load_obj(feat_path)\n",
    "        \n",
    "        for lvl in feat_dict:\n",
    "            \n",
    "            if lvl not in self.data_level:\n",
    "                self.data_level[lvl] = {}\n",
    "                \n",
    "            self.data_level[lvl][feat_name] = feat_dict[lvl]\n",
    "                    \n",
    "\n",
    "    \n",
    "    def add_new_question(self, q_id):\n",
    "        \"\"\"Initialize data stats dicts for a new question.\"\"\"\n",
    "        self.data[q_id] = {}\n",
    "    \n",
    "    \n",
    "        \n",
    "    def get_q_feats_from_curr_state(self, df_curr):\n",
    "        \"\"\" Get question features data using current user's states for each row of df_curr. Return a dataframe with \n",
    "        the new columns concatenated to df_curr.\n",
    "        Warning : df_curr must only contains questions. \"\"\"\n",
    "        \n",
    "        answ_corr_avg_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        answ_corr_std_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        answ_corr_median_arr = np.zeros(len(df_curr), dtype=np.int8)\n",
    "        answ_tag_count_arr = np.zeros(len(df_curr), dtype=np.int8)\n",
    "        \n",
    "        avg_question_elapsed_time_c_arr = np.zeros(len(df_curr),dtype=np.int64)\n",
    "        std_question_elapsed_time_c_arr = np.zeros(len(df_curr),dtype=np.int64)\n",
    "        avg_time_since_last_c_arr = np.zeros(len(df_curr),dtype=np.int64)\n",
    "        std_time_since_last_c_arr = np.zeros(len(df_curr),dtype=np.int64)\n",
    "\n",
    "        avg_part_score_c_arr = np.zeros(len(df_curr),dtype=np.float16)\n",
    "        std_part_score_c_arr = np.zeros(len(df_curr),dtype=np.float16)\n",
    "        \n",
    "        avg_score_q_level_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "        std_score_q_level_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "        \n",
    "        avg_q_elapsed_time_per_lvl_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        \n",
    "        num_answers_q_arr = np.zeros(len(df_curr), dtype=np.int8)\n",
    "        q_count_trainset_c_arr = np.zeros(len(df_curr), dtype=np.int16)\n",
    "        \n",
    "        q_tags_clusters_arr = np.zeros(len(df_curr), dtype=np.int8)\n",
    "        \n",
    "        curr_avg_q_elapsed_time_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        curr_avg_q_is_explained_arr = np.zeros(len(df_curr), dtype=np.float16)\n",
    "        \n",
    "        for i_row, (q_id, lvl) in enumerate(df_curr[['content_id', 'difficulty_level']].values):\n",
    "            \n",
    "            answ_corr_avg_arr[i_row] = self.data[q_id]['answered_correctly_avg']\n",
    "            answ_corr_std_arr[i_row] = self.data[q_id]['answered_correctly_std']\n",
    "            answ_corr_median_arr[i_row] = self.data[q_id]['answered_correctly_median']\n",
    "            answ_tag_count_arr[i_row] = self.data[q_id]['tag_count']\n",
    "            \n",
    "            avg_question_elapsed_time_c_arr[i_row] = self.data[q_id]['avg_question_elapsed_time']\n",
    "            std_question_elapsed_time_c_arr[i_row] = self.data[q_id]['std_question_elapsed_time']\n",
    "            avg_time_since_last_c_arr[i_row] = self.data[q_id]['avg_time_since_last']\n",
    "            std_time_since_last_c_arr[i_row] = self.data[q_id]['std_time_since_last']\n",
    "            \n",
    "            part = self.data[q_id]['part']\n",
    "            avg_part_score_c_arr[i_row] = self.data[part]['avg_part_score']\n",
    "            std_part_score_c_arr[i_row] = self.data[part]['std_part_score']\n",
    "            \n",
    "            avg_score_q_level_arr[i_row] = self.data_level[lvl]['difficulty_level_avg_score_c']\n",
    "            std_score_q_level_arr[i_row] = self.data_level[lvl]['difficulty_level_std_score_c']\n",
    "            \n",
    "            avg_q_elapsed_time_per_lvl_arr[i_row] = self.data_level[lvl]['avg_q_elapsed_time_level']\n",
    "            \n",
    "            num_answers_q_arr[i_row] = self.data[q_id]['num_answers_q']\n",
    "            q_count_trainset_c_arr[i_row] = self.data[q_id]['q_count_trainset_c']\n",
    "        \n",
    "            q_tags_clusters_arr[i_row] = self.data[q_id]['tag_cluster']\n",
    "            \n",
    "            curr_avg_q_elapsed_time_arr[i_row] = self.data[q_id]['curr_avg_q_elapsed_time_c']\n",
    "            curr_avg_q_is_explained_arr[i_row] = self.data[q_id]['curr_avg_q_is_explained_c']\n",
    "        \n",
    "        \n",
    "        question_feats_df = pd.DataFrame({'answered_correctly_avg_c': answ_corr_avg_arr, \\\n",
    "                                          'answered_correctly_std_c': answ_corr_std_arr, \\\n",
    "                                          'answered_correctly_median_c': answ_corr_median_arr, \\\n",
    "                                          'tag_count_q': answ_tag_count_arr,\n",
    "                                          'avg_question_elapsed_time_c': avg_question_elapsed_time_c_arr,\n",
    "                                          'std_question_elapsed_time_c': std_question_elapsed_time_c_arr,\n",
    "                                          'avg_time_since_last_c': avg_time_since_last_c_arr,\n",
    "                                          'std_time_since_last_c': std_time_since_last_c_arr,\n",
    "                                          'avg_part_score_c': avg_part_score_c_arr,\n",
    "                                          'std_part_score_c': std_part_score_c_arr,\n",
    "                                          'difficulty_level_avg_score_c': avg_score_q_level_arr,\n",
    "                                          'difficulty_level_std_score_c': std_score_q_level_arr,\n",
    "                                          'avg_q_elapsed_time_lvl': avg_q_elapsed_time_per_lvl_arr,\n",
    "                                          'num_answers_q': num_answers_q_arr,\n",
    "                                          'q_count_trainset_c': q_count_trainset_c_arr,\n",
    "                                          'tag_cluster': q_tags_clusters_arr,\n",
    "                                          'curr_avg_q_elapsed_time': curr_avg_q_elapsed_time_arr,\n",
    "                                          'curr_avg_q_is_explained': curr_avg_q_is_explained_arr, })\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "        return pd.concat([df_curr, question_feats_df], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_lq_feats_from_curr_state(self, df_curr):\n",
    "        \"\"\" Get features which requires both questions & lectures from the current state. \n",
    "        Must be called on test_df before dropping the questions.\n",
    "        Return a dataframe with the new columns concatenated to df_curr. \n",
    "        Used to get 'difficulty_level' in previous_test_df. \"\"\"\n",
    "        \n",
    "        q_level_arr = np.zeros(len(df_curr), dtype=np.int8)\n",
    "\n",
    "        for i_row, (c_id, c_type_id) in enumerate(df_curr[['content_id','content_type_id']].values):\n",
    "\n",
    "            is_lecture = c_type_id == 1\n",
    "            if is_lecture:\n",
    "                q_level_arr[i_row] = -1\n",
    "            else:\n",
    "                q_level_arr[i_row] = self.data[c_id]['difficulty_level']\n",
    "\n",
    "        lq_feats_df = pd.DataFrame({'difficulty_level': q_level_arr})\n",
    "        \n",
    "        df_curr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return pd.concat([df_curr, lq_feats_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:48:18.832820Z",
     "iopub.status.busy": "2021-04-05T04:48:18.831943Z",
     "iopub.status.idle": "2021-04-05T04:48:18.835632Z",
     "shell.execute_reply": "2021-04-05T04:48:18.836141Z"
    },
    "papermill": {
     "duration": 0.061183,
     "end_time": "2021-04-05T04:48:18.836342",
     "exception": false,
     "start_time": "2021-04-05T04:48:18.775159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LectureStats(object):\n",
    "    \"\"\" Object which contains stats related to lecture for inference. \"\"\"\n",
    "    \n",
    "    def __init__(self, lecture_feat_stats_paths):\n",
    "\n",
    "        self.data = {}\n",
    "        for feat in lecture_feat_stats_paths:\n",
    "            self.add_feat_to_data(feat, lecture_feat_stats_paths[feat])\n",
    "   \n",
    "    \n",
    "    def add_feat_to_data(self, feat_name, feat_path):\n",
    "        \"\"\" Load a feature state stored as a dict (pickle file) and set it to data for each lecture. \"\"\" \n",
    "        feat_dict = load_obj(feat_path)\n",
    "        \n",
    "        for l_id in feat_dict:\n",
    "            \n",
    "            if l_id not in self.data:\n",
    "                self.add_new_lecture(l_id)\n",
    "                \n",
    "            self.data[l_id][feat_name] = feat_dict[l_id]\n",
    "            \n",
    "            \n",
    "    def add_new_lecture(self, l_id):\n",
    "        \"\"\"Initialize data stats dicts for a new lecture.\"\"\"\n",
    "        self.data[l_id] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:48:18.935090Z",
     "iopub.status.busy": "2021-04-05T04:48:18.934207Z",
     "iopub.status.idle": "2021-04-05T04:48:18.960117Z",
     "shell.execute_reply": "2021-04-05T04:48:18.960731Z"
    },
    "papermill": {
     "duration": 0.077597,
     "end_time": "2021-04-05T04:48:18.960946",
     "exception": false,
     "start_time": "2021-04-05T04:48:18.883349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TagsStats(object):\n",
    "    \"\"\" Object which contains stats related to tags for inference. \"\"\"\n",
    "    \n",
    "    def __init__(self, tags_feat_stats_paths):\n",
    "\n",
    "        self.data = {}\n",
    "        for feat in tags_feat_stats_paths:\n",
    "            self.add_feat_to_data(feat, tags_feat_stats_paths[feat])\n",
    "\n",
    "    \n",
    "    def add_feat_to_data(self, feat_name, feat_path):\n",
    "        \"\"\" Load a feature state stored as a dict (pickle file) and set it to data for each tag. \"\"\" \n",
    "        feat_dict = load_obj(feat_path)\n",
    "        \n",
    "        for t_id in feat_dict:\n",
    "            \n",
    "            if t_id not in self.data:\n",
    "                self.add_new_tag(t_id)\n",
    "                \n",
    "            self.data[t_id][feat_name] = feat_dict[t_id]\n",
    "            \n",
    "            \n",
    "    def add_new_tag(self, t_id):\n",
    "        \"\"\"Initialize data stats dicts for a new question.\"\"\"\n",
    "        self.data[t_id] = {}\n",
    "        self.data[t_id]['answered_correctly_sum'] = 0\n",
    "        self.data[t_id]['q_count'] = 0\n",
    "        \n",
    "    \n",
    "        \n",
    "    def get_q_feats_from_curr_state(self, df_curr, question_stats):\n",
    "        \"\"\" Get question features data using current user's states for each row of df_curr. Return a dataframe with \n",
    "        the new columns concatenated to df_curr.\n",
    "        Warning : df_curr must only contains questions. \"\"\"\n",
    "        \n",
    "        avg_score_t_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        max_avg_score_t_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        min_avg_score_t_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        std_avg_score_t_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        \n",
    "        avg_tag_frequency_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        max_tag_frequency_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        min_tag_frequency_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        std_tag_frequency_arr = np.zeros(len(df_curr), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        for i_row, q_id in enumerate(df_curr['content_id'].values):\n",
    "            \n",
    "            tags_str = question_stats.data[q_id]['tags']\n",
    "            tags = np.array(tags_str.split(' '), dtype=np.int32)\n",
    "            \n",
    "            avg_score_tags_l = []\n",
    "            ncount_tags_l = []\n",
    "            for t_id in tags:\n",
    "                avg_score_tags_l.append(self.data[t_id]['answered_correctly_sum']/self.data[t_id]['q_count'])\n",
    "                ncount_tags_l.append(self.data[t_id]['ncount'])\n",
    "            \n",
    "            avg_score_tags_l = np.array(avg_score_tags_l)\n",
    "            tag_frequencies_arr = np.array(ncount_tags_l)/NUM_UNIQUE_QUESTIONS\n",
    "            \n",
    "            avg_score_t_arr[i_row] = avg_score_tags_l.mean()\n",
    "            max_avg_score_t_arr[i_row] = avg_score_tags_l.max()\n",
    "            min_avg_score_t_arr[i_row] = avg_score_tags_l.min()\n",
    "            std_avg_score_t_arr[i_row] = avg_score_tags_l.std()\n",
    "            \n",
    "            avg_tag_frequency_arr[i_row] = tag_frequencies_arr.mean()\n",
    "            max_tag_frequency_arr[i_row] = tag_frequencies_arr.max()\n",
    "            min_tag_frequency_arr[i_row] = tag_frequencies_arr.min()\n",
    "            std_tag_frequency_arr[i_row] = tag_frequencies_arr.std()\n",
    "            \n",
    "\n",
    "\n",
    "        tag_feats_df = pd.DataFrame({'avg_score_t': avg_score_t_arr,\n",
    "                                     'max_avg_score_t': max_avg_score_t_arr,\n",
    "                                     'min_avg_score_t': min_avg_score_t_arr,\n",
    "                                     'std_avg_score_t': std_avg_score_t_arr,\n",
    "                                     'avg_tag_freq': avg_tag_frequency_arr,\n",
    "                                     'max_tag_freq': max_tag_frequency_arr,\n",
    "                                     'min_tag_freq': min_tag_frequency_arr,\n",
    "                                     'std_tag_freq': std_tag_frequency_arr,})\n",
    "        \n",
    "\n",
    "        return pd.concat([df_curr, tag_feats_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:48:19.066775Z",
     "iopub.status.busy": "2021-04-05T04:48:19.065949Z",
     "iopub.status.idle": "2021-04-05T04:51:50.951700Z",
     "shell.execute_reply": "2021-04-05T04:51:50.950698Z"
    },
    "papermill": {
     "duration": 211.943233,
     "end_time": "2021-04-05T04:51:50.951889",
     "exception": false,
     "start_time": "2021-04-05T04:48:19.008656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading offline features dict to user state ...\n",
      "Loading user cluster avg start hist 200 dict.\n",
      "User state init complete.\n"
     ]
    }
   ],
   "source": [
    "# [CV] Reinitialize user_dicts  \n",
    "if validaten_flg:\n",
    "    \n",
    "    del data_preprocessor\n",
    "    gc.collect()\n",
    "    \n",
    "    data_preproc_params = {'offline_feats_train_paths': offline_features_train_paths, \n",
    "                           'offline_feats_valid_paths': offline_features_valid_paths,\n",
    "                           'train_raw_q_path': df_questions_kfold_paths[kfold_name]['train'], \n",
    "                           'valid_raw_q_path': df_questions_kfold_paths[kfold_name]['valid'],\n",
    "                           'X_feats': FEATS,\n",
    "                           'y_feats': TARGET,\n",
    "                           'train_slice_rows_i': train_slice_rows,\n",
    "                           'valid_slice_rows_i': valid_slice_rows,\n",
    "                           'valid_flag': validaten_flg}\n",
    "\n",
    "    data_preprocessor = DataPreprocessor(**data_preproc_params)\n",
    "\n",
    "\n",
    "# User state\n",
    "u_feat_states_paths = users_feature_states_paths[kfold_name]\n",
    "users_state = UsersState(u_feat_states_paths, user_cluster_avg_score_start_200_path)\n",
    "\n",
    "\n",
    "# Question, lecture, and tags objects\n",
    "question_stats = QuestionStats(question_feature_stats_paths, question_level_feature_stats_paths)\n",
    "lecture_stats = LectureStats(lecture_feature_stats_paths)\n",
    "tags_stats = TagsStats(tags_feature_stats_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:51:51.077203Z",
     "iopub.status.busy": "2021-04-05T04:51:51.076346Z",
     "iopub.status.idle": "2021-04-05T04:51:51.080377Z",
     "shell.execute_reply": "2021-04-05T04:51:51.079608Z"
    },
    "papermill": {
     "duration": 0.080015,
     "end_time": "2021-04-05T04:51:51.080521",
     "exception": false,
     "start_time": "2021-04-05T04:51:51.000506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceUnitTester(object):\n",
    "    \"\"\" Object prepare data for inference. \"\"\"\n",
    "    \n",
    "    def __init__(self, valid_pkl, example_test_file_path):\n",
    "        \n",
    "        self.field_needed = ['row_id','user_id', 'timestamp','content_type_id', 'answered_correctly', 'task_container_id', 'content_id']\n",
    "        \n",
    "        self.valid_df_q = self.get_valid_only_q(valid_pkl)\n",
    "        self.valid_df_l = self.get_valid_with_l(valid_pkl)\n",
    "        self.batch_test_df = self.get_test_batch(example_test_file_path)\n",
    "        \n",
    "    def get_valid_only_q(self, valid_pkl):\n",
    "        '''For debug : only questions.'''\n",
    "        valid_df = pd.read_pickle(valid_pkl)[self.field_needed]\n",
    "        valid_df.drop(valid_df[valid_df.content_type_id == 1].index, inplace=True)\n",
    "        valid_df = valid_df.reset_index(drop=True)\n",
    "        subset_valid_df = valid_df[:3]\n",
    "        return subset_valid_df\n",
    "    \n",
    "    def get_valid_with_l(self, valid_pkl):\n",
    "        '''For debug : include some lectures rows.'''\n",
    "        valid_df = pd.read_pickle(valid_pkl)\n",
    "        valid_df = valid_df.reset_index(drop=True)\n",
    "        subset_valid_df = valid_df.iloc[45:55,:]\n",
    "        subset_valid_df.drop(columns=['answered_correctly'], inplace=True)\n",
    "        return subset_valid_df\n",
    "    \n",
    "    def get_raw_valid(self, valid_pkl):\n",
    "        valid_df = pd.read_pickle(valid_pkl)\n",
    "        valid_df = valid_df.reset_index(drop=True)\n",
    "        return valid_df\n",
    "    \n",
    "    def get_test_batch(self, example_test_file_path):\n",
    "        '''For debug : include some lectures rows.'''\n",
    "        test_df = pd.read_csv(example_test_file_path)\n",
    "        batch_test_df = test_df[:6]\n",
    "        #batch_test_df = batch_test_df.append(batch_test_df.iloc[-1,:])\n",
    "        #batch_test_df = batch_test_df.append(batch_test_df.iloc[-1,:])\n",
    "        batch_test_df.reset_index(drop=True, inplace=True)\n",
    "        #batch_test_df.loc[6,'content_type_id'] = 1\n",
    "        #batch_test_df.loc[5:7,'content_id'] = 89\n",
    "        #batch_test_df = users_state.get_lq_feats_from_curr_state(batch_test_df, lecture_stats, question_stats)\n",
    "        #batch_test_df.loc[5:7,'part'] = 1\n",
    "        return batch_test_df\n",
    "\n",
    "    def get_valid_subset_change_session(self, X_valid, valid_pkl):\n",
    "        raw_valid_df = pd.read_pickle(valid_pkl)\n",
    "        raw_valid_df = raw_valid_df[raw_valid_df.content_type_id == 0]\n",
    "        raw_valid_df.reset_index(drop=True, inplace=True)\n",
    "        change_sess_u_subset = X_valid.iloc[raw_valid[raw_valid.user_id == 2147470777].index,:]\n",
    "        change_sess_u_subset['user_id'] = raw_valid[raw_valid.user_id == 2147470777].user_id\n",
    "        change_sess_u_subset['content_id'] = raw_valid[raw_valid.user_id == 2147470777].content_id\n",
    "        change_sess_u_subset.reset_index(drop=True, inplace=True)\n",
    "        return change_sess_u_subset[25:35]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:51:51.185414Z",
     "iopub.status.busy": "2021-04-05T04:51:51.184658Z",
     "iopub.status.idle": "2021-04-05T04:51:51.187656Z",
     "shell.execute_reply": "2021-04-05T04:51:51.188200Z"
    },
    "papermill": {
     "duration": 0.058809,
     "end_time": "2021-04-05T04:51:51.188399",
     "exception": false,
     "start_time": "2021-04-05T04:51:51.129590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# UNIT TEST INFERENCE\n",
    "######################\n",
    "\n",
    "if unit_test_inference_flg:\n",
    "    inference_unit_tester = InferenceUnitTester(valid_pickle, example_test_file)\n",
    "\n",
    "    valid_df_q = inference_unit_tester.valid_df_q\n",
    "    valid_df_l = inference_unit_tester.valid_df_l\n",
    "    batch_test_df = inference_unit_tester.batch_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:51:51.296583Z",
     "iopub.status.busy": "2021-04-05T04:51:51.295119Z",
     "iopub.status.idle": "2021-04-05T04:51:51.324880Z",
     "shell.execute_reply": "2021-04-05T04:51:51.324090Z"
    },
    "papermill": {
     "duration": 0.088298,
     "end_time": "2021-04-05T04:51:51.325022",
     "exception": false,
     "start_time": "2021-04-05T04:51:51.236724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceProcessor(object):\n",
    "    \"\"\" Object prepare data for inference. \"\"\"\n",
    "    \n",
    "    def __init__(self, data_preproc):\n",
    "        self.data_preprocessor = data_preproc\n",
    "        self.prior_question_elapsed_time_mean = PRIOR_QUESTION_ELAPSED_TIME_TRAIN_MEAN\n",
    "        \n",
    "        \n",
    "    def get_cont_q_count_inference(self, df):\n",
    "        '''Get the number of question per container for df.\n",
    "        Requires user_id',task_container_id,row_id.\n",
    "        Return an ndarray contianing the values of \"num_q_cont\". (values : from 1 to 6). '''\n",
    "\n",
    "        g = df[['user_id','task_container_id','row_id']].groupby(['user_id','task_container_id']).count()\n",
    "        g.reset_index(inplace=True)\n",
    "        g.rename(columns={'row_id':'num_q_cont'},inplace=True)\n",
    "        g['num_q_cont'] = g['num_q_cont'].astype(np.int8, copy=False)\n",
    "        g.set_index('user_id', inplace=True)\n",
    "        g.loc[g[g.num_q_cont >= 6].index,'num_q_cont'] = 6\n",
    "\n",
    "        return g.reindex(df['user_id'].values).reset_index(drop=True).num_q_cont.values\n",
    "\n",
    "    \n",
    "    def get_diff_avg_question_elapsed_time_c(self, df):\n",
    "        '''Req: avg_question_elapsed_time_c (in question_stats), question_elapsed_time (inference). '''\n",
    "        return (df.question_elapsed_time-df.avg_question_elapsed_time_c).fillna(0).astype(np.int64) \n",
    "\n",
    "    def get_diff_avg_time_since_last_c(self, df):\n",
    "        '''Req: avg_time_since_last_c (in question_stats), time_since_last (inference). '''\n",
    "        return (df.time_since_last-df.avg_time_since_last_c).fillna(0).astype(np.int64)\n",
    "\n",
    "    def get_ratio_diff_avg_question_elapsed_time_c(self, df):\n",
    "        '''Req: avg_question_elapsed_time_c (in question_stats), diff_avg_question_elapsed_time_c (inference). '''\n",
    "        return (df.diff_avg_question_elapsed_time_c/df.avg_question_elapsed_time_c).fillna(0).astype(np.float32) \n",
    "              \n",
    "   \n",
    "    def get_ratio_diff_avg_time_since_last_c(self, df):\n",
    "        '''Req: avg_time_since_last_c (in question_stats), diff_avg_time_since_last_c (inference). '''\n",
    "        return (df.diff_avg_time_since_last_c/df.avg_time_since_last_c).fillna(0).astype(np.float32)\n",
    "        \n",
    "        \n",
    "    def get_diff_avg_q_elapsed_time_lvl(self, df):\n",
    "        '''Req: avg_q_elapsed_time_lvl (in question_stats), question_elapsed_time (inference). '''\n",
    "        return (df.question_elapsed_time-df.avg_q_elapsed_time_lvl).fillna(0).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def get_ratio_diff_avg_q_elapsed_time_lvl(self, df):\n",
    "        '''Req: avg_q_elapsed_time_lvl (in question_stats), diff_avg_q_elapsed_time_lvl (inference). '''\n",
    "        return (df.diff_avg_q_elapsed_time_lvl/df.avg_q_elapsed_time_lvl).fillna(0).astype(np.float16)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_diff_avg_score_q_lvl(self, df):\n",
    "        '''Req:  avg_score_q_level_u (user), difficulty_level_avg_score_c (in question_stats). '''\n",
    "        return (df.avg_score_q_level_u-df.difficulty_level_avg_score_c).fillna(0).astype(np.float16)\n",
    "    \n",
    "    \n",
    "    def get_ratio_diff_avg_score_q_lvl(self, df):\n",
    "        '''Req: diff_avg_score_q_lvl (inference), difficulty_level_avg_score_c (in question_stats). '''\n",
    "        return (df.diff_avg_score_q_lvl/df.difficulty_level_avg_score_c).fillna(0).astype(np.float16)\n",
    "    \n",
    "\n",
    "    def get_ratio_diff_avg_score_u_c(self, df):\n",
    "        '''Req: answered_correctly_avg_u (user), answered_correctly_avg_c (in question_stats). '''\n",
    "        return ((df.answered_correctly_avg_u - answered_correctly_avg_c)/df.answered_correctly_avg_c).fillna(0).astype(np.float16)\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "    def apply_online_feats_inference(self, df):\n",
    "        '''Apply online feats which combine raws and user/question/lecture features. Return df with the additionnal features\n",
    "        concatenated.\n",
    "        Must be called after user states and static statistics features update, and before feeding df into the model.'''\n",
    "\n",
    "        # Add features\n",
    "        df['time_btw_cont_mean'] = data_preprocessor.get_time_btw_containers_mean(df)\n",
    "        df['time_per_action_mean'] = data_preprocessor.get_time_per_action_mean(df)\n",
    "        df['is_first_question'] = data_preprocessor.get_is_first_question(df)\n",
    "        df['last_cont_score_mean'] = data_preprocessor.get_last_cont_score_mean(df)\n",
    "        df['cont_q_count'] = self.get_cont_q_count_inference(df)\n",
    "        df['question_elapsed_time'] = data_preprocessor.get_question_elapsed_time(df)\n",
    "        df['time_ratio'] = data_preprocessor.get_time_ratio(df)\n",
    "        df['log_time_ratio'] = data_preprocessor.get_log_time_ratio(df)\n",
    "        \n",
    "        df['ratio_score_u_time_ratio'] = data_preprocessor.get_ratio_score_u_time_ratio(df)\n",
    "        df['ratio_q_count_u_time_ratio'] = data_preprocessor.get_ratio_q_count_u_time_ratio(df)\n",
    "        \n",
    "        df['diff_avg_question_elapsed_time_c'] = self.get_diff_avg_question_elapsed_time_c(df)\n",
    "        df['diff_avg_time_since_last_c'] = self.get_diff_avg_time_since_last_c(df)\n",
    "        df['ratio_diff_avg_question_elapsed_time_c'] = self.get_ratio_diff_avg_question_elapsed_time_c(df)\n",
    "        df['ratio_diff_avg_time_since_last_c'] = self.get_ratio_diff_avg_time_since_last_c(df)\n",
    "        \n",
    "        \n",
    "        df['diff_avg_q_elapsed_time_lvl'] = self.get_diff_avg_q_elapsed_time_lvl(df)\n",
    "        df['ratio_diff_avg_q_elapsed_time_lvl'] = self.get_ratio_diff_avg_q_elapsed_time_lvl(df)\n",
    "        \n",
    "        # NaNs & casts\n",
    "        df['prior_question_had_explanation'] = df.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "        df['prior_question_elapsed_time'] = df.prior_question_elapsed_time.fillna(self.prior_question_elapsed_time_mean)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048608,
     "end_time": "2021-04-05T04:51:51.422917",
     "exception": false,
     "start_time": "2021-04-05T04:51:51.374309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model 2: SAKT\n",
    "\n",
    "Credit : This model heavily rely on this public kernel : https://www.kaggle.com/wangsg/a-self-attentive-model-for-knowledge-tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:51:51.529926Z",
     "iopub.status.busy": "2021-04-05T04:51:51.528955Z",
     "iopub.status.idle": "2021-04-05T04:51:52.746472Z",
     "shell.execute_reply": "2021-04-05T04:51:52.745625Z"
    },
    "papermill": {
     "duration": 1.27411,
     "end_time": "2021-04-05T04:51:52.746615",
     "exception": false,
     "start_time": "2021-04-05T04:51:51.472505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:51:52.854604Z",
     "iopub.status.busy": "2021-04-05T04:51:52.853480Z",
     "iopub.status.idle": "2021-04-05T04:51:52.857306Z",
     "shell.execute_reply": "2021-04-05T04:51:52.857856Z"
    },
    "papermill": {
     "duration": 0.061034,
     "end_time": "2021-04-05T04:51:52.858045",
     "exception": false,
     "start_time": "2021-04-05T04:51:52.797011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DROPOUT_SAKT = 0.1\n",
    "MAX_SEQ = 180\n",
    "EMBED_SIZE_SAKT = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:51:52.985831Z",
     "iopub.status.busy": "2021-04-05T04:51:52.980543Z",
     "iopub.status.idle": "2021-04-05T04:51:53.008062Z",
     "shell.execute_reply": "2021-04-05T04:51:53.007370Z"
    },
    "papermill": {
     "duration": 0.098503,
     "end_time": "2021-04-05T04:51:53.008207",
     "exception": false,
     "start_time": "2021-04-05T04:51:52.909704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size = 200, forward_expansion = 1, bn_size=MAX_SEQ - 1, dropout=0.2):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.lr1 = nn.Linear(state_size, forward_expansion * state_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(bn_size)\n",
    "        self.lr2 = nn.Linear(forward_expansion * state_size, state_size) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.lr1(x))\n",
    "        x = self.bn(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "def future_mask(seq_length):\n",
    "    future_mask = (np.triu(np.ones([seq_length, seq_length]), k = 1)).astype('bool')\n",
    "    return torch.from_numpy(future_mask)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, heads = 8, dropout = DROPOUT_SAKT, forward_expansion = 1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=heads, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FFN(embed_dim, forward_expansion = forward_expansion, dropout=dropout)\n",
    "        self.layer_normal_2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, value, key, query, att_mask):\n",
    "        att_output, att_weight = self.multi_att(value, key, query, attn_mask=att_mask)\n",
    "        att_output = self.dropout(self.layer_normal(att_output + value))\n",
    "        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n",
    "        x = self.ffn(att_output)\n",
    "        x = self.dropout(self.layer_normal_2(x + att_output))\n",
    "        return x.squeeze(-1), att_weight\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_skill, max_seq=100, embed_dim=128, dropout = DROPOUT_SAKT, forward_expansion = 1, num_layers=1, heads = 8):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_skill, self.embed_dim = n_skill, embed_dim\n",
    "        self.embedding = nn.Embedding(2 * n_skill + 1, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq - 1, embed_dim)\n",
    "        self.e_embedding = nn.Embedding(n_skill+1, embed_dim)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, forward_expansion = forward_expansion) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, question_ids):\n",
    "        device = x.device\n",
    "        x = self.embedding(x)\n",
    "        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n",
    "        pos_x = self.pos_embedding(pos_id)\n",
    "        x = self.dropout(x + pos_x)\n",
    "        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n",
    "        e = self.e_embedding(question_ids)\n",
    "        e = e.permute(1, 0, 2)\n",
    "        for layer in self.layers:\n",
    "            att_mask = future_mask(e.size(0)).to(device)\n",
    "            x, att_weight = layer(e, x, x, att_mask=att_mask)\n",
    "            x = x.permute(1, 0, 2)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return x, att_weight\n",
    "\n",
    "class SAKTModel(nn.Module):\n",
    "    def __init__(self, n_skill, max_seq=100, embed_dim=128, dropout = DROPOUT_SAKT, forward_expansion = 1, enc_layers=1, heads = 8):\n",
    "        super(SAKTModel, self).__init__()\n",
    "        self.encoder = Encoder(n_skill, max_seq, embed_dim, dropout, forward_expansion, num_layers=enc_layers)\n",
    "        self.pred = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, x, question_ids):\n",
    "        x, att_weight = self.encoder(x, question_ids)\n",
    "        x = self.pred(x)\n",
    "        return x.squeeze(-1), att_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:51:53.122373Z",
     "iopub.status.busy": "2021-04-05T04:51:53.121200Z",
     "iopub.status.idle": "2021-04-05T04:52:06.569162Z",
     "shell.execute_reply": "2021-04-05T04:52:06.570051Z"
    },
    "papermill": {
     "duration": 13.509412,
     "end_time": "2021-04-05T04:52:06.570305",
     "exception": false,
     "start_time": "2021-04-05T04:51:53.060893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAKT model successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    return SAKTModel(n_skill, max_seq=MAX_SEQ, embed_dim=EMBED_SIZE_SAKT, forward_expansion=1, enc_layers=1, heads=8, dropout=0.1)\n",
    "\n",
    "# Paths\n",
    "sakt_group_pickle_path = SAKT_GROUP_PICKLE_FULLSET_PATH\n",
    "sakt_model_state_dict_path = SAKT_MODEL_STATE_DICT_FULLSET_PATH\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Skills\n",
    "skills = np.load(SAKT_SKILL_NP_PATH)\n",
    "n_skill = len(skills)\n",
    "\n",
    "# Loading group\n",
    "group = load_obj(sakt_group_pickle_path)\n",
    "\n",
    "# Loading SAKT model\n",
    "sakt_model = create_model()\n",
    "sakt_model.load_state_dict(torch.load(sakt_model_state_dict_path, map_location=device))\n",
    "sakt_model.to(device)\n",
    "\n",
    "\n",
    "log('SAKT model successfully loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:52:06.692838Z",
     "iopub.status.busy": "2021-04-05T04:52:06.691858Z",
     "iopub.status.idle": "2021-04-05T04:52:06.696852Z",
     "shell.execute_reply": "2021-04-05T04:52:06.697485Z"
    },
    "papermill": {
     "duration": 0.073876,
     "end_time": "2021-04-05T04:52:06.697663",
     "exception": false,
     "start_time": "2021-04-05T04:52:06.623787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, samples, test_df, n_skill, max_seq=100):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.samples, self.user_ids, self.test_df = samples, [x for x in test_df[\"user_id\"].unique()], test_df\n",
    "        self.n_skill, self.max_seq = n_skill, max_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        test_info = self.test_df.iloc[index]\n",
    "        \n",
    "        user_id = test_info['user_id']\n",
    "        target_id = test_info['content_id']\n",
    "        \n",
    "        content_id_seq = np.zeros(self.max_seq, dtype=int)\n",
    "        answered_correctly_seq = np.zeros(self.max_seq, dtype=int)\n",
    "        \n",
    "        if user_id in self.samples.index:\n",
    "            content_id, answered_correctly = self.samples[user_id]\n",
    "            \n",
    "            seq_len = len(content_id)\n",
    "            \n",
    "            if seq_len >= self.max_seq:\n",
    "                content_id_seq = content_id[-self.max_seq:]\n",
    "                answered_correctly_seq = answered_correctly[-self.max_seq:]\n",
    "            else:\n",
    "                content_id_seq[-seq_len:] = content_id\n",
    "                answered_correctly_seq[-seq_len:] = answered_correctly\n",
    "                \n",
    "        x = content_id_seq[1:].copy()\n",
    "        x += (answered_correctly_seq[1:] == 1) * self.n_skill\n",
    "        \n",
    "        questions = np.append(content_id_seq[2:], [target_id])\n",
    "        \n",
    "        return x, questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0504,
     "end_time": "2021-04-05T04:52:06.799168",
     "exception": false,
     "start_time": "2021-04-05T04:52:06.748768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference : bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:52:06.909922Z",
     "iopub.status.busy": "2021-04-05T04:52:06.908536Z",
     "iopub.status.idle": "2021-04-05T04:52:06.912811Z",
     "shell.execute_reply": "2021-04-05T04:52:06.912217Z"
    },
    "papermill": {
     "duration": 0.063357,
     "end_time": "2021-04-05T04:52:06.912938",
     "exception": false,
     "start_time": "2021-04-05T04:52:06.849581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference ...\n"
     ]
    }
   ],
   "source": [
    "log('Inference ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:52:07.019255Z",
     "iopub.status.busy": "2021-04-05T04:52:07.017778Z",
     "iopub.status.idle": "2021-04-05T04:52:08.435546Z",
     "shell.execute_reply": "2021-04-05T04:52:08.434746Z"
    },
    "papermill": {
     "duration": 1.472198,
     "end_time": "2021-04-05T04:52:08.435685",
     "exception": false,
     "start_time": "2021-04-05T04:52:06.963487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# INFERENCE LOOP\n",
    "########################################################################################\n",
    "\n",
    "import psutil\n",
    "sakt_model.eval()\n",
    "\n",
    "\n",
    "inference_preprocessor = InferenceProcessor(data_preprocessor)\n",
    "\n",
    "previous_test_df = None\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    # State updating\n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    if previous_test_df is not None:\n",
    "        answers = eval(test_df[\"prior_group_answers_correct\"].iloc[0])\n",
    "        previous_test_df[TARGET] = answers\n",
    "        \n",
    "        #---------\n",
    "        # LBGM states\n",
    "        #---------\n",
    "        users_state.update_dicts_from_prev_state(previous_test_df) # lectures are not updated\n",
    "        \n",
    "        #---------\n",
    "        # SAKT states\n",
    "        #---------\n",
    "        if psutil.virtual_memory().percent < 90:\n",
    " \n",
    "\n",
    "            previous_test_df = previous_test_df[previous_test_df.content_type_id == False]\n",
    "            prev_group = previous_test_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n",
    "                r['content_id'].values,\n",
    "                r['answered_correctly'].values))\n",
    "            for prev_user_id in prev_group.index:\n",
    "                prev_group_content = prev_group[prev_user_id][0]\n",
    "                prev_group_ac = prev_group[prev_user_id][1]\n",
    "                if prev_user_id in group.index:\n",
    "                    group[prev_user_id] = (np.append(group[prev_user_id][0],prev_group_content), \n",
    "                                           np.append(group[prev_user_id][1],prev_group_ac))\n",
    "\n",
    "                else:\n",
    "                    group[prev_user_id] = (prev_group_content,prev_group_ac)\n",
    "                if len(group[prev_user_id][0])>MAX_SEQ:\n",
    "                    new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n",
    "                    new_group_ac = group[prev_user_id][1][-MAX_SEQ:]\n",
    "                    group[prev_user_id] = (new_group_content,new_group_ac)    \n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    # Features preparation\n",
    "    #--------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    test_df['prior_question_had_explanation'] = test_df.prior_question_had_explanation.fillna(False).astype('int8')         \n",
    "        \n",
    "    #-- 1) Add 'difficulty level' and 'part', required for dicts update\n",
    "    test_df = question_stats.get_lq_feats_from_curr_state(test_df)\n",
    "    test_df = users_state.get_lq_feats_from_curr_state(test_df, lecture_stats, question_stats)\n",
    "    \n",
    "    # Save test_df for the next iteration\n",
    "    previous_test_df = test_df.copy()\n",
    "    \n",
    "    #-- 2) Add feats from q&l to concate after lecture dropping\n",
    "    test_l_df = users_state.get_l_feats_df_from_curr_state(test_df)\n",
    "\n",
    "    #-- 3) Drop lectures\n",
    "    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    #-- 4) Add feats on questions only\n",
    "    \n",
    "    test_df = pd.concat([test_df, test_l_df], axis=1)\n",
    "    \n",
    "    test_df = question_stats.get_q_feats_from_curr_state(test_df) \n",
    "    test_df = users_state.get_q_feats_from_curr_state(test_df)\n",
    "    test_df = tags_stats.get_q_feats_from_curr_state(test_df, question_stats)\n",
    "    \n",
    "    test_df = inference_preprocessor.apply_online_feats_inference(test_df)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    # LGBM predictions\n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    lgbm_preds = model.predict(test_df[FEATS])\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    # TODO: SAKT predictions\n",
    "    #--------------------------------------------------------------\n",
    "    test_dataset = TestDataset(group, test_df, n_skill, max_seq=MAX_SEQ)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=51200, shuffle=False)\n",
    "    \n",
    "    \n",
    "    sakt_preds = []\n",
    "\n",
    "    for item in test_dataloader:\n",
    "        x = item[0].to(device).long()\n",
    "        target_id = item[1].to(device).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, att_weight = sakt_model(x, target_id)\n",
    "        \n",
    "        \n",
    "        output = torch.sigmoid(output)\n",
    "        output = output[:, -1]\n",
    "\n",
    "        sakt_preds.extend(output.view(-1).data.cpu().numpy()) \n",
    "    \n",
    "    \n",
    "    \n",
    "    # LGBM + SAKT averaging\n",
    "    sakt_preds = np.array(sakt_preds)\n",
    "    ensemble_preds = (0.7*lgbm_preds+0.3*sakt_preds)\n",
    "    \n",
    "    \n",
    "    #-- 5) Predict\n",
    "    test_df[TARGET] = ensemble_preds\n",
    "    set_predict(test_df[['row_id', TARGET]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:52:08.542945Z",
     "iopub.status.busy": "2021-04-05T04:52:08.542170Z",
     "iopub.status.idle": "2021-04-05T04:52:08.548579Z",
     "shell.execute_reply": "2021-04-05T04:52:08.547778Z"
    },
    "papermill": {
     "duration": 0.063327,
     "end_time": "2021-04-05T04:52:08.548726",
     "exception": false,
     "start_time": "2021-04-05T04:52:08.485399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# CROSS VALIDATION\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "if validaten_flg:\n",
    "    y_true = target_df[target_df.content_type_id == 0].answered_correctly # again drop lectures\n",
    "    y_pred = pd.concat(predicted).answered_correctly # ['user_id','answered_correctly'] -> ndarray\n",
    "    print(roc_auc_score(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:52:08.656006Z",
     "iopub.status.busy": "2021-04-05T04:52:08.654963Z",
     "iopub.status.idle": "2021-04-05T04:52:08.659720Z",
     "shell.execute_reply": "2021-04-05T04:52:08.659010Z"
    },
    "papermill": {
     "duration": 0.061423,
     "end_time": "2021-04-05T04:52:08.659868",
     "exception": false,
     "start_time": "2021-04-05T04:52:08.598445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run OK : Ready for submission\n"
     ]
    }
   ],
   "source": [
    "print('Run OK : Ready for submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:52:08.791737Z",
     "iopub.status.busy": "2021-04-05T04:52:08.786276Z",
     "iopub.status.idle": "2021-04-05T04:52:08.807914Z",
     "shell.execute_reply": "2021-04-05T04:52:08.806974Z"
    },
    "papermill": {
     "duration": 0.097101,
     "end_time": "2021-04-05T04:52:08.808083",
     "exception": false,
     "start_time": "2021-04-05T04:52:08.710982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answered_correctly_avg_c</th>\n",
       "      <th>avg_score_q_level_u</th>\n",
       "      <th>avg_score_u_part</th>\n",
       "      <th>is_first_attempt</th>\n",
       "      <th>time_since_last</th>\n",
       "      <th>time_since_last_2</th>\n",
       "      <th>answered_correctly_std_c</th>\n",
       "      <th>current_session_q_count</th>\n",
       "      <th>current_session_time</th>\n",
       "      <th>question_elapsed_time</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_1_right_answ_streak</th>\n",
       "      <th>part</th>\n",
       "      <th>q_explanation_sum_u</th>\n",
       "      <th>ratio_score_u_time_ratio</th>\n",
       "      <th>last_session_break_time</th>\n",
       "      <th>q_count_trainset_c</th>\n",
       "      <th>avg_right_answ_streak_hist_3</th>\n",
       "      <th>q_explanation_avg_u</th>\n",
       "      <th>avg_time_since_last_c</th>\n",
       "      <th>l_count_u</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.627890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666504</td>\n",
       "      <td>1</td>\n",
       "      <td>32799</td>\n",
       "      <td>18903</td>\n",
       "      <td>0.483368</td>\n",
       "      <td>3</td>\n",
       "      <td>75311</td>\n",
       "      <td>32799.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>726817.898074</td>\n",
       "      <td>0</td>\n",
       "      <td>5468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.733491</td>\n",
       "      <td>0.782715</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0</td>\n",
       "      <td>255228</td>\n",
       "      <td>97394</td>\n",
       "      <td>0.442133</td>\n",
       "      <td>14</td>\n",
       "      <td>1427667</td>\n",
       "      <td>255228.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4069</td>\n",
       "      <td>1.696466</td>\n",
       "      <td>115960204</td>\n",
       "      <td>32767</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>37889</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.415218</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0.845703</td>\n",
       "      <td>1</td>\n",
       "      <td>35781230</td>\n",
       "      <td>92999</td>\n",
       "      <td>0.492760</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7156246.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2070</td>\n",
       "      <td>1.381715</td>\n",
       "      <td>35781230</td>\n",
       "      <td>3205</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.985840</td>\n",
       "      <td>524954</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.864874</td>\n",
       "      <td>0.974609</td>\n",
       "      <td>0.845703</td>\n",
       "      <td>1</td>\n",
       "      <td>35781230</td>\n",
       "      <td>92999</td>\n",
       "      <td>0.341858</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7156246.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2070</td>\n",
       "      <td>1.381715</td>\n",
       "      <td>35781230</td>\n",
       "      <td>3205</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.985840</td>\n",
       "      <td>524954</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.456215</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0.845703</td>\n",
       "      <td>1</td>\n",
       "      <td>35781230</td>\n",
       "      <td>92999</td>\n",
       "      <td>0.498079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7156246.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2070</td>\n",
       "      <td>1.381715</td>\n",
       "      <td>35781230</td>\n",
       "      <td>3205</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.985840</td>\n",
       "      <td>524954</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   answered_correctly_avg_c  avg_score_q_level_u  avg_score_u_part  \\\n",
       "0                  0.627890             0.000000          0.666504   \n",
       "1                  0.733491             0.782715          0.671875   \n",
       "2                  0.415218             0.571289          0.845703   \n",
       "3                  0.864874             0.974609          0.845703   \n",
       "4                  0.456215             0.571289          0.845703   \n",
       "\n",
       "   is_first_attempt  time_since_last  time_since_last_2  \\\n",
       "0                 1            32799              18903   \n",
       "1                 0           255228              97394   \n",
       "2                 1         35781230              92999   \n",
       "3                 1         35781230              92999   \n",
       "4                 1         35781230              92999   \n",
       "\n",
       "   answered_correctly_std_c  current_session_q_count  current_session_time  \\\n",
       "0                  0.483368                        3                 75311   \n",
       "1                  0.442133                       14               1427667   \n",
       "2                  0.492760                        0                     0   \n",
       "3                  0.341858                        0                     0   \n",
       "4                  0.498079                        0                     0   \n",
       "\n",
       "   question_elapsed_time  ...  hist_1_right_answ_streak  part  \\\n",
       "0                32799.0  ...                         0     5   \n",
       "1               255228.0  ...                         0     2   \n",
       "2              7156246.0  ...                         0     7   \n",
       "3              7156246.0  ...                         0     7   \n",
       "4              7156246.0  ...                         0     7   \n",
       "\n",
       "   q_explanation_sum_u  ratio_score_u_time_ratio  last_session_break_time  \\\n",
       "0                    0             726817.898074                        0   \n",
       "1                 4069                  1.696466                115960204   \n",
       "2                 2070                  1.381715                 35781230   \n",
       "3                 2070                  1.381715                 35781230   \n",
       "4                 2070                  1.381715                 35781230   \n",
       "\n",
       "   q_count_trainset_c  avg_right_answ_streak_hist_3  q_explanation_avg_u  \\\n",
       "0                5468                      0.000000             0.000000   \n",
       "1               32767                      2.333333             0.998047   \n",
       "2                3205                      0.333333             0.985840   \n",
       "3                3205                      0.333333             0.985840   \n",
       "4                3205                      0.333333             0.985840   \n",
       "\n",
       "   avg_time_since_last_c  l_count_u  \n",
       "0                  41233          0  \n",
       "1                  37889        123  \n",
       "2                 524954         27  \n",
       "3                 524954         27  \n",
       "4                 524954         27  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference features sample\n",
    "test_df[FEATS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T04:52:08.923676Z",
     "iopub.status.busy": "2021-04-05T04:52:08.922556Z",
     "iopub.status.idle": "2021-04-05T04:52:08.928530Z",
     "shell.execute_reply": "2021-04-05T04:52:08.927806Z"
    },
    "papermill": {
     "duration": 0.069012,
     "end_time": "2021-04-05T04:52:08.928667",
     "exception": false,
     "start_time": "2021-04-05T04:52:08.859655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.538380\n",
       "1     0.663291\n",
       "2     0.590066\n",
       "3     0.973922\n",
       "4     0.643600\n",
       "5     0.534273\n",
       "6     0.940147\n",
       "7     0.814437\n",
       "8     0.542967\n",
       "9     0.853229\n",
       "10    0.599507\n",
       "11    0.692839\n",
       "12    0.980183\n",
       "13    0.942471\n",
       "14    0.892039\n",
       "15    0.857754\n",
       "16    0.612839\n",
       "17    0.769468\n",
       "18    0.743086\n",
       "19    0.735842\n",
       "20    0.292633\n",
       "21    0.483123\n",
       "22    0.904868\n",
       "23    0.776741\n",
       "24    0.686324\n",
       "25    0.760144\n",
       "26    0.652089\n",
       "27    0.784797\n",
       "28    0.434220\n",
       "29    0.820995\n",
       "30    0.669553\n",
       "31    0.651687\n",
       "32    0.975113\n",
       "Name: answered_correctly, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction sample\n",
    "test_df[TARGET]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 26653.898077,
   "end_time": "2021-04-05T04:52:09.496534",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-04T21:27:55.598457",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "062e691ace1240c3a000763a020def75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c195e5fd671b4862907a6ba7c686b978",
       "placeholder": "",
       "style": "IPY_MODEL_edf0820c35214f21bf7e360f691fc461",
       "value": " 132/132 [00:15&lt;00:00,  8.50it/s]"
      }
     },
     "1be0e25923f44df69aa347071004366a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28bc8629a5b34ee2a79fb8e13547e398": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2967b1b6dbdf4fd782dc4ca032317d7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4609f0a491e94b23b5ebe1701dfb9f34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6eb2f3d8270e43f08375e7aeb8931bc1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "720912112f1c4f1781275d47b2e0d8ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "7858b65bee844d48afb31c64ba48251a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "786cb0296d4846b49670c87bbae75fb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7858b65bee844d48afb31c64ba48251a",
       "placeholder": "",
       "style": "IPY_MODEL_4609f0a491e94b23b5ebe1701dfb9f34",
       "value": " 132/132 [08:15&lt;00:00,  3.75s/it]"
      }
     },
     "8b1e1905983343638fac95a7a76f784e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_faf2b89ecb38499a838a8f860a2fb05f",
        "IPY_MODEL_786cb0296d4846b49670c87bbae75fb4"
       ],
       "layout": "IPY_MODEL_28bc8629a5b34ee2a79fb8e13547e398"
      }
     },
     "a1126a537c6345379fc5ae0eeafb7bab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ad82615148304ffaa29e999ec048946c",
        "IPY_MODEL_062e691ace1240c3a000763a020def75"
       ],
       "layout": "IPY_MODEL_2967b1b6dbdf4fd782dc4ca032317d7c"
      }
     },
     "a8f747013be645d7ae70b84c0a2cff4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "ad82615148304ffaa29e999ec048946c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Loading offline features: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1be0e25923f44df69aa347071004366a",
       "max": 132.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_720912112f1c4f1781275d47b2e0d8ff",
       "value": 132.0
      }
     },
     "c195e5fd671b4862907a6ba7c686b978": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "edf0820c35214f21bf7e360f691fc461": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "faf2b89ecb38499a838a8f860a2fb05f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Loading offline features: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6eb2f3d8270e43f08375e7aeb8931bc1",
       "max": 132.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a8f747013be645d7ae70b84c0a2cff4a",
       "value": 132.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
